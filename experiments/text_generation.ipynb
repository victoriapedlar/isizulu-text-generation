{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategies\n",
    "Author: Victoria Pedlar\n",
    "\n",
    "This notebook explores open-ended text generation for isiZulu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/victoriapedlar/repos/isizulu-text-generation\n",
      "/Users/victoriapedlar/repos/isizulu-text-generation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/victoriapedlar/repos/isizulu-text-generation/')\n",
    "\n",
    "# Get the current working directory again\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r transformer_requirements.txt\n",
    "# %cd src/transformers\n",
    "# %pip install .\n",
    "# %cd /Users/victoriapedlar/repos/isizulu-text-generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import transformers\n",
    "from transformers import GPT2TokenizerFast\n",
    "import sys\n",
    "sys.path.insert(0,'/Users/victoriapedlar/repos/isizulu-text-generation/scripts')\n",
    "import layer_switching_gpt2\n",
    "from layer_switching_gpt2 import LayerSwitchingGPT2Config, GPT2LayerSwitchingLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('experiments/trained_models/transformer/tokenizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "config = LayerSwitchingGPT2Config.from_pretrained(\"experiments/trained_models/transformer/config.json\")\n",
    "# Load the model from the checkpoint\n",
    "model = GPT2LayerSwitchingLMHeadModel.from_pretrained(\"experiments/trained_models/transformer\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tokenizer attribute of the model\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of decoding strategies to try\n",
    "decoding_strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several decoding strategies available for open-ended language generation using transformer models:\n",
    "\n",
    "1. Argmax decoding: This is a type of greedy search, where the model chooses the most likely next word at each step.\n",
    "\n",
    "2. Beam search: In beam search, the model considers a fixed number of top options at each step, and expands the search space by adding the next most likely words to these options. This can produce more coherent and diverse output than argmax decoding, but can also be slower.\n",
    "\n",
    "3. Nucleus sampling: Nucleus sampling is a variant of top-k sampling, where the model samples from the set of most likely words, with probability proportional to their likelihood. This can produce more diverse output than argmax decoding, but may still be biased towards the most likely words.\n",
    "\n",
    "4. Top-k sampling: In top-k sampling, the model samples from the top-k most likely words at each step. This can produce more diverse output than argmax decoding, but may still be biased towards the most likely words.\n",
    "\n",
    "5. Temperature sampling: In temperature sampling, the model samples from its output distribution with a specified temperature. A high temperature will produce more diverse output, but may also introduce more errors and randomness. A low temperature will produce less diverse output, but may be more accurate.\n",
    "\n",
    "6. Entmax sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_and_references(file_path):\n",
    "    prompts = []\n",
    "    references = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            prompt = ' '.join(line.split()[:5])\n",
    "            reference = line[len(prompt)+1:]\n",
    "\n",
    "            prompts.append(prompt)\n",
    "            references.append(reference)\n",
    "\n",
    "    return prompts, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, references = extract_prompts_and_references(\"data/test/isolezwe.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of tokens in the references is 29.10 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average number of tokens in each reference\n",
    "total_tokens = sum(len(tokenizer.encode(ref)) for ref in references)\n",
    "average_tokens = total_tokens / len(references)\n",
    "\n",
    "print(f\"The average number of tokens in the references is {average_tokens:.2f} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, strategy, hyperparameters, max_length=round(average_tokens)):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    prompt_length = input_ids.shape[1]\n",
    "\n",
    "    if strategy == 'argmax':\n",
    "        output_ids = model.generate(input_ids, max_length=max_length+prompt_length)\n",
    "    elif strategy == 'beam_search':\n",
    "        beam_size = hyperparameters.get('beam_size', 1)\n",
    "        no_repeat_ngram_size = hyperparameters.get('no_repeat_ngram_size', 2)\n",
    "        output_ids = model.generate(input_ids, num_beams=beam_size, no_repeat_ngram_size=no_repeat_ngram_size, max_length=max_length+prompt_length)\n",
    "    elif strategy == 'nucleus_sampling':\n",
    "        top_p = hyperparameters.get('top_p', 0.9)\n",
    "        output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, top_p=top_p)\n",
    "    elif strategy == 'top_k_sampling':\n",
    "        k = hyperparameters.get('k', 10)\n",
    "        output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, top_k=k)\n",
    "    elif strategy == 'temperature_sampling':\n",
    "        temperature = hyperparameters.get('temperature', 1.0)\n",
    "        output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, temperature=temperature)\n",
    "    elif strategy == 'entmax_sampling':\n",
    "        alpha = hyperparameters.get('alpha', 1.5)\n",
    "        output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, entmax=True, alpha=alpha)\n",
    "\n",
    "    # Remove the prompt tokens from the generated text\n",
    "    generated_text = model.tokenizer.decode(output_ids[0][prompt_length:].tolist(), skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rouge scores for a reference/generated sentence pair\n",
    "#source google seq2seq source code.\n",
    "\n",
    "import itertools\n",
    "\n",
    "#supporting function\n",
    "# def _split_into_words(sentences):\n",
    "#   \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
    "#   return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
    "\n",
    "def _split_into_words(sentences):\n",
    "    \"\"\"Splits multiple sentences into tokens and flattens the result\"\"\"\n",
    "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    return list(itertools.chain.from_iterable(tokenized_sentences))\n",
    "\n",
    "#supporting function\n",
    "def _get_word_ngrams(n, sentences):\n",
    "  \"\"\"Calculates word n-grams for multiple sentences.\n",
    "  \"\"\"\n",
    "  assert len(sentences) > 0\n",
    "  assert n > 0\n",
    "\n",
    "  words = _split_into_words(sentences)\n",
    "  return _get_ngrams(n, words)\n",
    "\n",
    "#supporting function\n",
    "def _get_ngrams(n, text):\n",
    "  \"\"\"Calcualtes n-grams.\n",
    "  Args:\n",
    "    n: which n-grams to calculate\n",
    "    text: An array of tokens\n",
    "  Returns:\n",
    "    A set of n-grams\n",
    "  \"\"\"\n",
    "  ngram_set = set()\n",
    "  text_length = len(text)\n",
    "  max_index_ngram_start = text_length - n\n",
    "  for i in range(max_index_ngram_start + 1):\n",
    "    ngram_set.add(tuple(text[i:i + n]))\n",
    "  return ngram_set\n",
    "\n",
    "def rouge_n(reference_sentences, evaluated_sentences, n=2):\n",
    "  \"\"\"\n",
    "  Computes ROUGE-N of two text collections of sentences.\n",
    "  Source: http://research.microsoft.com/en-us/um/people/cyl/download/\n",
    "  papers/rouge-working-note-v1.3.1.pdf\n",
    "  Args:\n",
    "    evaluated_sentences: The sentences that have been picked by the summarizer\n",
    "    reference_sentences: The sentences from the referene set\n",
    "    n: Size of ngram.  Defaults to 2.\n",
    "  Returns:\n",
    "    recall rouge score(float)\n",
    "  Raises:\n",
    "    ValueError: raises exception if a param has len <= 0\n",
    "  \"\"\"\n",
    "  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
    "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
    "\n",
    "  evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
    "  reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
    "  reference_count = len(reference_ngrams)\n",
    "  evaluated_count = len(evaluated_ngrams)\n",
    "\n",
    "  # Gets the overlapping ngrams between evaluated and reference\n",
    "  overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
    "  overlapping_count = len(overlapping_ngrams)\n",
    "\n",
    "  # Handle edge case. This isn't mathematically correct, but it's good enough\n",
    "  if evaluated_count == 0:\n",
    "    precision = 0.0\n",
    "  else:\n",
    "    precision = overlapping_count / evaluated_count\n",
    "\n",
    "  if reference_count == 0:\n",
    "    recall = 0.0\n",
    "  else:\n",
    "    recall = overlapping_count / reference_count\n",
    "\n",
    "  f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
    "\n",
    "  #just returning recall count in rouge, useful for our purpose\n",
    "  return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def bleu_i(weights, all_sentences, smoothing_function, i):\n",
    "    return sentence_bleu(\n",
    "        references=all_sentences[:i] + all_sentences[i + 1:],\n",
    "        hypothesis=all_sentences[i],\n",
    "        weights=weights,\n",
    "        smoothing_function=smoothing_function)\n",
    "\n",
    "def compute_rouge_scores(reference_sentences, generated_sentences, n=2):\n",
    "    return rouge_n(reference_sentences, generated_sentences, n)\n",
    "\n",
    "def evaluate(generated_texts, reference_texts):\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    n_sample = len(generated_texts)\n",
    "    \n",
    "    perplexities = []\n",
    "    bleu_scores = [[] for _ in range(5)]\n",
    "    \n",
    "    for idx, (generated_text, reference_text) in enumerate(zip(generated_texts, reference_texts)):\n",
    "        # Compute perplexity\n",
    "        input_ids = tokenizer.encode(generated_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids, return_dict=True)\n",
    "            loss = outputs.loss\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        perplexities.append(perplexity)\n",
    "        \n",
    "        # Compute BLEU score\n",
    "        all_sentences = [list(reference_text)] + [list(generated_text) for generated_text in generated_texts]\n",
    "\n",
    "        for n_gram in range(1, 6):\n",
    "            if n_gram == 1:\n",
    "                weights = (1.0, 0, 0, 0)\n",
    "            elif n_gram == 2:\n",
    "                weights = (0.5, 0.5, 0, 0)\n",
    "            elif n_gram == 3:\n",
    "                weights = (1.0 / 3, 1.0 / 3, 1.0 / 3, 0)\n",
    "            elif n_gram == 4:\n",
    "                weights = (0.25, 0.25, 0.25, 0.25)\n",
    "            elif n_gram == 5:\n",
    "                weights = (0.2, 0.2, 0.2, 0.2, 0.2)\n",
    "            else:\n",
    "                raise ValueError\n",
    "            bleu_score = bleu_i(weights, all_sentences, smoothing_function, idx)\n",
    "            bleu_scores[n_gram - 1].append(bleu_score)\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        rouge_scores = [[] for _ in range(2)]\n",
    "        for i in range(1, 3):\n",
    "            rouge_score = compute_rouge_scores(reference_texts, generated_texts, i)\n",
    "            rouge_scores[i - 1].append(rouge_score)\n",
    "    \n",
    "    avg_perplexity = sum(perplexities) / n_sample\n",
    "    avg_bleu_scores = [sum(scores) / n_sample for scores in bleu_scores]\n",
    "    avg_rouge_scores = [sum(scores) / n_sample for scores in rouge_scores]\n",
    "\n",
    "    return avg_perplexity, avg_bleu_scores, avg_rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version 1 evaluation code\n",
    "# import torch\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# from rouge import Rouge\n",
    "\n",
    "# def evaluate(generated_text, reference_text):\n",
    "#     rouge_metric = Rouge()\n",
    "\n",
    "#     # Compute perplexity\n",
    "#     input_ids = tokenizer.encode(generated_text, return_tensors=\"pt\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids, labels=input_ids, return_dict=True)\n",
    "#         logits = outputs.logits\n",
    "#         loss = outputs.loss\n",
    "#     perplexity = torch.exp(loss).item()\n",
    "\n",
    "#     # Compute BLEU score\n",
    "#     reference_tokens = reference_text.split()\n",
    "#     generated_tokens = generated_text.split()\n",
    "#     bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
    "\n",
    "#     # Compute ROUGE score\n",
    "#     rouge_scores = rouge_metric.get_scores(generated_text, reference_text)\n",
    "\n",
    "#     return perplexity, bleu_score, rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "def save_results(results: Dict[str, Any], file_path: str):\n",
    "    def convert_keys_to_strings(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {str(k): convert_keys_to_strings(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [convert_keys_to_strings(elem) for elem in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(convert_keys_to_strings(results), f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version 1 pipeline\n",
    "# import os\n",
    "\n",
    "# def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    \n",
    "#     prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "#     folder_path = 'text_generation_results'\n",
    "#     if not os.path.exists(folder_path):\n",
    "#         os.makedirs(folder_path)\n",
    "\n",
    "#     # Create a results dictionary to store the evaluation scores\n",
    "#     results = {}\n",
    "#     for strategy in strategies:\n",
    "#         results[strategy] = {}\n",
    "#         for hyperparameter in hyperparameters[strategy]:\n",
    "#             key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "#             results[strategy][key] = {'perplexity': [], 'bleu_score': [], 'rouge_scores': []}\n",
    "    \n",
    "#     output_file_path = \"text_generation_results/transformer_output_samples.txt\"\n",
    "#     with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "#         # Loop through each prompt and generate text for each decoding strategy with the specified hyperparameters\n",
    "#         for i in range(len(prompts)):\n",
    "#             prompt = prompts[i]\n",
    "#             reference = references[i]\n",
    "#             for strategy in strategies:\n",
    "#                 for hyperparameter in hyperparameters[strategy]:\n",
    "#                     key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "#                     generated_text = generate_text(prompt, strategy, hyperparameter)\n",
    "#                     perplexity, bleu_score, rouge_scores = evaluate(generated_text, reference)\n",
    "#                     results[strategy][key]['perplexity'].append(perplexity)\n",
    "#                     results[strategy][key]['bleu_score'].append(bleu_score)\n",
    "#                     results[strategy][key]['rouge_scores'].append(rouge_scores)\n",
    "#                     # Save the results to the output file\n",
    "#                     output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "#                     output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "#                     output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "#                     output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "#                     output_file.write(f\"Reference text: {reference}\\n\")\n",
    "#                     output_file.write(f\"Perplexity: {perplexity}\\n\")\n",
    "#                     output_file.write(f\"BLEU score: {bleu_score}\\n\")\n",
    "#                     output_file.write(f\"ROUGE scores: {rouge_scores}\\n\\n\")\n",
    "        \n",
    "#     results_file_path = 'text_generation_results/transformer_results.json'\n",
    "#     save_results(results, results_file_path)\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Version 2 pipeline\n",
    "# import os\n",
    "\n",
    "# def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    \n",
    "#     prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "#     folder_path = 'experiments/results'\n",
    "#     if not os.path.exists(folder_path):\n",
    "#         os.makedirs(folder_path)\n",
    "\n",
    "#     # Create a results dictionary to store the evaluation scores\n",
    "#     results = {}\n",
    "#     for strategy in strategies:\n",
    "#         results[strategy] = {}\n",
    "#         for hyperparameter in hyperparameters[strategy]:\n",
    "#             key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "#             results[strategy][key] = {'perplexity': [], 'bleu_score': [], 'rouge_scores': []}\n",
    "    \n",
    "#     output_file_path = \"experiments/results/transformer_output_samples.txt\"\n",
    "#     with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "#         # Loop through each prompt and generate text for each decoding strategy with the specified hyperparameters\n",
    "#         for i in range(len(prompts)):\n",
    "#             prompt = prompts[i]\n",
    "#             reference = references[i]\n",
    "#             for strategy in strategies:\n",
    "#                 for hyperparameter in hyperparameters[strategy]:\n",
    "#                     key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "#                     generated_text = generate_text(prompt, strategy, hyperparameter)\n",
    "#                     perplexity, bleu_score, rouge_scores = evaluate(generated_text, reference)\n",
    "#                     results[strategy][key]['perplexity'].append(perplexity)\n",
    "#                     results[strategy][key]['bleu_score'].append(bleu_score)\n",
    "#                     results[strategy][key]['rouge_scores'].append(rouge_scores)\n",
    "\n",
    "#                     # Save the results to the output file\n",
    "#                     output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "#                     output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "#                     output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "#                     output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "#                     output_file.write(f\"Reference text: {reference}\\n\")\n",
    "#                     output_file.write(f\"Perplexity: {perplexity}\\n\")\n",
    "#                     output_file.write(f\"BLEU score: {bleu_score}\\n\")\n",
    "#                     output_file.write(f\"ROUGE scores: {rouge_scores}\\n\\n\")\n",
    "\n",
    "#         # Calculate the average scores for each strategy and hyperparameter combination\n",
    "#         num_prompts = len(prompts)\n",
    "#         for strategy in results:\n",
    "#             for key in results[strategy]:\n",
    "#                 results[strategy][key]['average_perplexity'] = sum(results[strategy][key]['perplexity']) / num_prompts\n",
    "#                 results[strategy][key]['average_bleu_score'] = sum(results[strategy][key]['bleu_score']) / num_prompts\n",
    "                \n",
    "#                 # Calculate the average ROUGE scores\n",
    "#                 total_rouge_scores = {\"rouge-1\": {\"r\": 0, \"p\": 0, \"f\": 0},\n",
    "#                                       \"rouge-2\": {\"r\": 0, \"p\": 0, \"f\": 0},\n",
    "#                                       \"rouge-l\": {\"r\": 0, \"p\": 0, \"f\": 0}}\n",
    "#                 for rouge_score_list in results[strategy][key]['rouge_scores']:\n",
    "#                     for rouge_score in rouge_score_list:\n",
    "#                         for rouge_type in rouge_score:\n",
    "#                             for metric in rouge_score[rouge_type]:\n",
    "#                                 total_rouge_scores[rouge_type][metric] += rouge_score[rouge_type][metric]\n",
    "#                 for rouge_type in total_rouge_scores:\n",
    "#                     for metric in total_rouge_scores[rouge_type]:\n",
    "#                         total_rouge_scores[rouge_type][metric] /= num_prompts\n",
    "#                 results[strategy][key]['average_rouge_scores'] = total_rouge_scores\n",
    "        \n",
    "#     results_file_path = 'experiments/results/transformer_results.json'\n",
    "#     save_results(results, results_file_path)\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "    folder_path = 'experiments/results'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Create a results dictionary to store the evaluation scores\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        results[strategy] = {}\n",
    "        for hyperparameter in hyperparameters[strategy]:\n",
    "            key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "            results[strategy][key] = {'perplexity': [], 'bleu_scores': [], 'rouge_scores': []}\n",
    "    \n",
    "    output_file_path = \"experiments/results/transformer_output_samples.txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "        # Loop through each prompt and generate text for each decoding strategy with the specified hyperparameters\n",
    "        generated_texts_list = []\n",
    "        for i in range(len(prompts)):\n",
    "            prompt = prompts[i]\n",
    "            reference = references[i]\n",
    "            generated_texts = []\n",
    "            for strategy in strategies:\n",
    "                for hyperparameter in hyperparameters[strategy]:\n",
    "                    key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "                    generated_text = generate_text(prompt, strategy, hyperparameter)\n",
    "                    generated_texts.append(generated_text)\n",
    "                    generated_texts_list.append((generated_text, reference))\n",
    "            avg_perplexity, avg_bleu_scores, avg_rouge_scores = evaluate(*zip(*generated_texts_list))\n",
    "\n",
    "            for idx, strategy in enumerate(strategies):\n",
    "                for hyperparameter in hyperparameters[strategy]:\n",
    "                    key = tuple(hyperparameter.items())\n",
    "                    results[strategy][key]['perplexity'].append(avg_perplexity)\n",
    "                    results[strategy][key]['bleu_scores'].append(avg_bleu_scores)\n",
    "                    results[strategy][key]['rouge_scores'].append(avg_rouge_scores)\n",
    "                    \n",
    "                    # Save the results to the output file\n",
    "                    generated_text = generated_texts[idx]\n",
    "                    output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                    output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                    output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "                    output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "                    output_file.write(f\"Reference text: {reference}\\n\")\n",
    "                    output_file.write(f\"Perplexity: {avg_perplexity}\\n\")\n",
    "                    output_file.write(f\"BLEU scores: {avg_bleu_scores}\\n\")\n",
    "                    output_file.write(f\"ROUGE scores: {avg_rouge_scores}\\n\\n\")\n",
    "\n",
    "    results_file_path = 'experiments/results/transformer_results.json'\n",
    "    save_results(results, results_file_path)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax_sampling']\n",
    "hyperparameters = {\n",
    "    'argmax': [],\n",
    "    'beam_search': [\n",
    "        {'beam_size': 2},\n",
    "        {'beam_size': 5},\n",
    "        {'beam_size': 10},\n",
    "    ],\n",
    "    'nucleus_sampling': [\n",
    "        {'top_p': 0.5},\n",
    "        {'top_p': 0.8},\n",
    "        {'top_p': 0.9},\n",
    "    ],\n",
    "    'top_k_sampling': [\n",
    "        {'k': 5},\n",
    "        {'k': 10},\n",
    "        {'k': 20},\n",
    "    ],\n",
    "    'temperature_sampling': [\n",
    "        {'temperature': 0.5},\n",
    "        {'temperature': 1.0},\n",
    "        {'temperature': 1.5},\n",
    "    ],\n",
    "    'entmax_sampling': [\n",
    "        {'alpha': 0.5},\n",
    "        {'alpha': 1.0},\n",
    "        {'alpha': 1.5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "file_path = 'data/test/small.txt'\n",
    "\n",
    "results = run_pipeline(file_path, strategies, hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWD-LSTM Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/victoriapedlar/repos/isizulu-text-generation/awd_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from model import LSTMModel\n",
    "from drop_connect import WeightDrop\n",
    "from locked_dropout import LockedDropout\n",
    "from embedding_dropout import embedded_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the .pt file\n",
    "model_path = \"experiments/trained_models/awd_lstm/model.pt\"\n",
    "model, _, _ = torch.load(model_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved Corpus object\n",
    "corpus = torch.load(\"experiments/trained_models/awd_lstm/corpus.data\")\n",
    "\n",
    "# Access the tokenizer (dictionary) from the loaded Corpus object\n",
    "dictionary = corpus.dictionary\n",
    "\n",
    "dictionary.unk_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, dictionary):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        if word in dictionary.word2idx:\n",
    "            tokens.append(dictionary.word2idx[word])\n",
    "        else:\n",
    "            tokens.append(dictionary.unk_index)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(logits, temperature):\n",
    "    word_weights = logits.squeeze().div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    return word_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(prompt_tokens, initial_hidden, k=3, temperature=1.0, max_length=round(average_tokens)):\n",
    "    # Initialize beams with (tokens, hidden_state, log_prob)\n",
    "    beams = [([token], initial_hidden, 0.0) for token in prompt_tokens]  # (tokens, hidden_state, log_prob)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        new_beams = []\n",
    "\n",
    "        for tokens, hidden, log_prob in beams:\n",
    "            # Create a new input tensor by taking the last token\n",
    "            token_input = torch.tensor([[tokens[-1]]], dtype=torch.long)\n",
    "            \n",
    "            logits, new_hidden = model(token_input, hidden)\n",
    "            last_token_logits = logits.squeeze()\n",
    "\n",
    "            top_k_logits, top_k_indices = torch.topk(last_token_logits, k)\n",
    "            top_k_probs = torch.softmax(top_k_logits / temperature, dim=-1)\n",
    "\n",
    "            for prob, idx in zip(top_k_probs.squeeze(), top_k_indices.squeeze()):\n",
    "                new_tokens = tokens + [idx.item()]\n",
    "                new_prob = log_prob + prob.item()\n",
    "                new_beams.append((new_tokens, new_hidden, new_prob))\n",
    "\n",
    "        beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:k]\n",
    "\n",
    "    best_beam = torch.tensor(beams[0][0], dtype=torch.long).unsqueeze(0)\n",
    "    return best_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entmax_sampling_step(logits, alpha=1.5):\n",
    "    probabilities = entmax15(logits, dim=-1)\n",
    "    word_idx = torch.multinomial(probabilities, 1)[0]\n",
    "    return word_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from entmax import entmax15\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def generate_text_awd_lstm(prompt, strategy, hyperparameters, max_length=round(average_tokens)):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_prompt_tokens = tokenize_text(prompt, dictionary)\n",
    "        \n",
    "        # Initialize hidden state for a single token\n",
    "        hidden = model.init_hidden(1)\n",
    "\n",
    "        # Process input_prompt_tokens one by one to update the hidden state\n",
    "        for token in input_prompt_tokens:\n",
    "            input_token = torch.tensor([[token]], dtype=torch.long)\n",
    "            _, hidden = model(input_token, hidden)\n",
    "\n",
    "        generated_tokens = []\n",
    "        last_token = input_prompt_tokens[-1] if input_prompt_tokens else None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        if last_token is not None:\n",
    "            input_token = torch.tensor([[last_token]], dtype=torch.long)\n",
    "            logits, hidden = model(input_token, hidden)\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "        else:\n",
    "            # Placeholder for cases when input_prompt_tokens is empty\n",
    "            last_token_logits = torch.zeros(1, len(dictionary.word2idx))\n",
    "\n",
    "        if strategy == 'beam_search':\n",
    "            k = hyperparameters.get('k', 3)\n",
    "            temperature = hyperparameters.get('temperature', 1.0)\n",
    "            best_beam = beam_search(input_prompt_tokens, hidden, k=k, temperature=temperature)\n",
    "            generated_tokens = best_beam.squeeze().tolist()[len(input_prompt_tokens):]\n",
    "            break\n",
    "        else:\n",
    "            # The code for other strategies should be indented to be inside the for-loop\n",
    "            if strategy == 'argmax':\n",
    "                token_index = torch.argmax(last_token_logits, dim=-1).item()\n",
    "            elif strategy == 'nucleus_sampling':\n",
    "                top_p = hyperparameters.get('top_p', 0.9)\n",
    "                sorted_logits, sorted_indices = torch.sort(last_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                indices_to_remove = cumulative_probs > top_p\n",
    "                indices_to_remove[..., 1:] = indices_to_remove[..., :-1].clone()\n",
    "                indices_to_remove[..., 0] = 0\n",
    "                sorted_logits[indices_to_remove] = float('-inf')\n",
    "                probabilities = torch.softmax(sorted_logits, dim=-1)\n",
    "                token_index = np.random.choice(len(dictionary.word2idx), p=probabilities.squeeze().cpu().detach().numpy())\n",
    "            elif strategy == 'top_k_sampling':\n",
    "                k = hyperparameters.get('k', 10)\n",
    "                top_k_logits, _ = torch.topk(last_token_logits, k)\n",
    "                min_top_k_logits = torch.min(top_k_logits, dim=-1).values.unsqueeze(-1)\n",
    "                last_token_logits[last_token_logits < min_top_k_logits] = float('-inf')\n",
    "                probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "                token_index = np.random.choice(len(dictionary.word2idx), p=probabilities.squeeze().cpu().detach().numpy())\n",
    "            elif strategy == 'temperature_sampling':\n",
    "                temperature = hyperparameters.get('temperature', 1.0)\n",
    "                token_index = sample_with_temperature(last_token_logits, temperature)\n",
    "            elif strategy == 'entmax_sampling':\n",
    "                alpha = hyperparameters.get('alpha', 1.5)\n",
    "                token_index = entmax_sampling_step(last_token_logits, alpha)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid strategy: {strategy}\")\n",
    "\n",
    "            generated_tokens.append(token_index)\n",
    "            last_token = token_index\n",
    "            \n",
    "    generated_text = [dictionary.idx2word[token_idx] for token_idx in generated_tokens]\n",
    "    return \" \".join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_perplexity(generated_text):\n",
    "#     # Compute perplexity\n",
    "#     input_ids = torch.tensor([tokenize_text(generated_text, dictionary)], dtype=torch.long)\n",
    "#     hidden = model.init_hidden(input_ids.size(1))\n",
    "#     with torch.no_grad():\n",
    "#         outputs, _ = model(input_ids, hidden)\n",
    "#         logits = outputs[:, :-1, :]  # Remove the last token (there is no next token to predict)\n",
    "#         labels = input_ids[:, 1:]  # Remove the first token (there is no previous token)\n",
    "#         loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
    "#     perplexity = torch.exp(loss).item()\n",
    "\n",
    "#     return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "def _split_into_words(sentences):\n",
    "    \"\"\"Splits multiple sentences into tokens and flattens the result\"\"\"\n",
    "    tokenized_sentences = [tokenize_text(sentence, dictionary) for sentence in sentences]\n",
    "    return list(itertools.chain.from_iterable(tokenized_sentences))\n",
    "\n",
    "def evaluate_awd_lstm(generated_texts, reference_texts):\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    n_sample = len(generated_texts)\n",
    "    \n",
    "    perplexities = []\n",
    "    bleu_scores = [[] for _ in range(5)]\n",
    "    \n",
    "    for idx, (generated_text, reference_text) in enumerate(zip(generated_texts, reference_texts)):\n",
    "        # Compute perplexity\n",
    "        input_ids = torch.tensor([tokenize_text(generated_text, dictionary)], dtype=torch.long)\n",
    "        hidden = model.init_hidden(input_ids.size(1))\n",
    "        with torch.no_grad():\n",
    "            outputs, _ = model(input_ids, hidden)\n",
    "            logits = outputs[:, :-1, :]  # Remove the last token (there is no next token to predict)\n",
    "            labels = input_ids[:, 1:]  # Remove the first token (there is no previous token)\n",
    "            loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        perplexities.append(perplexity)\n",
    "        \n",
    "        # Compute BLEU score\n",
    "        all_sentences = [list(reference_text)] + [list(generated_text) for generated_text in generated_texts]\n",
    "\n",
    "        for n_gram in range(1, 6):\n",
    "            if n_gram == 1:\n",
    "                weights = (1.0, 0, 0, 0)\n",
    "            elif n_gram == 2:\n",
    "                weights = (0.5, 0.5, 0, 0)\n",
    "            elif n_gram == 3:\n",
    "                weights = (1.0 / 3, 1.0 / 3, 1.0 / 3, 0)\n",
    "            elif n_gram == 4:\n",
    "                weights = (0.25, 0.25, 0.25, 0.25)\n",
    "            elif n_gram == 5:\n",
    "                weights = (0.2, 0.2, 0.2, 0.2, 0.2)\n",
    "            else:\n",
    "                raise ValueError\n",
    "            bleu_score = bleu_i(weights, all_sentences, smoothing_function, idx)\n",
    "            bleu_scores[n_gram - 1].append(bleu_score)\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        rouge_scores = [[] for _ in range(2)]\n",
    "        for i in range(1, 3):\n",
    "            rouge_score = compute_rouge_scores(reference_texts, generated_texts, i)\n",
    "            rouge_scores[i - 1].append(rouge_score)\n",
    "    \n",
    "    avg_rouge_scores = [sum(scores) / n_sample for scores in rouge_scores]       \n",
    "    avg_perplexity = sum(perplexities) / n_sample\n",
    "    avg_bleu_scores = [sum(scores) / n_sample for scores in bleu_scores]\n",
    "\n",
    "    return avg_perplexity, avg_bleu_scores, avg_rouge_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Version 1 pipeline code\n",
    "# import os\n",
    "\n",
    "# def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    \n",
    "#     prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "#     folder_path = 'experiments/results'\n",
    "#     if not os.path.exists(folder_path):\n",
    "#         os.makedirs(folder_path)\n",
    "\n",
    "#     # Create a results dictionary to store the evaluation scores\n",
    "#     results = {}\n",
    "#     for strategy in strategies:\n",
    "#         results[strategy] = {}\n",
    "#         for hyperparameter in hyperparameters[strategy]:\n",
    "#             key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "#             results[strategy][key] = {'perplexity': [], 'bleu_score': [], 'rouge_scores': []}\n",
    "    \n",
    "#     output_file_path = \"experiments/results/awd_lstm_output_samples.txt\"\n",
    "#     with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "#         # Loop through each prompt and generate text for each decoding strategy with the specified hyperparameters\n",
    "#         for i in range(len(prompts)):\n",
    "#             prompt = prompts[i]\n",
    "#             reference = references[i]\n",
    "#             for strategy in strategies:\n",
    "#                 for hyperparameter in hyperparameters[strategy]:\n",
    "#                     key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "#                     generated_text = generate_text_awd_lstm(prompt, strategy, hyperparameter)\n",
    "#                     output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "#                     output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "#                     output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "#                     output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "#                     output_file.write(f\"Reference text: {reference}\\n\")\n",
    "#                     perplexity, bleu_score, rouge_scores = evaluate_awd_lstm(generated_text, reference)\n",
    "#                     results[strategy][key]['perplexity'].append(perplexity)\n",
    "#                     results[strategy][key]['bleu_score'].append(bleu_score)\n",
    "#                     results[strategy][key]['rouge_scores'].append(rouge_scores)\n",
    "\n",
    "#                     # Save the results to the output file\n",
    "#                     output_file.write(f\"Perplexity: {perplexity}\\n\")\n",
    "#                     output_file.write(f\"BLEU score: {bleu_score}\\n\")\n",
    "#                     output_file.write(f\"ROUGE scores: {rouge_scores}\\n\\n\")\n",
    "\n",
    "#         # Calculate the average scores for each strategy and hyperparameter combination\n",
    "#         num_prompts = len(prompts)\n",
    "#         for strategy in results:\n",
    "#             for key in results[strategy]:\n",
    "#                 results[strategy][key]['average_perplexity'] = sum(results[strategy][key]['perplexity']) / num_prompts\n",
    "#                 results[strategy][key]['average_bleu_score'] = sum(results[strategy][key]['bleu_score']) / num_prompts\n",
    "                \n",
    "#                 # Calculate the average ROUGE scores\n",
    "#                 total_rouge_scores = {\"rouge-1\": {\"r\": 0, \"p\": 0, \"f\": 0},\n",
    "#                                       \"rouge-2\": {\"r\": 0, \"p\": 0, \"f\": 0},\n",
    "#                                       \"rouge-l\": {\"r\": 0, \"p\": 0, \"f\": 0}}\n",
    "#                 for rouge_score_list in results[strategy][key]['rouge_scores']:\n",
    "#                     for rouge_score in rouge_score_list:\n",
    "#                         for rouge_type in rouge_score:\n",
    "#                             for metric in rouge_score[rouge_type]:\n",
    "#                                 total_rouge_scores[rouge_type][metric] += rouge_score[rouge_type][metric]\n",
    "#                 for rouge_type in total_rouge_scores:\n",
    "#                     for metric in total_rouge_scores[rouge_type]:\n",
    "#                         total_rouge_scores[rouge_type][metric] /= num_prompts \n",
    "#                 results[strategy][key]['average_rouge_scores'] = total_rouge_scores\n",
    "        \n",
    "#     results_file_path = 'experiments/results/awd_lstm_results.json'\n",
    "#     save_results(results, results_file_path)\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    \n",
    "    prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "    folder_path = 'experiments/results'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Create a results dictionary to store the evaluation scores\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        results[strategy] = {}\n",
    "        for hyperparameter in hyperparameters[strategy]:\n",
    "            key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "            results[strategy][key] = {'perplexity': [], 'bleu_scores': [], 'rouge_scores': []}\n",
    "    \n",
    "    output_file_path = \"experiments/results/awd_lstm_output_samples.txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "        # Loop through each prompt and generate text for each decoding strategy with the specified hyperparameters\n",
    "        generated_texts_list = []\n",
    "        for i in range(len(prompts)):\n",
    "            prompt = prompts[i]\n",
    "            reference = references[i]\n",
    "            generated_texts = []\n",
    "            for strategy in strategies:\n",
    "                for hyperparameter in hyperparameters[strategy]:\n",
    "                    key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "                    generated_text = generate_text_awd_lstm(prompt, strategy, hyperparameter)\n",
    "                    generated_texts.append(generated_text)\n",
    "                    generated_texts_list.append((generated_text, reference))\n",
    "            avg_perplexity, avg_bleu_scores, avg_rouge_scores = evaluate_awd_lstm(*zip(*generated_texts_list))\n",
    "\n",
    "            for idx, strategy in enumerate(strategies):\n",
    "                for hyperparameter in hyperparameters[strategy]:\n",
    "                    key = tuple(hyperparameter.items())\n",
    "                    results[strategy][key]['perplexity'].append(avg_perplexity)\n",
    "                    results[strategy][key]['bleu_scores'].append(avg_bleu_scores)\n",
    "                    results[strategy][key]['bleu_scores'].append(avg_rouge_scores)\n",
    "\n",
    "                    # Save the results to the output file\n",
    "                    generated_text = generated_texts[idx]\n",
    "                    output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                    output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                    output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "                    output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "                    output_file.write(f\"Reference text: {reference}\\n\")\n",
    "                    output_file.write(f\"Perplexity: {avg_perplexity}\\n\")\n",
    "                    output_file.write(f\"BLEU scores: {avg_bleu_scores}\\n\")\n",
    "                    output_file.write(f\"ROUGE scores: {avg_rouge_scores}\\n\\n\")\n",
    "\n",
    "    results_file_path = 'experiments/results/awd_lstm_results.json'\n",
    "    save_results(results, results_file_path)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['argmax', 'beam_search', 'top_k_sampling', 'temperature_sampling', 'entmax_sampling']\n",
    "hyperparameters = {\n",
    "    'argmax': [],\n",
    "    'beam_search': [\n",
    "        {'beam_size': 2},\n",
    "        {'beam_size': 5},\n",
    "        {'beam_size': 10},\n",
    "    ],\n",
    "    'nucleus_sampling': [\n",
    "        {'top_p': 0.5},\n",
    "        {'top_p': 0.8},\n",
    "        {'top_p': 0.9},\n",
    "    ],\n",
    "    'top_k_sampling': [\n",
    "        {'k': 5},\n",
    "        {'k': 10},\n",
    "        {'k': 20},\n",
    "    ],\n",
    "    'temperature_sampling': [\n",
    "        {'temperature': 0.5},\n",
    "        {'temperature': 1.0},\n",
    "        {'temperature': 1.5},\n",
    "    ],\n",
    "    'entmax_sampling': [\n",
    "        {'alpha': 0.5},\n",
    "        {'alpha': 1.0},\n",
    "        {'alpha': 1.5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "file_path = 'data/test/small.txt'\n",
    "\n",
    "results = run_pipeline(file_path, strategies, hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/victoriapedlar/repos/isizulu-text-generation/sparse_text_generation/language_modeling/pytorch_transformers')\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('experiments/trained_models/sparse_model/tokenizers')\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"experiments/trained_models/sparse_model\")\n",
    "# Set the tokenizer attribute of the model\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_into_words(sentences):\n",
    "    \"\"\"Splits multiple sentences into tokens and flattens the result\"\"\"\n",
    "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    return list(itertools.chain.from_iterable(tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "    folder_path = 'experiments/results'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Create a results dictionary to store the evaluation scores\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        results[strategy] = {}\n",
    "        for hyperparameter in hyperparameters[strategy]:\n",
    "            key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "            results[strategy][key] = {'perplexity': [], 'bleu_scores': [], 'rouge_scores': []}\n",
    "    \n",
    "    output_file_path = \"experiments/results/sparse_output_samples.txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "        # Loop through each prompt and generate text for each decoding strategy with the specified hyperparameters\n",
    "        generated_texts_list = []\n",
    "        for i in range(len(prompts)):\n",
    "            prompt = prompts[i]\n",
    "            reference = references[i]\n",
    "            generated_texts = []\n",
    "            for strategy in strategies:\n",
    "                for hyperparameter in hyperparameters[strategy]:\n",
    "                    key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "                    generated_text = generate_text(prompt, strategy, hyperparameter)\n",
    "                    generated_texts.append(generated_text)\n",
    "                    generated_texts_list.append((generated_text, reference))\n",
    "            avg_perplexity, avg_bleu_scores, avg_rouge_scores = evaluate(*zip(*generated_texts_list))\n",
    "\n",
    "            for idx, strategy in enumerate(strategies):\n",
    "                for hyperparameter in hyperparameters[strategy]:\n",
    "                    key = tuple(hyperparameter.items())\n",
    "                    results[strategy][key]['perplexity'].append(avg_perplexity)\n",
    "                    results[strategy][key]['bleu_scores'].append(avg_bleu_scores)\n",
    "                    results[strategy][key]['rouge_scores'].append(avg_rouge_scores)\n",
    "                    \n",
    "                    # Save the results to the output file\n",
    "                    generated_text = generated_texts[idx]\n",
    "                    output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                    output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                    output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "                    output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "                    output_file.write(f\"Reference text: {reference}\\n\")\n",
    "                    output_file.write(f\"Perplexity: {avg_perplexity}\\n\")\n",
    "                    output_file.write(f\"BLEU scores: {avg_bleu_scores}\\n\")\n",
    "                    output_file.write(f\"ROUGE scores: {avg_rouge_scores}\\n\\n\")\n",
    "\n",
    "    results_file_path = 'experiments/results/sparse_results.json'\n",
    "    save_results(results, results_file_path)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax_sampling']\n",
    "hyperparameters = {\n",
    "    'argmax': [],\n",
    "    'beam_search': [\n",
    "        {'beam_size': 2},\n",
    "        {'beam_size': 5},\n",
    "        {'beam_size': 10},\n",
    "    ],\n",
    "    'nucleus_sampling': [\n",
    "        {'top_p': 0.5},\n",
    "        {'top_p': 0.8},\n",
    "        {'top_p': 0.9},\n",
    "    ],\n",
    "    'top_k_sampling': [\n",
    "        {'k': 5},\n",
    "        {'k': 10},\n",
    "        {'k': 20},\n",
    "    ],\n",
    "    'temperature_sampling': [\n",
    "        {'temperature': 0.5},\n",
    "        {'temperature': 1.0},\n",
    "        {'temperature': 1.5},\n",
    "    ],\n",
    "    'entmax_sampling': [\n",
    "        {'alpha': 0.5},\n",
    "        {'alpha': 1.0},\n",
    "        {'alpha': 1.5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "file_path = 'data/test/small.txt'\n",
    "\n",
    "results = run_pipeline(file_path, strategies, hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8498851fdb8f20aac0be0c513133504d41018d140b7992e21b4a91b7f62ead50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
