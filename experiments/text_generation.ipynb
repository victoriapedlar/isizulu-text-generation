{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategies\n",
    "Author: Victoria Pedlar\n",
    "\n",
    "This notebook explores open-ended text generation for isiZulu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/victoriapedlar/repos/isizulu-text-generation\n",
      "/Users/victoriapedlar/repos/isizulu-text-generation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/victoriapedlar/repos/isizulu-text-generation/')\n",
    "\n",
    "# Get the current working directory again\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r transformer_requirements.txt\n",
    "# %cd src/transformers\n",
    "# %pip install .\n",
    "# %cd /Users/victoriapedlar/repos/isizulu-text-generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import transformers\n",
    "from transformers import GPT2TokenizerFast\n",
    "import sys\n",
    "sys.path.insert(0,'/Users/victoriapedlar/repos/isizulu-text-generation/scripts')\n",
    "import layer_switching_gpt2\n",
    "from layer_switching_gpt2 import LayerSwitchingGPT2Config, GPT2LayerSwitchingLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('experiments/trained_models/transformer/tokenizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "config = LayerSwitchingGPT2Config.from_pretrained(\"experiments/trained_models/transformer/config.json\")\n",
    "# Load the model from the checkpoint\n",
    "model = GPT2LayerSwitchingLMHeadModel.from_pretrained(\"experiments/trained_models/transformer\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tokenizer attribute of the model\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of decoding strategies to try\n",
    "decoding_strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several decoding strategies available for open-ended language generation using transformer models:\n",
    "\n",
    "1. Argmax decoding: This is a type of greedy search, where the model chooses the most likely next word at each step.\n",
    "\n",
    "2. Beam search: In beam search, the model considers a fixed number of top options at each step, and expands the search space by adding the next most likely words to these options. This can produce more coherent and diverse output than argmax decoding, but can also be slower.\n",
    "\n",
    "3. Nucleus sampling: Nucleus sampling is a variant of top-k sampling, where the model samples from the set of most likely words, with probability proportional to their likelihood. This can produce more diverse output than argmax decoding, but may still be biased towards the most likely words.\n",
    "\n",
    "4. Top-k sampling: In top-k sampling, the model samples from the top-k most likely words at each step. This can produce more diverse output than argmax decoding, but may still be biased towards the most likely words.\n",
    "\n",
    "5. Temperature sampling: In temperature sampling, the model samples from its output distribution with a specified temperature. A high temperature will produce more diverse output, but may also introduce more errors and randomness. A low temperature will produce less diverse output, but may be more accurate.\n",
    "\n",
    "6. Entmax sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_and_references(file_path):\n",
    "    prompts = []\n",
    "    references = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            prompt = ' '.join(line.split()[:5])\n",
    "            reference = line[len(prompt)+1:]\n",
    "\n",
    "            prompts.append(prompt)\n",
    "            references.append(reference)\n",
    "\n",
    "    return prompts, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, references = extract_prompts_and_references(\"data/test/isolezwe.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of tokens in the references is 29.14 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average number of tokens in each reference\n",
    "total_tokens = sum(len(tokenizer.encode(ref)) for ref in references)\n",
    "average_tokens = total_tokens / len(references)\n",
    "max_length = round(average_tokens) + 2\n",
    "\n",
    "print(f\"The average number of tokens in the references is {average_tokens:.2f} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, strategy, hyperparameters, max_length=max_length):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    prompt_length = input_ids.shape[1]\n",
    "\n",
    "    if strategy == 'argmax':\n",
    "        # Generate text with argmax decoding\n",
    "        num_beams = hyperparameters.get('num_beams', 1)\n",
    "        output_ids = model.generate(input_ids, max_length=max_length+prompt_length, num_beams=num_beams, do_sample=False)\n",
    "    else:\n",
    "        # Generate text with the specified decoding strategy and hyperparameters\n",
    "        if strategy == 'beam_search':\n",
    "            beam_size = hyperparameters.get('beam_size', 1)\n",
    "            no_repeat_ngram_size = hyperparameters.get('no_repeat_ngram_size', 2)\n",
    "            output_ids = model.generate(input_ids, num_beams=beam_size, no_repeat_ngram_size=no_repeat_ngram_size, max_length=max_length+prompt_length)\n",
    "        elif strategy == 'nucleus_sampling':\n",
    "            top_p = hyperparameters.get('top_p', 0.9)\n",
    "            output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, top_p=top_p)\n",
    "        elif strategy == 'top_k_sampling':\n",
    "            k = hyperparameters.get('k', 10)\n",
    "            output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, top_k=k)\n",
    "        elif strategy == 'temperature_sampling':\n",
    "            temperature = hyperparameters.get('temperature', 1.0)\n",
    "            output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, temperature=temperature)\n",
    "        elif strategy == 'entmax_sampling':\n",
    "            alpha = hyperparameters.get('alpha', 1.5)\n",
    "            output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, entmax=True, alpha=alpha)\n",
    "\n",
    "    # Remove the prompt tokens from the generated text\n",
    "    if strategy == 'argmax':\n",
    "        generated_text = model.tokenizer.decode(output_ids[0][prompt_length:].tolist(), skip_special_tokens=True)\n",
    "    else:\n",
    "        generated_text = model.tokenizer.decode(output_ids[0][prompt_length:-1].tolist(), skip_special_tokens=True)\n",
    "\n",
    "    generated_text = generated_text.replace('\\n', '')\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def pad_sequence(sequence, n, pad_left=False, pad_right=False,\n",
    "                 left_pad_symbol=None, right_pad_symbol=None):\n",
    "    \"\"\"\n",
    "    Returns a padded sequence of items before ngram extraction.\n",
    "    :param sequence: the source data to be padded\n",
    "    :type sequence: sequence or iter\n",
    "    :param n: the degree of the ngrams\n",
    "    :type n: int\n",
    "    :param pad_left: whether the ngrams should be left-padded\n",
    "    :type pad_left: bool\n",
    "    :param pad_right: whether the ngrams should be right-padded\n",
    "    :type pad_right: bool\n",
    "    :param left_pad_symbol: the symbol to use for left padding (default is None)\n",
    "    :type left_pad_symbol: any\n",
    "    :param right_pad_symbol: the symbol to use for right padding (default is None)\n",
    "    :type right_pad_symbol: any\n",
    "    :rtype: sequence or iter\n",
    "    \"\"\"\n",
    "    sequence = iter(sequence)\n",
    "    if pad_left:\n",
    "        sequence = chain((left_pad_symbol,) * (n - 1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = chain(sequence, (right_pad_symbol,) * (n - 1))\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def ngrams(sequence, n, pad_left=False, pad_right=False,\n",
    "           left_pad_symbol=None, right_pad_symbol=None):\n",
    "    \"\"\"\n",
    "    Return the ngrams generated from a sequence of items, as an iterator.\n",
    "    :param sequence: the source data to be converted into ngrams\n",
    "    :type sequence: sequence or iter\n",
    "    :param n: the degree of the ngrams\n",
    "    :type n: int\n",
    "    :param pad_left: whether the ngrams should be left-padded\n",
    "    :type pad_left: bool\n",
    "    :param pad_right: whether the ngrams should be right-padded\n",
    "    :type pad_right: bool\n",
    "    :param left_pad_symbol: the symbol to use for left padding (default is None)\n",
    "    :type left_pad_symbol: any\n",
    "    :param right_pad_symbol: the symbol to use for right padding (default is None)\n",
    "    :type right_pad_symbol: any\n",
    "    :rtype: sequence or iter\n",
    "    \"\"\"\n",
    "    sequence = pad_sequence(sequence, n, pad_left, pad_right,\n",
    "                            left_pad_symbol, right_pad_symbol)\n",
    "\n",
    "    history = []\n",
    "    while n > 1:\n",
    "        history.append(next(sequence))\n",
    "        n -= 1\n",
    "    for item in sequence:\n",
    "        history.append(item)\n",
    "        yield tuple(history)\n",
    "        del history[0]\n",
    "\n",
    "def distinct_n_sentence_level(sentence, n):\n",
    "    \"\"\"\n",
    "    Compute distinct-N for a single sentence.\n",
    "    :param sentence: a list of words.\n",
    "    :param n: int, ngram.\n",
    "    :return: float, the metric value.\n",
    "    \"\"\"\n",
    "    if len(sentence) == 0:\n",
    "        return 0.0  # Prevent a zero division\n",
    "    distinct_ngrams = set(ngrams(sentence, n))\n",
    "    return len(distinct_ngrams) / len(sentence)\n",
    "\n",
    "\n",
    "def distinct_n_corpus_level(sentences, n):\n",
    "    \"\"\"\n",
    "    Compute average distinct-N of a list of sentences (the corpus).\n",
    "    :param sentences: a list of sentence.\n",
    "    :param n: int, ngram.\n",
    "    :return: float, the average value.\n",
    "    \"\"\"\n",
    "    return sum(distinct_n_sentence_level(sentence, n) for sentence in sentences) / len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "# from cer import calculate_cer_corpus\n",
    "from evaluate import load\n",
    "\n",
    "def evaluate(generated_texts, reference_texts):    \n",
    "    \n",
    "    # # Compute perplexity\n",
    "    # Concatenate all generated texts using newline characters\n",
    "    # concatenated_generated_text = '\\n'.join(generated_texts)\n",
    "    # input_ids = tokenizer.encode(concatenated_generated_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # # Divide input_ids into smaller chunks if the length exceeds the model's max length\n",
    "    # chunk_size = model.config.max_position_embeddings - 1\n",
    "    # input_id_chunks = [input_ids[:, i:i + chunk_size] for i in range(0, input_ids.size(1), chunk_size)]\n",
    "    \n",
    "    # perplexities = []\n",
    "    # with torch.no_grad():\n",
    "    #     for chunk in input_id_chunks:\n",
    "    #         outputs = model(chunk, labels=chunk, return_dict=True)\n",
    "    #         loss = outputs.loss\n",
    "    #         perplexity = torch.exp(loss).item()\n",
    "    #         perplexities.append(perplexity)\n",
    "    \n",
    "    # # Average perplexities\n",
    "    # corpus_perplexity = sum(perplexities) / len(perplexities)\n",
    "        \n",
    "    # Compute corpus-level sacreBLEU score\n",
    "    bleu = BLEU()\n",
    "    bleu_score = bleu.corpus_score(generated_texts, [reference_texts]).score\n",
    "    \n",
    "    # Compute corpus-level chrF++ score\n",
    "    chrf = CHRF()\n",
    "    chrf_score = chrf.corpus_score(generated_texts, [reference_texts]).score\n",
    "\n",
    "    # Compute corpus-level CER score\n",
    "    cer = load(\"cer\")\n",
    "    cer_score = cer.compute(predictions=generated_texts, references=reference_texts)\n",
    "\n",
    "    # Compute corpus-level distinct-2 score\n",
    "    distinct_2_score = distinct_n_corpus_level([text.split() for text in generated_texts], n=2)\n",
    "\n",
    "    return bleu_score, chrf_score, cer_score, distinct_2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "def save_results(results: Dict[str, Any], file_path: str):\n",
    "    def convert_keys_to_strings(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {str(k): convert_keys_to_strings(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [convert_keys_to_strings(elem) for elem in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(convert_keys_to_strings(results), f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "    folder_path = 'experiments/results'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Create a results dictionary to store the evaluation scores\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        results[strategy] = {}\n",
    "        for hyperparameter in hyperparameters[strategy]:\n",
    "            key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "            results[strategy][key] = {'bleu_scores': [], 'chrf_scores': [], 'cer_scores': [], 'distinct_2_scores': []}\n",
    "    \n",
    "    output_file_path = \"experiments/results/transformer_output_samples.txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "        # Loop through each decoding strategy with the specified hyperparameters\n",
    "        for strategy in strategies:\n",
    "            for hyperparameter in hyperparameters[strategy]:\n",
    "                key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "                generated_texts_list = []\n",
    "                for i in range(len(prompts)):\n",
    "                    prompt = prompts[i]\n",
    "                    reference = references[i]\n",
    "                    if not hyperparameter:  # Check if the hyperparameter list is empty\n",
    "                        generated_text = generate_text(prompt, strategy, {})\n",
    "                    else:\n",
    "                        generated_text = generate_text(prompt, strategy, hyperparameter)\n",
    "                    generated_texts_list.append(generated_text)\n",
    "\n",
    "                    output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "                    output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                    output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                    output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "                    output_file.write(f\"Reference text: {reference}\\n\\n\")\n",
    "                \n",
    "                # Evaluate all generated texts using the modified evaluate function\n",
    "                bleu_score, chrf_score, cer_score, distinct_2_score = evaluate(generated_texts_list, references)\n",
    "\n",
    "                # Save the results to the results dictionary\n",
    "                results[strategy][key]['bleu_scores'].append(bleu_score)\n",
    "                results[strategy][key]['chrf_scores'].append(chrf_score)\n",
    "                results[strategy][key]['cer_scores'].append(cer_score)\n",
    "                results[strategy][key]['distinct_2_scores'].append(distinct_2_score)\n",
    "\n",
    "                output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                output_file.write(f\"BLEU score: {bleu_score}\\n\")\n",
    "                output_file.write(f\"chrF++ score: {chrf_score}\\n\")\n",
    "                output_file.write(f\"CER score: {cer_score}\\n\")\n",
    "                output_file.write(f\"Distinct-2 score: {distinct_2_score}\\n\\n\")\n",
    "\n",
    "    results_file_path = 'experiments/results/transformer_results.json'\n",
    "    save_results(results, results_file_path)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax_sampling']\n",
    "hyperparameters = {\n",
    "    'argmax': [\n",
    "        {'num_beams': 1}\n",
    "    ],\n",
    "    'beam_search': [\n",
    "        {'beam_size': 5},\n",
    "        {'beam_size': 10},\n",
    "        {'beam_size': 15},\n",
    "    ],\n",
    "    'nucleus_sampling': [\n",
    "        {'top_p': 0.45},\n",
    "        {'top_p': 0.85},\n",
    "        {'top_p': 0.90},\n",
    "        {'top_p': 0.95},\n",
    "    ],\n",
    "    'top_k_sampling': [\n",
    "        {'k': 10},\n",
    "        {'k': 20},\n",
    "        {'k': 40},\n",
    "        {'k': 80},\n",
    "    ],\n",
    "    'temperature_sampling': [\n",
    "        {'temperature': 0.5},\n",
    "        {'temperature': 0.7},\n",
    "        {'temperature': 0.9},\n",
    "    ],\n",
    "    'entmax_sampling': [\n",
    "        {'alpha': 1.2},\n",
    "        {'alpha': 1.3},\n",
    "        {'alpha': 1.4},\n",
    "        {'alpha': 1.5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "file_path = 'data/test/isolezwe.txt'\n",
    "\n",
    "results = run_pipeline(file_path, strategies, hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWD-LSTM Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/victoriapedlar/repos/isizulu-text-generation/awd_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from model import LSTMModel\n",
    "from drop_connect import WeightDrop\n",
    "from locked_dropout import LockedDropout\n",
    "from embedding_dropout import embedded_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the .pt file\n",
    "model_path = \"experiments/trained_models/awd_lstm/model.pt\"\n",
    "model= torch.load(model_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved Corpus object\n",
    "corpus = torch.load(\"experiments/trained_models/awd_lstm/corpus.data\")\n",
    "\n",
    "# Access the tokenizer (dictionary) from the loaded Corpus object\n",
    "dictionary = corpus.dictionary\n",
    "\n",
    "dictionary.unk_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, dictionary):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        if word in dictionary.word2idx:\n",
    "            tokens.append(dictionary.word2idx[word])\n",
    "        else:\n",
    "            tokens.append(dictionary.unk_index)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(logits, temperature):\n",
    "    word_weights = logits.squeeze().div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    return word_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(prompt_tokens, initial_hidden, k=3, temperature=1.0, max_length=round(average_tokens)):\n",
    "    # Initialize beams with (tokens, hidden_state, log_prob)\n",
    "    beams = [([token], initial_hidden, 0.0) for token in prompt_tokens]  # (tokens, hidden_state, log_prob)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        new_beams = []\n",
    "\n",
    "        for tokens, hidden, log_prob in beams:\n",
    "            # Create a new input tensor by taking the last token\n",
    "            token_input = torch.tensor([[tokens[-1]]], dtype=torch.long)\n",
    "            \n",
    "            logits, new_hidden = model(token_input, hidden)\n",
    "            last_token_logits = logits.squeeze()\n",
    "\n",
    "            top_k_logits, top_k_indices = torch.topk(last_token_logits, k)\n",
    "            top_k_probs = torch.softmax(top_k_logits / temperature, dim=-1)\n",
    "\n",
    "            for prob, idx in zip(top_k_probs.squeeze(), top_k_indices.squeeze()):\n",
    "                new_tokens = tokens + [idx.item()]\n",
    "                new_prob = log_prob + prob.item()\n",
    "                new_beams.append((new_tokens, new_hidden, new_prob))\n",
    "\n",
    "        beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:k]\n",
    "\n",
    "    best_beam = torch.tensor(beams[0][0], dtype=torch.long).unsqueeze(0)\n",
    "    return best_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entmax_sampling_step(logits, alpha=1.5):\n",
    "    probabilities = entmax15(logits, dim=-1)\n",
    "    word_idx = torch.multinomial(probabilities, 1)[0]\n",
    "    return word_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from entmax import entmax15\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def generate_text_awd_lstm(prompt, strategy, hyperparameters, max_length=max_length):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_prompt_tokens = tokenize_text(prompt, dictionary)\n",
    "        \n",
    "        # Initialize hidden state for a single token\n",
    "        hidden = model.init_hidden(1)\n",
    "\n",
    "        # Process input_prompt_tokens one by one to update the hidden state\n",
    "        for token in input_prompt_tokens:\n",
    "            input_token = torch.tensor([[token]], dtype=torch.long)\n",
    "            _, hidden = model(input_token, hidden)\n",
    "\n",
    "        generated_tokens = []\n",
    "        last_token = input_prompt_tokens[-1] if input_prompt_tokens else None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        if last_token is not None:\n",
    "            input_token = torch.tensor([[last_token]], dtype=torch.long)\n",
    "            logits, hidden = model(input_token, hidden)\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "        else:\n",
    "            # Placeholder for cases when input_prompt_tokens is empty\n",
    "            last_token_logits = torch.zeros(1, len(dictionary.word2idx))\n",
    "\n",
    "        if strategy == 'beam_search':\n",
    "            k = hyperparameters.get('k', 3)\n",
    "            temperature = hyperparameters.get('temperature', 1.0)\n",
    "            best_beam = beam_search(input_prompt_tokens, hidden, k=k, temperature=temperature)\n",
    "            generated_tokens = best_beam.squeeze().tolist()[len(input_prompt_tokens):]\n",
    "            break\n",
    "        else:\n",
    "            # The code for other strategies should be indented to be inside the for-loop\n",
    "            if strategy == 'argmax':\n",
    "                dim = hyperparameters.get('dim', -1)\n",
    "                token_index = torch.argmax(last_token_logits, dim=dim).item()\n",
    "            elif strategy == 'nucleus_sampling':\n",
    "                top_p = hyperparameters.get('top_p', 0.9)\n",
    "                sorted_logits, sorted_indices = torch.sort(last_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                indices_to_remove = cumulative_probs > top_p\n",
    "                indices_to_remove[..., 1:] = indices_to_remove[..., :-1].clone()\n",
    "                indices_to_remove[..., 0] = 0\n",
    "                sorted_logits[indices_to_remove] = float('-inf')\n",
    "                probabilities = torch.softmax(sorted_logits, dim=-1)\n",
    "                token_index = np.random.choice(len(dictionary.word2idx), p=probabilities.squeeze().cpu().detach().numpy())\n",
    "            elif strategy == 'top_k_sampling':\n",
    "                k = hyperparameters.get('k', 10)\n",
    "                top_k_logits, _ = torch.topk(last_token_logits, k)\n",
    "                min_top_k_logits = torch.min(top_k_logits, dim=-1).values.unsqueeze(-1)\n",
    "                last_token_logits[last_token_logits < min_top_k_logits] = float('-inf')\n",
    "                probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "                token_index = np.random.choice(len(dictionary.word2idx), p=probabilities.squeeze().cpu().detach().numpy())\n",
    "            elif strategy == 'temperature_sampling':\n",
    "                temperature = hyperparameters.get('temperature', 1.0)\n",
    "                token_index = sample_with_temperature(last_token_logits, temperature)\n",
    "            elif strategy == 'entmax_sampling':\n",
    "                alpha = hyperparameters.get('alpha', 1.5)\n",
    "                token_index = entmax_sampling_step(last_token_logits, alpha)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid strategy: {strategy}\")\n",
    "\n",
    "            generated_tokens.append(token_index)\n",
    "            last_token = token_index\n",
    "            \n",
    "    generated_text = [dictionary.idx2word[token_idx] for token_idx in generated_tokens]\n",
    "    generated_text = [dictionary.idx2word[token_idx].replace('Ġ', '').replace('Ċ', ' ') for token_idx in generated_tokens]\n",
    "    generated_text = \" \".join(generated_text)\n",
    "    generated_text = generated_text.replace('\\n', '')\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_awd_lstm(generated_texts, reference_texts):\n",
    "\n",
    "    # # Ensure each generated text is on a single line\n",
    "    # cleaned_generated_texts = [' '.join(text.strip().split()) for text in generated_texts]\n",
    "    \n",
    "    # # Compute perplexity for each generated text\n",
    "    # perplexities = []\n",
    "    # for text in cleaned_generated_texts:\n",
    "    #     input_ids = torch.tensor([tokenize_text(text, dictionary)], dtype=torch.long)\n",
    "    #     hidden = model.init_hidden(input_ids.size(1))\n",
    "    #     with torch.no_grad():\n",
    "    #         outputs, _ = model(input_ids, hidden)\n",
    "    #         logits = outputs[:, :-1, :]  # Remove the last token (there is no next token to predict)\n",
    "    #         labels = input_ids[:, 1:]  # Remove the first token (there is no previous token)\n",
    "    #         loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
    "    #         perplexity = torch.exp(loss).item()\n",
    "    #         perplexities.append(perplexity)\n",
    "\n",
    "    # # Average perplexities\n",
    "    # corpus_perplexity = sum(perplexities) / len(perplexities)\n",
    "        \n",
    "    # Compute corpus-level sacreBLEU score\n",
    "    bleu = BLEU()\n",
    "    bleu_score = bleu.corpus_score(generated_texts, [reference_texts]).score\n",
    "    \n",
    "    # Compute corpus-level chrF++ score\n",
    "    chrf = CHRF()\n",
    "    chrf_score = chrf.corpus_score(generated_texts, [reference_texts]).score\n",
    "\n",
    "    # Compute corpus-level CER score\n",
    "    cer = load(\"cer\")\n",
    "    cer_score = cer.compute(predictions=generated_texts, references=reference_texts)\n",
    "\n",
    "    # Compute corpus-level distinct-N score\n",
    "    distinct_2_score = distinct_n_corpus_level([text.split() for text in generated_texts], n=2)\n",
    "\n",
    "    return bleu_score, chrf_score, cer_score, distinct_2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    \n",
    "    prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "    folder_path = 'experiments/results'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Create a results dictionary to store the evaluation scores\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        results[strategy] = {}\n",
    "        for hyperparameter in hyperparameters[strategy]:\n",
    "            key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "            results[strategy][key] = {'bleu_scores': [], 'chrf_scores': [], 'cer_scores': [], 'distinct_2_scores': []}\n",
    "    \n",
    "    output_file_path = \"experiments/results/awd_lstm_output_samples.txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "        # Loop through each decoding strategy with the specified hyperparameters\n",
    "        for strategy in strategies:\n",
    "            for hyperparameter in hyperparameters[strategy]:\n",
    "                key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "                generated_texts_list = []\n",
    "                for i in range(len(prompts)):\n",
    "                    prompt = prompts[i]\n",
    "                    reference = references[i]\n",
    "                    if not hyperparameter:  # Check if the hyperparameter list is empty\n",
    "                        generated_text = generate_text_awd_lstm(prompt, strategy, {})\n",
    "                    else:\n",
    "                        generated_text = generate_text_awd_lstm(prompt, strategy, hyperparameter)\n",
    "                    generated_texts_list.append(generated_text)\n",
    "\n",
    "                    output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "                    output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                    output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                    output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "                    output_file.write(f\"Reference text: {reference}\\n\\n\")\n",
    "                \n",
    "                # Evaluate all generated texts using the modified evaluate function\n",
    "                bleu_score, chrf_score, cer_score, distinct_2_score = evaluate_awd_lstm(generated_texts_list, references)\n",
    "\n",
    "                # Save the results to the results dictionary\n",
    "                results[strategy][key]['bleu_scores'].append(bleu_score)\n",
    "                results[strategy][key]['chrf_scores'].append(chrf_score)\n",
    "                results[strategy][key]['cer_scores'].append(cer_score)\n",
    "                results[strategy][key]['distinct_2_scores'].append(distinct_2_score)\n",
    "\n",
    "                output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                output_file.write(f\"BLEU score: {bleu_score}\\n\")\n",
    "                output_file.write(f\"chrF++ score: {chrf_score}\\n\")\n",
    "                output_file.write(f\"CER score: {cer_score}\\n\")\n",
    "                output_file.write(f\"Distinct-2 score: {distinct_2_score}\\n\\n\")\n",
    "\n",
    "    results_file_path = 'experiments/results/awd_lstm_results.json'\n",
    "    save_results(results, results_file_path)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax_sampling']\n",
    "hyperparameters = {\n",
    "    'argmax': [\n",
    "        {'dim': -1}\n",
    "    ],\n",
    "    'beam_search': [\n",
    "        {'beam_size': 5},\n",
    "        {'beam_size': 10},\n",
    "        {'beam_size': 15},\n",
    "    ],\n",
    "    'nucleus_sampling': [\n",
    "        {'top_p': 0.45},\n",
    "        {'top_p': 0.85},\n",
    "        {'top_p': 0.90},\n",
    "        {'top_p': 0.95},\n",
    "    ],\n",
    "    'top_k_sampling': [\n",
    "        {'k': 10},\n",
    "        {'k': 20},\n",
    "        {'k': 40},\n",
    "        {'k': 80},\n",
    "    ],\n",
    "    'temperature_sampling': [\n",
    "        {'temperature': 0.5},\n",
    "        {'temperature': 0.7},\n",
    "        {'temperature': 0.9},\n",
    "    ],\n",
    "    'entmax_sampling': [\n",
    "        {'alpha': 1.2},\n",
    "        {'alpha': 1.3},\n",
    "        {'alpha': 1.4},\n",
    "        {'alpha': 1.5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "file_path = 'data/test/isolezwe.txt'\n",
    "\n",
    "results = run_pipeline(file_path, strategies, hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/victoriapedlar/repos/isizulu-text-generation/sparse_text_generation/language_modeling/pytorch_transformers')\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('experiments/trained_models/sparse_model/tokenizers')\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"experiments/trained_models/sparse_model\")\n",
    "# Set the tokenizer attribute of the model\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "    folder_path = 'experiments/results'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Create a results dictionary to store the evaluation scores\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        results[strategy] = {}\n",
    "        for hyperparameter in hyperparameters[strategy]:\n",
    "            key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "            results[strategy][key] = {'bleu_scores': [], 'chrf_scores': [], 'cer_scores': [], 'distinct_2_scores': []}\n",
    "    \n",
    "    output_file_path = \"experiments/results/sparse_output_samples.txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "        # Loop through each decoding strategy with the specified hyperparameters\n",
    "        for strategy in strategies:\n",
    "            for hyperparameter in hyperparameters[strategy]:\n",
    "                key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "                generated_texts_list = []\n",
    "                for i in range(len(prompts)):\n",
    "                    prompt = prompts[i]\n",
    "                    reference = references[i]\n",
    "                    if not hyperparameter:  # Check if the hyperparameter list is empty\n",
    "                        generated_text = generate_text(prompt, strategy, {})\n",
    "                    else:\n",
    "                        generated_text = generate_text(prompt, strategy, hyperparameter)\n",
    "                    generated_texts_list.append(generated_text)\n",
    "\n",
    "                    output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "                    output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                    output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                    output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "                    output_file.write(f\"Reference text: {reference}\\n\\n\")\n",
    "                \n",
    "                # Evaluate all generated texts using the modified evaluate function\n",
    "                bleu_score, chrf_score, cer_score, distinct_2_score = evaluate(generated_texts_list, references)\n",
    "\n",
    "                # Save the results to the results dictionary\n",
    "                results[strategy][key]['bleu_scores'].append(bleu_score)\n",
    "                results[strategy][key]['chrf_scores'].append(chrf_score)\n",
    "                results[strategy][key]['cer_scores'].append(cer_score)\n",
    "                results[strategy][key]['distinct_1_scores'].append(distinct_2_score)\n",
    "\n",
    "                output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                output_file.write(f\"BLEU score: {bleu_score}\\n\")\n",
    "                output_file.write(f\"chrF++ score: {chrf_score}\\n\")\n",
    "                output_file.write(f\"CER score: {cer_score}\\n\")\n",
    "                output_file.write(f\"Distinct-2 score: {distinct_2_score}\\n\\n\")\n",
    "\n",
    "    results_file_path = 'experiments/results/sparse_results.json'\n",
    "    save_results(results, results_file_path)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0s/n2h82mgn247_9xz7jbz3w97c0000gn/T/ipykernel_59731/946865724.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/test/isolezwe.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/0s/n2h82mgn247_9xz7jbz3w97c0000gn/T/ipykernel_59731/2064256098.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(file_path, strategies, hyperparameters)\u001b[0m\n\u001b[1;32m     28\u001b[0m                         \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                         \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0mgenerated_texts_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/0s/n2h82mgn247_9xz7jbz3w97c0000gn/T/ipykernel_59731/3020354274.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(prompt, strategy, hyperparameters, max_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Generate text with argmax decoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mnum_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'num_beams'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mprompt_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Generate text with the specified decoding strategy and hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, **model_kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             )\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_generate_no_beam_search\u001b[0;34m(self, input_ids, cur_len, max_length, min_length, do_sample, temperature, top_k, top_p, repetition_penalty, no_repeat_ngram_size, bad_words_ids, pad_token_id, eos_token_id, batch_size, attention_mask, use_cache, model_kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m             )\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/transformers/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/myenv/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax_sampling']\n",
    "hyperparameters = {\n",
    "    'argmax': [\n",
    "        {'num_beams': 1}\n",
    "    ],\n",
    "    'beam_search': [\n",
    "        {'beam_size': 5},\n",
    "        {'beam_size': 10},\n",
    "        {'beam_size': 15},\n",
    "    ],\n",
    "    'nucleus_sampling': [\n",
    "        {'top_p': 0.45},\n",
    "        {'top_p': 0.85},\n",
    "        {'top_p': 0.90},\n",
    "        {'top_p': 0.95},\n",
    "    ],\n",
    "    'top_k_sampling': [\n",
    "        {'k': 10},\n",
    "        {'k': 20},\n",
    "        {'k': 40},\n",
    "        {'k': 80},\n",
    "    ],\n",
    "    'temperature_sampling': [\n",
    "        {'temperature': 0.5},\n",
    "        {'temperature': 0.7},\n",
    "        {'temperature': 0.9},\n",
    "    ],\n",
    "    'entmax_sampling': [\n",
    "        {'alpha': 1.2},\n",
    "        {'alpha': 1.3},\n",
    "        {'alpha': 1.4},\n",
    "        {'alpha': 1.5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "file_path = 'data/test/isolezwe.txt'\n",
    "\n",
    "results = run_pipeline(file_path, strategies, hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8498851fdb8f20aac0be0c513133504d41018d140b7992e21b4a91b7f62ead50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
