{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Strategies\n",
    "Author: Victoria Pedlar\n",
    "\n",
    "This notebook explores open-ended text generation for isiZulu."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/victoriapedlar/repos/isizulu-text-generation\n",
      "/Users/victoriapedlar/repos/isizulu-text-generation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir('/Users/victoriapedlar/repos/isizulu-text-generation/')\n",
    "\n",
    "# Get the current working directory again\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r transformer_requirements.txt\n",
    "# %cd src/transformers\n",
    "# %pip install .\n",
    "# %cd /Users/victoriapedlar/repos/isizulu-text-generation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "import transformers\n",
    "from transformers import GPT2TokenizerFast\n",
    "import sys\n",
    "sys.path.insert(0,'/Users/victoriapedlar/repos/isizulu-text-generation/scripts')\n",
    "import layer_switching_gpt2\n",
    "from layer_switching_gpt2 import LayerSwitchingGPT2Config, GPT2LayerSwitchingLMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('experiments/trained_models/transformer/tokenizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "config = LayerSwitchingGPT2Config.from_pretrained(\"experiments/trained_models/transformer/config.json\")\n",
    "# Load the model from the checkpoint\n",
    "model = GPT2LayerSwitchingLMHeadModel.from_pretrained(\"experiments/trained_models/transformer\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tokenizer attribute of the model\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of decoding strategies to try\n",
    "decoding_strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several decoding strategies available for open-ended language generation using transformer models:\n",
    "\n",
    "1. Argmax decoding: This is a type of greedy search, where the model chooses the most likely next word at each step.\n",
    "\n",
    "2. Beam search: In beam search, the model considers a fixed number of top options at each step, and expands the search space by adding the next most likely words to these options. This can produce more coherent and diverse output than argmax decoding, but can also be slower.\n",
    "\n",
    "3. Nucleus sampling: Nucleus sampling is a variant of top-k sampling, where the model samples from the set of most likely words, with probability proportional to their likelihood. This can produce more diverse output than argmax decoding, but may still be biased towards the most likely words.\n",
    "\n",
    "4. Top-k sampling: In top-k sampling, the model samples from the top-k most likely words at each step. This can produce more diverse output than argmax decoding, but may still be biased towards the most likely words.\n",
    "\n",
    "5. Temperature sampling: In temperature sampling, the model samples from its output distribution with a specified temperature. A high temperature will produce more diverse output, but may also introduce more errors and randomness. A low temperature will produce less diverse output, but may be more accurate.\n",
    "\n",
    "6. Entmax sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_and_references(file_path):\n",
    "    prompts = []\n",
    "    references = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            prompt = ' '.join(line.split()[:5])\n",
    "            reference = line[len(prompt)+1:]\n",
    "\n",
    "            prompts.append(prompt)\n",
    "            references.append(reference)\n",
    "\n",
    "    return prompts, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, references = extract_prompts_and_references(\"data/test/isolezwe.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of tokens in the references is 29.14 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average number of tokens in each reference\n",
    "total_tokens = sum(len(tokenizer.encode(ref)) for ref in references)\n",
    "average_tokens = total_tokens / len(references)\n",
    "\n",
    "print(f\"The average number of tokens in the references is {average_tokens:.2f} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, strategy, hyperparameters, max_length=round(average_tokens)):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    prompt_length = input_ids.shape[1]\n",
    "\n",
    "    if strategy == 'argmax':\n",
    "        # Generate text with argmax decoding\n",
    "        num_beams = hyperparameters.get('num_beams', 1)\n",
    "        output_ids = model.generate(input_ids, max_length=max_length+prompt_length, num_beams=num_beams, do_sample=False)\n",
    "    else:\n",
    "        # Generate text with the specified decoding strategy and hyperparameters\n",
    "        if strategy == 'beam_search':\n",
    "            beam_size = hyperparameters.get('beam_size', 1)\n",
    "            no_repeat_ngram_size = hyperparameters.get('no_repeat_ngram_size', 2)\n",
    "            output_ids = model.generate(input_ids, num_beams=beam_size, no_repeat_ngram_size=no_repeat_ngram_size, max_length=max_length+prompt_length)\n",
    "        elif strategy == 'nucleus_sampling':\n",
    "            top_p = hyperparameters.get('top_p', 0.9)\n",
    "            output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, top_p=top_p)\n",
    "        elif strategy == 'top_k_sampling':\n",
    "            k = hyperparameters.get('k', 10)\n",
    "            output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, top_k=k)\n",
    "        elif strategy == 'temperature_sampling':\n",
    "            temperature = hyperparameters.get('temperature', 1.0)\n",
    "            output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, temperature=temperature)\n",
    "        elif strategy == 'entmax_sampling':\n",
    "            alpha = hyperparameters.get('alpha', 1.5)\n",
    "            output_ids = model.generate(input_ids, do_sample=True, max_length=max_length+prompt_length, entmax=True, alpha=alpha)\n",
    "\n",
    "    # Remove the prompt tokens from the generated text\n",
    "    if strategy == 'argmax':\n",
    "        generated_text = model.tokenizer.decode(output_ids[0][prompt_length:].tolist(), skip_special_tokens=True)\n",
    "    else:\n",
    "        generated_text = model.tokenizer.decode(output_ids[0][prompt_length:-1].tolist(), skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "def evaluate(generated_texts, reference_texts):\n",
    "    \n",
    "    # Ensure each generated text is on a single line\n",
    "    cleaned_generated_texts = [' '.join(text.strip().split()) for text in generated_texts]\n",
    "    \n",
    "    # Concatenate all generated texts using newline characters\n",
    "    concatenated_generated_text = '\\n'.join(cleaned_generated_texts)\n",
    "    \n",
    "    # Compute perplexity\n",
    "    input_ids = tokenizer.encode(concatenated_generated_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Divide input_ids into smaller chunks if the length exceeds the model's max length\n",
    "    chunk_size = model.config.max_position_embeddings - 1\n",
    "    input_id_chunks = [input_ids[:, i:i + chunk_size] for i in range(0, input_ids.size(1), chunk_size)]\n",
    "    \n",
    "    perplexities = []\n",
    "    with torch.no_grad():\n",
    "        for chunk in input_id_chunks:\n",
    "            outputs = model(chunk, labels=chunk, return_dict=True)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            perplexities.append(perplexity)\n",
    "    \n",
    "    # Average perplexities\n",
    "    corpus_perplexity = sum(perplexities) / len(perplexities)\n",
    "        \n",
    "    # Compute corpus-level sacreBLEU score\n",
    "    bleu = BLEU()\n",
    "    bleu_score = bleu.corpus_score(generated_texts, [reference_texts]).score\n",
    "    \n",
    "    # Compute corpus-level chrF++ score\n",
    "    chrf = CHRF()\n",
    "    chrf_score = chrf.corpus_score(generated_texts, [reference_texts]).score\n",
    "\n",
    "    return corpus_perplexity, bleu_score, chrf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "def save_results(results: Dict[str, Any], file_path: str):\n",
    "    def convert_keys_to_strings(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {str(k): convert_keys_to_strings(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [convert_keys_to_strings(elem) for elem in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(convert_keys_to_strings(results), f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "    folder_path = 'experiments/results'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Create a results dictionary to store the evaluation scores\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        results[strategy] = {}\n",
    "        for hyperparameter in hyperparameters[strategy]:\n",
    "            key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "            results[strategy][key] = {'perplexity': [], 'bleu_scores': [], 'chrf_scores': []}\n",
    "    \n",
    "    output_file_path = \"experiments/results/transformer_output_samples.txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "        # Loop through each decoding strategy with the specified hyperparameters\n",
    "        for strategy in strategies:\n",
    "            for hyperparameter in hyperparameters[strategy]:\n",
    "                key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "                generated_texts_list = []\n",
    "                for i in range(len(prompts)):\n",
    "                    prompt = prompts[i]\n",
    "                    reference = references[i]\n",
    "                    if not hyperparameter:  # Check if the hyperparameter list is empty\n",
    "                        generated_text = generate_text(prompt, strategy, {})\n",
    "                    else:\n",
    "                        generated_text = generate_text(prompt, strategy, hyperparameter)\n",
    "                    generated_texts_list.append(generated_text)\n",
    "\n",
    "                    output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "                    output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                    output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                    output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "                    output_file.write(f\"Reference text: {reference}\\n\\n\")\n",
    "                \n",
    "                # Evaluate all generated texts using the modified evaluate function\n",
    "                corpus_perplexity, bleu_score, chrf_score = evaluate(generated_texts_list, references)\n",
    "\n",
    "                # Save the results to the results dictionary\n",
    "                results[strategy][key]['perplexity'].append(corpus_perplexity)\n",
    "                results[strategy][key]['bleu_scores'].append(bleu_score)\n",
    "                results[strategy][key]['chrf_scores'].append(chrf_score)\n",
    "\n",
    "                output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                output_file.write(f\"Perplexity: {corpus_perplexity}\\n\")\n",
    "                output_file.write(f\"BLEU score: {bleu_score}\\n\")\n",
    "                output_file.write(f\"chrF++ score: {chrf_score}\\n\\n\")\n",
    "\n",
    "    results_file_path = 'experiments/results/transformer_results.json'\n",
    "    save_results(results, results_file_path)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax_sampling']\n",
    "hyperparameters = {\n",
    "    'argmax': [\n",
    "        {'num_beams': 1}\n",
    "    ],\n",
    "    'beam_search': [\n",
    "        {'beam_size': 5},\n",
    "        {'beam_size': 10},\n",
    "        {'beam_size': 15},\n",
    "    ],\n",
    "    'nucleus_sampling': [\n",
    "        {'top_p': 0.45},\n",
    "        {'top_p': 0.85},\n",
    "        {'top_p': 0.95},\n",
    "    ],\n",
    "    'top_k_sampling': [\n",
    "        {'k': 10},\n",
    "        {'k': 20},\n",
    "        {'k': 40},\n",
    "    ],\n",
    "    'temperature_sampling': [\n",
    "        {'temperature': 0.5},\n",
    "        {'temperature': 0.7},\n",
    "        {'temperature': 0.9},\n",
    "    ],\n",
    "    'entmax_sampling': [\n",
    "        {'alpha': 1.2},\n",
    "        {'alpha': 1.3},\n",
    "        {'alpha': 1.4},\n",
    "        {'alpha': 1.5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "file_path = 'data/test/isolezwe.txt'\n",
    "\n",
    "results = run_pipeline(file_path, strategies, hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWD-LSTM Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/victoriapedlar/repos/isizulu-text-generation/awd_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from model import LSTMModel\n",
    "from drop_connect import WeightDrop\n",
    "from locked_dropout import LockedDropout\n",
    "from embedding_dropout import embedded_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the .pt file\n",
    "model_path = \"experiments/trained_models/awd_lstm/model.pt\"\n",
    "model= torch.load(model_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved Corpus object\n",
    "corpus = torch.load(\"experiments/trained_models/awd_lstm/corpus.data\")\n",
    "\n",
    "# Access the tokenizer (dictionary) from the loaded Corpus object\n",
    "dictionary = corpus.dictionary\n",
    "\n",
    "dictionary.unk_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, dictionary):\n",
    "    tokens = []\n",
    "    for word in text.split():\n",
    "        if word in dictionary.word2idx:\n",
    "            tokens.append(dictionary.word2idx[word])\n",
    "        else:\n",
    "            tokens.append(dictionary.unk_index)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(logits, temperature):\n",
    "    word_weights = logits.squeeze().div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    return word_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(prompt_tokens, initial_hidden, k=3, temperature=1.0, max_length=round(average_tokens)):\n",
    "    # Initialize beams with (tokens, hidden_state, log_prob)\n",
    "    beams = [([token], initial_hidden, 0.0) for token in prompt_tokens]  # (tokens, hidden_state, log_prob)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        new_beams = []\n",
    "\n",
    "        for tokens, hidden, log_prob in beams:\n",
    "            # Create a new input tensor by taking the last token\n",
    "            token_input = torch.tensor([[tokens[-1]]], dtype=torch.long)\n",
    "            \n",
    "            logits, new_hidden = model(token_input, hidden)\n",
    "            last_token_logits = logits.squeeze()\n",
    "\n",
    "            top_k_logits, top_k_indices = torch.topk(last_token_logits, k)\n",
    "            top_k_probs = torch.softmax(top_k_logits / temperature, dim=-1)\n",
    "\n",
    "            for prob, idx in zip(top_k_probs.squeeze(), top_k_indices.squeeze()):\n",
    "                new_tokens = tokens + [idx.item()]\n",
    "                new_prob = log_prob + prob.item()\n",
    "                new_beams.append((new_tokens, new_hidden, new_prob))\n",
    "\n",
    "        beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:k]\n",
    "\n",
    "    best_beam = torch.tensor(beams[0][0], dtype=torch.long).unsqueeze(0)\n",
    "    return best_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entmax_sampling_step(logits, alpha=1.5):\n",
    "    probabilities = entmax15(logits, dim=-1)\n",
    "    word_idx = torch.multinomial(probabilities, 1)[0]\n",
    "    return word_idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from entmax import entmax15\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def generate_text_awd_lstm(prompt, strategy, hyperparameters, max_length=round(average_tokens)):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_prompt_tokens = tokenize_text(prompt, dictionary)\n",
    "        \n",
    "        # Initialize hidden state for a single token\n",
    "        hidden = model.init_hidden(1)\n",
    "\n",
    "        # Process input_prompt_tokens one by one to update the hidden state\n",
    "        for token in input_prompt_tokens:\n",
    "            input_token = torch.tensor([[token]], dtype=torch.long)\n",
    "            _, hidden = model(input_token, hidden)\n",
    "\n",
    "        generated_tokens = []\n",
    "        last_token = input_prompt_tokens[-1] if input_prompt_tokens else None\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        if last_token is not None:\n",
    "            input_token = torch.tensor([[last_token]], dtype=torch.long)\n",
    "            logits, hidden = model(input_token, hidden)\n",
    "            last_token_logits = logits[:, -1, :]\n",
    "        else:\n",
    "            # Placeholder for cases when input_prompt_tokens is empty\n",
    "            last_token_logits = torch.zeros(1, len(dictionary.word2idx))\n",
    "\n",
    "        if strategy == 'beam_search':\n",
    "            k = hyperparameters.get('k', 3)\n",
    "            temperature = hyperparameters.get('temperature', 1.0)\n",
    "            best_beam = beam_search(input_prompt_tokens, hidden, k=k, temperature=temperature)\n",
    "            generated_tokens = best_beam.squeeze().tolist()[len(input_prompt_tokens):]\n",
    "            break\n",
    "        else:\n",
    "            # The code for other strategies should be indented to be inside the for-loop\n",
    "            if strategy == 'argmax':\n",
    "                dim = hyperparameters.get('dim', -1)\n",
    "                token_index = torch.argmax(last_token_logits, dim=dim).item()\n",
    "            elif strategy == 'nucleus_sampling':\n",
    "                top_p = hyperparameters.get('top_p', 0.9)\n",
    "                sorted_logits, sorted_indices = torch.sort(last_token_logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                indices_to_remove = cumulative_probs > top_p\n",
    "                indices_to_remove[..., 1:] = indices_to_remove[..., :-1].clone()\n",
    "                indices_to_remove[..., 0] = 0\n",
    "                sorted_logits[indices_to_remove] = float('-inf')\n",
    "                probabilities = torch.softmax(sorted_logits, dim=-1)\n",
    "                token_index = np.random.choice(len(dictionary.word2idx), p=probabilities.squeeze().cpu().detach().numpy())\n",
    "            elif strategy == 'top_k_sampling':\n",
    "                k = hyperparameters.get('k', 10)\n",
    "                top_k_logits, _ = torch.topk(last_token_logits, k)\n",
    "                min_top_k_logits = torch.min(top_k_logits, dim=-1).values.unsqueeze(-1)\n",
    "                last_token_logits[last_token_logits < min_top_k_logits] = float('-inf')\n",
    "                probabilities = torch.softmax(last_token_logits, dim=-1)\n",
    "                token_index = np.random.choice(len(dictionary.word2idx), p=probabilities.squeeze().cpu().detach().numpy())\n",
    "            elif strategy == 'temperature_sampling':\n",
    "                temperature = hyperparameters.get('temperature', 1.0)\n",
    "                token_index = sample_with_temperature(last_token_logits, temperature)\n",
    "            elif strategy == 'entmax_sampling':\n",
    "                alpha = hyperparameters.get('alpha', 1.5)\n",
    "                token_index = entmax_sampling_step(last_token_logits, alpha)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid strategy: {strategy}\")\n",
    "\n",
    "            generated_tokens.append(token_index)\n",
    "            last_token = token_index\n",
    "            \n",
    "    generated_text = [dictionary.idx2word[token_idx] for token_idx in generated_tokens]\n",
    "    generated_text = [dictionary.idx2word[token_idx].replace('Ġ', ' ').replace('Ċ', '') for token_idx in generated_tokens]\n",
    "    return \" \".join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sacrebleu\n",
    "\n",
    "def evaluate_awd_lstm(generated_texts, reference_texts):\n",
    "    # Ensure each generated text is on a single line\n",
    "    cleaned_generated_texts = [' '.join(text.strip().split()) for text in generated_texts]\n",
    "    \n",
    "    # Compute perplexity for each generated text\n",
    "    perplexities = []\n",
    "    for text in cleaned_generated_texts:\n",
    "        input_ids = torch.tensor([tokenize_text(text, dictionary)], dtype=torch.long)\n",
    "        hidden = model.init_hidden(input_ids.size(1))\n",
    "        with torch.no_grad():\n",
    "            outputs, _ = model(input_ids, hidden)\n",
    "            logits = outputs[:, :-1, :]  # Remove the last token (there is no next token to predict)\n",
    "            labels = input_ids[:, 1:]  # Remove the first token (there is no previous token)\n",
    "            loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), labels.reshape(-1))\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            perplexities.append(perplexity)\n",
    "\n",
    "    # Average perplexities\n",
    "    corpus_perplexity = sum(perplexities) / len(perplexities)\n",
    "        \n",
    "    # Compute corpus-level sacreBLEU score\n",
    "    bleu = BLEU()\n",
    "    bleu_score = bleu.corpus_score(generated_texts, [reference_texts]).score\n",
    "    \n",
    "    # Compute corpus-level chrF++ score\n",
    "    chrf = CHRF()\n",
    "    chrf_score = chrf.corpus_score(generated_texts, [reference_texts]).score\n",
    "\n",
    "    return corpus_perplexity, bleu_score, chrf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    \n",
    "    prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "    folder_path = 'experiments/results'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Create a results dictionary to store the evaluation scores\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        results[strategy] = {}\n",
    "        for hyperparameter in hyperparameters[strategy]:\n",
    "            key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "            results[strategy][key] = {'perplexity': [], 'bleu_scores': [], 'chrf_scores': []}\n",
    "    \n",
    "    output_file_path = \"experiments/results/awd_lstm_output_samples.txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "        # Loop through each decoding strategy with the specified hyperparameters\n",
    "        for strategy in strategies:\n",
    "            for hyperparameter in hyperparameters[strategy]:\n",
    "                key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "                generated_texts_list = []\n",
    "                for i in range(len(prompts)):\n",
    "                    prompt = prompts[i]\n",
    "                    reference = references[i]\n",
    "                    if not hyperparameter:  # Check if the hyperparameter list is empty\n",
    "                        generated_text = generate_text_awd_lstm(prompt, strategy, {})\n",
    "                    else:\n",
    "                        generated_text = generate_text_awd_lstm(prompt, strategy, hyperparameter)\n",
    "                    generated_texts_list.append(generated_text)\n",
    "\n",
    "                    output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "                    output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                    output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                    output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "                    output_file.write(f\"Reference text: {reference}\\n\\n\")\n",
    "                \n",
    "                # Evaluate all generated texts using the modified evaluate function\n",
    "                corpus_perplexity, bleu_score, chrf_score = evaluate_awd_lstm(generated_texts_list, references)\n",
    "\n",
    "                # Save the results to the results dictionary\n",
    "                results[strategy][key]['perplexity'].append(corpus_perplexity)\n",
    "                results[strategy][key]['bleu_scores'].append(bleu_score)\n",
    "                results[strategy][key]['chrf_scores'].append(chrf_score)\n",
    "\n",
    "                output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                output_file.write(f\"Perplexity: {corpus_perplexity}\\n\")\n",
    "                output_file.write(f\"BLEU score: {bleu_score}\\n\")\n",
    "                output_file.write(f\"chrF++ score: {chrf_score}\\n\\n\")\n",
    "\n",
    "    results_file_path = 'experiments/results/awd_lstm_results.json'\n",
    "    save_results(results, results_file_path)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax_sampling']\n",
    "hyperparameters = {\n",
    "    'argmax': [\n",
    "        {'dim': -1}\n",
    "    ],\n",
    "    'beam_search': [\n",
    "        {'beam_size': 5},\n",
    "        {'beam_size': 10},\n",
    "        {'beam_size': 15},\n",
    "    ],\n",
    "    'nucleus_sampling': [\n",
    "        {'top_p': 0.45},\n",
    "        {'top_p': 0.85},\n",
    "        {'top_p': 0.95},\n",
    "    ],\n",
    "    'top_k_sampling': [\n",
    "        {'k': 10},\n",
    "        {'k': 20},\n",
    "        {'k': 40},\n",
    "    ],\n",
    "    'temperature_sampling': [\n",
    "        {'temperature': 0.5},\n",
    "        {'temperature': 0.7},\n",
    "        {'temperature': 0.9},\n",
    "    ],\n",
    "    'entmax_sampling': [\n",
    "        {'alpha': 1.2},\n",
    "        {'alpha': 1.3},\n",
    "        {'alpha': 1.4},\n",
    "        {'alpha': 1.5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "file_path = 'data/test/isolezwe.txt'\n",
    "\n",
    "results = run_pipeline(file_path, strategies, hyperparameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/Users/victoriapedlar/repos/isizulu-text-generation/sparse_text_generation/language_modeling/pytorch_transformers')\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained('experiments/trained_models/sparse_model/tokenizers')\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"experiments/trained_models/sparse_model\")\n",
    "# Set the tokenizer attribute of the model\n",
    "model.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(file_path, strategies, hyperparameters):\n",
    "    prompts, references = extract_prompts_and_references(file_path)\n",
    "\n",
    "    folder_path = 'experiments/results'\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    # Create a results dictionary to store the evaluation scores\n",
    "    results = {}\n",
    "    for strategy in strategies:\n",
    "        results[strategy] = {}\n",
    "        for hyperparameter in hyperparameters[strategy]:\n",
    "            key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "            results[strategy][key] = {'perplexity': [], 'bleu_scores': [], 'chrf_scores': []}\n",
    "    \n",
    "    output_file_path = \"experiments/results/sparse_output_samples.txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "\n",
    "        # Loop through each decoding strategy with the specified hyperparameters\n",
    "        for strategy in strategies:\n",
    "            for hyperparameter in hyperparameters[strategy]:\n",
    "                key = tuple(hyperparameter.items())  # Convert the dictionary to a tuple of tuples\n",
    "                generated_texts_list = []\n",
    "                for i in range(len(prompts)):\n",
    "                    prompt = prompts[i]\n",
    "                    reference = references[i]\n",
    "                    if not hyperparameter:  # Check if the hyperparameter list is empty\n",
    "                        generated_text = generate_text(prompt, strategy, {})\n",
    "                    else:\n",
    "                        generated_text = generate_text(prompt, strategy, hyperparameter)\n",
    "                    generated_texts_list.append(generated_text)\n",
    "\n",
    "                    output_file.write(f\"Prompt: {prompt}\\n\")\n",
    "                    output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                    output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                    output_file.write(f\"Generated text: {generated_text}\\n\")\n",
    "                    output_file.write(f\"Reference text: {reference}\\n\\n\")\n",
    "                \n",
    "                # Evaluate all generated texts using the modified evaluate function\n",
    "                corpus_perplexity, bleu_score, chrf_score = evaluate(generated_texts_list, references)\n",
    "\n",
    "                # Save the results to the results dictionary\n",
    "                results[strategy][key]['perplexity'].append(corpus_perplexity)\n",
    "                results[strategy][key]['bleu_scores'].append(bleu_score)\n",
    "                results[strategy][key]['chrf_scores'].append(chrf_score)\n",
    "\n",
    "                output_file.write(f\"Strategy: {strategy}\\n\")\n",
    "                output_file.write(f\"Hyperparameters: {hyperparameter}\\n\")\n",
    "                output_file.write(f\"Perplexity: {corpus_perplexity}\\n\")\n",
    "                output_file.write(f\"BLEU score: {bleu_score}\\n\")\n",
    "                output_file.write(f\"chrF++ score: {chrf_score}\\n\\n\")\n",
    "\n",
    "    results_file_path = 'experiments/results/sparse_results.json'\n",
    "    save_results(results, results_file_path)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "strategies = ['argmax', 'beam_search', 'nucleus_sampling', 'top_k_sampling', 'temperature_sampling', 'entmax_sampling']\n",
    "hyperparameters = {\n",
    "    'argmax': [\n",
    "        {'num_beams': 1}\n",
    "    ],\n",
    "    'beam_search': [\n",
    "        {'beam_size': 5},\n",
    "        {'beam_size': 10},\n",
    "        {'beam_size': 15},\n",
    "    ],\n",
    "    'nucleus_sampling': [\n",
    "        {'top_p': 0.45},\n",
    "        {'top_p': 0.85},\n",
    "        {'top_p': 0.95},\n",
    "    ],\n",
    "    'top_k_sampling': [\n",
    "        {'k': 10},\n",
    "        {'k': 20},\n",
    "        {'k': 40},\n",
    "    ],\n",
    "    'temperature_sampling': [\n",
    "        {'temperature': 0.5},\n",
    "        {'temperature': 0.7},\n",
    "        {'temperature': 0.9},\n",
    "    ],\n",
    "    'entmax_sampling': [\n",
    "        {'alpha': 1.2},\n",
    "        {'alpha': 1.3},\n",
    "        {'alpha': 1.4},\n",
    "        {'alpha': 1.5},\n",
    "    ],\n",
    "}\n",
    "\n",
    "file_path = 'data/test/isolezwe.txt'\n",
    "\n",
    "results = run_pipeline(file_path, strategies, hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8498851fdb8f20aac0be0c513133504d41018d140b7992e21b4a91b7f62ead50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
