{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AWD-LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victoriapedlar/isizulu-text-generation/blob/main/Copy_of_AWD_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "e204UkJVwJ4J"
      },
      "cell_type": "markdown",
      "source": [
        "#Language Model Using Techniques from AWD - LSTM\n",
        "This notebook contains an implementation of a Language Model using state-of-the-art techniques also seen in the current state-of-the-art AWD-LSTM\n",
        "\n",
        "This model was built as part of a project in the course 02456 Deep Learning @ DTU - Technical University of Denmark\n",
        "+ This code was originally forked from the [PyTorch word level language modeling example](https://github.com/pytorch/examples/tree/master/word_language_model) and is heavily inspired by the original AWD-LSTM implementation [LSTM and QRNN Language Model Toolkit](https://github.com/salesforce/awd-lstm-lm)\n",
        "\n",
        "+ The code in this notebook is available on [google colab](https://colab.research.google.com/drive/1yyUGJfyYKdvPi6J7ZlsxPg9E_ppZG1xU) and on [github](https://github.com/mikkelbrusen/awd-inspired-lstm).\n",
        "\n",
        "The model comes with instructions to train a word level language models over the Penn Treebank (PTB).\n",
        "\n",
        "The project was carried out by [Gustav Madslund](https://github.com/gustavmadslund) and [Mikkel Møller Brusen](https://github.com/mikkelbrusen).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Qxi6G-Vy5kjq"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Below is a checklist on which components from the original AWD-LSTM we have implemented in this model:\n",
        "### Core components\n",
        "\n",
        "1.   **[x]  - Multi Layer** - We will need to controll what happens in between the layers, therefore, instead of using the multi layer cuDNN lstm implementation, we will create multiple single layer cuDNN lstms.\n",
        "2.   **[x] - Weight drop** using DropConnect on hidden-hidden weights $[U^i, U^f, U^o, U^c]$ before forward and backward pass - makes it possible to use cuDNN LSTM\n",
        "3.   **[x] - Optimization** using SGD and ASGD while training\n",
        "\n",
        "### Extended regularization techniques\n",
        "4.   **[ ] - Variable sequence length** to allow all elements in the dataset to experience a full BPTT window\n",
        "  - **[ ] - Rescale learning rate** to counter the varible sequence lengths favoring short sequences with fixed learning rate\n",
        "5.   **[x] - Variational dropout AKA LockDrop** for everything else than hidden-hidden, such that we use same dropout mask for all input/output in a forward backward pass of LSTM\n",
        "6.   **[x] - Embedding dropout** which is **not** just a dropout applied on the embedding\n",
        "7.   **[x]  - Weight tying** to reduce parameters and prevent model from having to learn one-to-one correspondance between input and output\n",
        "8.   **[x] - Embed size** independent from hidden size, to reduce parameters.\n",
        "9.   **[ ] - AR and TAR** - $L_2$-regularization by applying AR and TAR loss on the final RNN layer - can screw stuff up\n"
      ]
    },
    {
      "metadata": {
        "id": "zOWzOGa9x5I2"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "This section contains all the necessary setup as hyperparameters, data processing and utility functions"
      ]
    },
    {
      "metadata": {
        "id": "ioySkYZfx7PA"
      },
      "cell_type": "markdown",
      "source": [
        "## Google Colab Setup\n",
        "Since we are running on Google Colab, we will need to install PyTorch as they only support TensorFlow by default, because, well, they are Google and not Facebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch 0.4.1 with CUDA 9.2 backend"
      ],
      "metadata": {
        "id": "Vxv2qOVp1KVB"
      }
    },
    {
      "metadata": {
        "id": "qQoHVrVgR8lh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18288cff-fbcc-4098-d234-6a997b638d79"
      },
      "cell_type": "code",
      "source": [
        " # http://pytorch.org/\n",
        "from os.path import exists\n",
        "# !pip install wheel==0.34.2\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "# !pip uninstall torch -y\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "# import torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -orch (/usr/local/lib/python3.7/dist-packages)\u001b[0m\n",
            "\u001b[31m  ERROR: HTTP error 403 while getting http://download.pytorch.org/whl/cu110/torch-0.4.1-cp37-cp37m-linux_x86_64.whl\u001b[0m\n",
            "\u001b[31mERROR: Could not install requirement torch==0.4.1 from http://download.pytorch.org/whl/cu110/torch-0.4.1-cp37-cp37m-linux_x86_64.whl because of HTTP error 403 Client Error: Forbidden for url: http://download.pytorch.org/whl/cu110/torch-0.4.1-cp37-cp37m-linux_x86_64.whl for URL http://download.pytorch.org/whl/cu110/torch-0.4.1-cp37-cp37m-linux_x86_64.whl\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "JgKFkeoAigxv"
      },
      "cell_type": "markdown",
      "source": [
        "We will need some data to train on, and a place to save our model. \n",
        "We connect to google drive and position our data in the following path: *MyDrive/NLP/data/penn/*"
      ]
    },
    {
      "metadata": {
        "id": "pImz6t2Ligxw"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9zQbaNkLh3FN"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports and params\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mtulgShphXWT"
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZmumaoCEh61G"
      },
      "cell_type": "code",
      "source": [
        "args_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "args_train_batch_size = 20 # batch size\n",
        "args_bptt = 70 # sequence length\n",
        "args_embed_size = 400 # emsize\n",
        "args_hidden_size = 1150 # nhid\n",
        "args_num_layers = 3 # nlayers\n",
        "args_num_epochs = 750\n",
        "args_learning_rate = 30\n",
        "args_dropout = 0.4\n",
        "args_dropouth = 0.25\n",
        "args_dropouti = 0.4\n",
        "args_dropoute = 0.1\n",
        "args_clip = 0.25\n",
        "args_log_interval = 100\n",
        "\n",
        "# if you dont already have the penn treebank data, grab it from our github repo\n",
        "# here: https://github.com/mikkelbrusen/awd-inspired-lstm\n",
        "args_data = \"/content/gdrive/My Drive/NLP/data/penn/\"\n",
        "\n",
        "# The file in which we want to save our trained model.\n",
        "args_save = \"/content/gdrive/My Drive/NLP/save/AWD_LSTM_Model.pt\"\n",
        "\n",
        "args_seed = 141\n",
        "args_nonmono = 5\n",
        "args_wdrop = 0.5\n",
        "args_tie_weights = True\n",
        "\n",
        "np.random.seed(args_seed)\n",
        "torch.manual_seed(args_seed)\n",
        "\n",
        "if args_cuda:\n",
        "  torch.cuda.manual_seed(args_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qA3L-WvsSHfi"
      },
      "cell_type": "markdown",
      "source": [
        "## The data loader\n",
        "Dictionary and corpus to process the dataset"
      ]
    },
    {
      "metadata": {
        "id": "KHjT2i8oQ9xv"
      },
      "cell_type": "code",
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = Counter()\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        token_id = self.word2idx[word]\n",
        "        self.counter[token_id] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "\n",
        "        return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z9wEHvYcgnPO"
      },
      "cell_type": "markdown",
      "source": [
        "## Utils\n",
        "Utility functions which will be used while training, validating and testing"
      ]
    },
    {
      "metadata": {
        "id": "Dl_pQuvugmWW"
      },
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length args.bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "  \n",
        "def get_batch(source, i):\n",
        "    seq_len = min(args_bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "  \n",
        "def model_save(fn):\n",
        "    with open(fn, 'wb') as f:\n",
        "        torch.save([model, criterion, optimizer], f)\n",
        "        \n",
        "def model_load(fn):\n",
        "    global model, criterion, optimizer\n",
        "    with open(fn, 'rb') as f:\n",
        "        model, criterion, optimizer = torch.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TxW3poQSyrQm"
      },
      "cell_type": "markdown",
      "source": [
        "## Process data\n",
        "Load the dataset and make train, validaiton and test sets"
      ]
    },
    {
      "metadata": {
        "id": "8jM-ww1fh5Vz"
      },
      "cell_type": "code",
      "source": [
        "# Load \"Penn Treebank\" dataset\n",
        "corpus = Corpus(\"/content/gdrive/My Drive/NLP/data/penn/\")\n",
        "\n",
        "eval_batch_size = 10\n",
        "test_batch_size = 10\n",
        "train_data = batchify(corpus.train, args_train_batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, test_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WC7Xi43vkeGl"
      },
      "cell_type": "markdown",
      "source": [
        "# AWD-LSTM\n",
        "ASGD Weight-Dropped LSTM -> AWD-LSTM\n",
        "\n",
        "https://github.com/salesforce/awd-lstm-lm\n",
        "\n",
        "and article can be found here:\n",
        "\n",
        "https://arxiv.org/abs/1708.02182\n",
        "\n",
        "The functions for WeightDrop, LockDrop, EmbeddingDropout have been taken from their source code.\n",
        "\n",
        "An explanation of these functions can be found here:\n",
        "\n",
        "https://towardsdatascience.com/learning-note-dropout-in-recurrent-networks-part-2-f209222481f8\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "eju446s56I1y"
      },
      "cell_type": "markdown",
      "source": [
        "##Embedding Dropout"
      ]
    },
    {
      "metadata": {
        "id": "LLS2eCxM6MmE"
      },
      "cell_type": "code",
      "source": [
        "def embedded_dropout(embed, words, dropout=0.1):\n",
        "  if dropout:\n",
        "    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n",
        "    masked_embed_weight = mask * embed.weight\n",
        "  else:\n",
        "    masked_embed_weight = embed.weight\n",
        "\n",
        "  padding_idx = embed.padding_idx\n",
        "  if padding_idx is None:\n",
        "      padding_idx = -1\n",
        "\n",
        "  X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
        "    padding_idx, embed.max_norm, embed.norm_type,\n",
        "    embed.scale_grad_by_freq, embed.sparse\n",
        "  )\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6XCz9ypJfpDM"
      },
      "cell_type": "markdown",
      "source": [
        "##DropConnect "
      ]
    },
    {
      "metadata": {
        "id": "XhOSjkSve0Ab"
      },
      "cell_type": "code",
      "source": [
        "from functools import wraps\n",
        "\n",
        "class WeightDrop(torch.nn.Module):\n",
        "    def __init__(self, module, weights, dropout=0):\n",
        "        super(WeightDrop, self).__init__()\n",
        "        self.module = module\n",
        "        self.weights = weights\n",
        "        self.dropout = dropout\n",
        "        self._setup()\n",
        "\n",
        "    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n",
        "        return\n",
        "\n",
        "    def _setup(self):\n",
        "        if issubclass(type(self.module), torch.nn.RNNBase):\n",
        "            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n",
        "\n",
        "        for name_w in self.weights:\n",
        "            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n",
        "            w = getattr(self.module, name_w)\n",
        "            del self.module._parameters[name_w]\n",
        "            self.module.register_parameter(name_w + '_raw', nn.Parameter(w.data))\n",
        "\n",
        "    def _setweights(self):\n",
        "        for name_w in self.weights:\n",
        "            raw_w = getattr(self.module, name_w + '_raw')\n",
        "            w = None\n",
        "            w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n",
        "            setattr(self.module, name_w, w)\n",
        "\n",
        "    def forward(self, *args):\n",
        "        self._setweights()\n",
        "        return self.module.forward(*args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dtvF5O0s1-Sp"
      },
      "cell_type": "markdown",
      "source": [
        "##Locked Dropout"
      ]
    },
    {
      "metadata": {
        "id": "xh6XT3-8xPJp"
      },
      "cell_type": "code",
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x, dropout=0.5):\n",
        "        if not self.training or not dropout:\n",
        "            return x\n",
        "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n",
        "        mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "        mask = mask.expand_as(x)\n",
        "        return mask * x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J0m4i5sNfv3s"
      },
      "cell_type": "markdown",
      "source": [
        "#Create the Model\n",
        "First we define our model"
      ]
    },
    {
      "metadata": {
        "id": "tt_aD-iGkdbz"
      },
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, num_tokens, hidden_size, embed_size, output_size, dropout=0.5, n_layers=1, wdrop=0, dropouth=0.5, dropouti=0.5, dropoute=0.1, tie_weights=False):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.tie_weights = tie_weights\n",
        "        self.lockdrop = LockedDropout()\n",
        "        self.dropouti = dropouti\n",
        "        self.dropouth = dropouth\n",
        "        self.dropoute = dropoute\n",
        "        self.dropout = dropout\n",
        "        self.encoder = nn.Embedding(num_tokens, embed_size)\n",
        "        \n",
        "        #init LSTM layers\n",
        "        self.lstms = []\n",
        "        \n",
        "        for l in range(n_layers):\n",
        "          layer_input_size = embed_size if l == 0 else hidden_size\n",
        "          layer_output_size = hidden_size if l != n_layers-1 else (embed_size if tie_weights else hidden_size)\n",
        "          self.lstms.append(nn.LSTM(layer_input_size, layer_output_size, num_layers=1, dropout=0))\n",
        "        if wdrop:\n",
        "          # Encapsulate lstms in DropConnect class to tap in on their forward() function and drop connections\n",
        "          self.lstms = [WeightDrop(lstm, ['weight_hh_l0'], dropout=wdrop) for lstm in self.lstms]\n",
        "        self.lstms = nn.ModuleList(self.lstms)\n",
        "        \n",
        "        self.decoder = nn.Linear(embed_size if tie_weights else hidden_size, output_size)\n",
        "        \n",
        "        if tie_weights:\n",
        "          #Tie weights\n",
        "          self.decoder.weight = self.encoder.weight\n",
        "          \n",
        "        self.init_weights()\n",
        "       \n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, inp, hidden):\n",
        "      # Do embedding dropout\n",
        "        emb = embedded_dropout(self.encoder, inp, dropout=self.dropoute if self.training else 0)\n",
        "        # Do variational dropout\n",
        "        emb = self.lockdrop(emb, self.dropouti)\n",
        "        \n",
        "        new_hidden = []\n",
        "        outputs = []\n",
        "        output = emb\n",
        "        for i, lstm in enumerate(self.lstms): \n",
        "            output, new_hid = lstm(output, hidden[i])\n",
        "            \n",
        "            new_hidden.append(new_hid)\n",
        "            if i != self.n_layers - 1:\n",
        "              # Do variational dropout\n",
        "              output = self.lockdrop(output, self.dropouth)\n",
        "        \n",
        "        hidden = new_hidden\n",
        "        # Do variational dropout\n",
        "        output = self.lockdrop(output, self.dropout)\n",
        "   \n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        decoded = decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self,bsz):\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        return [(weight.new(1, bsz, self.hidden_size if l != self.n_layers - 1 else (self.embed_size if self.tie_weights else self.hidden_size)).zero_(),\n",
        "                weight.new(1, bsz, self.hidden_size if l != self.n_layers - 1 else (self.embed_size if self.tie_weights else self.hidden_size)).zero_())\n",
        "                for l in range(self.n_layers)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PH3dQqwjiXP5"
      },
      "cell_type": "markdown",
      "source": [
        "Then we build the model and specify our loss function"
      ]
    },
    {
      "metadata": {
        "id": "CAAf3g95JigU"
      },
      "cell_type": "code",
      "source": [
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "model = LSTMModel(ntokens, args_hidden_size, args_embed_size, ntokens, args_dropout, args_num_layers, args_wdrop, args_dropouth, args_dropouti, args_dropoute, args_tie_weights).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Print number of parameters for comparison with other language models\n",
        "params = list(model.parameters()) + list(criterion.parameters())\n",
        "total_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in params if x.size())\n",
        "print('Model total parameters:', total_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a-TQgIkBn8_B"
      },
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "\n",
        "First we define our training and evalutation"
      ]
    },
    {
      "metadata": {
        "id": "D8e1jsOdn_wj"
      },
      "cell_type": "code",
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(test_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, args_bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(args_train_batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, args_bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        optimizer.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), args_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % args_log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / args_log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // args_bptt, lr,\n",
        "                elapsed * 1000 / args_log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mXCZzJmHkRFy"
      },
      "cell_type": "markdown",
      "source": [
        "Then do the actual training"
      ]
    },
    {
      "metadata": {
        "id": "IXolA9BNj9JG",
        "outputId": "fd49149f-bb86-4d90-89e3-bccfe4b81aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21644
        }
      },
      "cell_type": "code",
      "source": [
        "# Loop over epochs.\n",
        "lr = args_learning_rate\n",
        "best_val_loss = 100000000\n",
        "stored_losses = []\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    for epoch in range(1, args_num_epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train() \n",
        "        if 't0' in optimizer.param_groups[0]:\n",
        "            tmp = {}\n",
        "            for prm in model.parameters():\n",
        "                tmp[prm] = prm.data.clone()\n",
        "                prm.data = optimizer.state[prm]['ax'].clone()\n",
        "\n",
        "            val_loss2 = evaluate(val_data)\n",
        "            print('-' * 89)\n",
        "            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f} | valid bpc {:8.3f}'.format(\n",
        "                    epoch, (time.time() - epoch_start_time), val_loss2, math.exp(val_loss2), val_loss2 / math.log(2)))\n",
        "            print('-' * 89)\n",
        "\n",
        "            if val_loss2 < best_val_loss:\n",
        "                model_save(args_save)\n",
        "                best_val_loss = val_loss\n",
        "\n",
        "            for prm in model.parameters():\n",
        "                prm.data = tmp[prm].clone()\n",
        "\n",
        "        else:\n",
        "            val_loss = evaluate(val_data)\n",
        "            print('-' * 89)\n",
        "            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                    'valid ppl {:8.2f}'.format(\n",
        "                        epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss)))\n",
        "            print('-' * 89)\n",
        "            # Save the model if the validation loss is the best we've seen so far.\n",
        "            if val_loss < best_val_loss:\n",
        "                model_save(args_save)\n",
        "                best_val_loss = val_loss\n",
        "            \n",
        "            elif len(stored_losses) > args_nonmono and val_loss > min(stored_losses[:-args_nonmono]):\n",
        "                print('Switching to ASGD')\n",
        "                optimizer = torch.optim.ASGD(model.parameters(), lr=lr, t0=0, lambd=0.)\n",
        "\n",
        "            stored_losses.append(val_loss)\n",
        "               \n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
            "  result = self.forward(*input, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   100/  663 batches | lr 30.00 | ms/batch 279.10 | loss  7.58 | ppl  1954.87\n",
            "| epoch   1 |   200/  663 batches | lr 30.00 | ms/batch 275.48 | loss  6.71 | ppl   818.22\n",
            "| epoch   1 |   300/  663 batches | lr 30.00 | ms/batch 275.57 | loss  6.49 | ppl   661.46\n",
            "| epoch   1 |   400/  663 batches | lr 30.00 | ms/batch 275.75 | loss  6.26 | ppl   520.77\n",
            "| epoch   1 |   500/  663 batches | lr 30.00 | ms/batch 276.15 | loss  6.10 | ppl   444.75\n",
            "| epoch   1 |   600/  663 batches | lr 30.00 | ms/batch 275.70 | loss  5.95 | ppl   382.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 192.70s | valid loss  5.74 | valid ppl   312.41\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LSTMModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type LockedDropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type WeightDrop. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   100/  663 batches | lr 30.00 | ms/batch 278.65 | loss  5.87 | ppl   354.77\n",
            "| epoch   2 |   200/  663 batches | lr 30.00 | ms/batch 276.11 | loss  5.75 | ppl   313.99\n",
            "| epoch   2 |   300/  663 batches | lr 30.00 | ms/batch 275.69 | loss  5.71 | ppl   302.87\n",
            "| epoch   2 |   400/  663 batches | lr 30.00 | ms/batch 276.12 | loss  5.60 | ppl   271.39\n",
            "| epoch   2 |   500/  663 batches | lr 30.00 | ms/batch 276.14 | loss  5.55 | ppl   257.65\n",
            "| epoch   2 |   600/  663 batches | lr 30.00 | ms/batch 276.14 | loss  5.47 | ppl   237.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 192.82s | valid loss  5.31 | valid ppl   201.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/  663 batches | lr 30.00 | ms/batch 279.06 | loss  5.50 | ppl   243.65\n",
            "| epoch   3 |   200/  663 batches | lr 30.00 | ms/batch 276.11 | loss  5.43 | ppl   227.37\n",
            "| epoch   3 |   300/  663 batches | lr 30.00 | ms/batch 276.21 | loss  5.39 | ppl   219.90\n",
            "| epoch   3 |   400/  663 batches | lr 30.00 | ms/batch 276.09 | loss  5.33 | ppl   206.89\n",
            "| epoch   3 |   500/  663 batches | lr 30.00 | ms/batch 275.86 | loss  5.31 | ppl   203.17\n",
            "| epoch   3 |   600/  663 batches | lr 30.00 | ms/batch 276.25 | loss  5.25 | ppl   190.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 192.89s | valid loss  5.10 | valid ppl   164.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/  663 batches | lr 30.00 | ms/batch 278.80 | loss  5.29 | ppl   198.28\n",
            "| epoch   4 |   200/  663 batches | lr 30.00 | ms/batch 276.16 | loss  5.24 | ppl   187.75\n",
            "| epoch   4 |   300/  663 batches | lr 30.00 | ms/batch 276.15 | loss  5.22 | ppl   184.61\n",
            "| epoch   4 |   400/  663 batches | lr 30.00 | ms/batch 276.05 | loss  5.17 | ppl   175.29\n",
            "| epoch   4 |   500/  663 batches | lr 30.00 | ms/batch 276.05 | loss  5.17 | ppl   175.31\n",
            "| epoch   4 |   600/  663 batches | lr 30.00 | ms/batch 276.08 | loss  5.09 | ppl   162.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 192.88s | valid loss  4.95 | valid ppl   141.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   100/  663 batches | lr 30.00 | ms/batch 278.67 | loss  5.15 | ppl   171.86\n",
            "| epoch   5 |   200/  663 batches | lr 30.00 | ms/batch 276.13 | loss  5.10 | ppl   164.49\n",
            "| epoch   5 |   300/  663 batches | lr 30.00 | ms/batch 275.89 | loss  5.10 | ppl   164.38\n",
            "| epoch   5 |   400/  663 batches | lr 30.00 | ms/batch 275.98 | loss  5.04 | ppl   155.24\n",
            "| epoch   5 |   500/  663 batches | lr 30.00 | ms/batch 276.00 | loss  5.07 | ppl   158.77\n",
            "| epoch   5 |   600/  663 batches | lr 30.00 | ms/batch 276.06 | loss  4.98 | ppl   145.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 192.79s | valid loss  4.85 | valid ppl   128.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   100/  663 batches | lr 30.00 | ms/batch 278.73 | loss  5.05 | ppl   155.33\n",
            "| epoch   6 |   200/  663 batches | lr 30.00 | ms/batch 275.83 | loss  5.01 | ppl   150.03\n",
            "| epoch   6 |   300/  663 batches | lr 30.00 | ms/batch 276.17 | loss  4.99 | ppl   147.64\n",
            "| epoch   6 |   400/  663 batches | lr 30.00 | ms/batch 276.13 | loss  4.95 | ppl   140.67\n",
            "| epoch   6 |   500/  663 batches | lr 30.00 | ms/batch 275.87 | loss  4.98 | ppl   145.20\n",
            "| epoch   6 |   600/  663 batches | lr 30.00 | ms/batch 276.15 | loss  4.88 | ppl   131.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 192.80s | valid loss  4.78 | valid ppl   119.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   100/  663 batches | lr 30.00 | ms/batch 278.89 | loss  4.95 | ppl   141.29\n",
            "| epoch   7 |   200/  663 batches | lr 30.00 | ms/batch 276.17 | loss  4.92 | ppl   137.65\n",
            "| epoch   7 |   300/  663 batches | lr 30.00 | ms/batch 276.17 | loss  4.92 | ppl   136.59\n",
            "| epoch   7 |   400/  663 batches | lr 30.00 | ms/batch 276.16 | loss  4.87 | ppl   130.85\n",
            "| epoch   7 |   500/  663 batches | lr 30.00 | ms/batch 276.16 | loss  4.92 | ppl   136.81\n",
            "| epoch   7 |   600/  663 batches | lr 30.00 | ms/batch 276.16 | loss  4.81 | ppl   122.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 192.92s | valid loss  4.72 | valid ppl   112.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   100/  663 batches | lr 30.00 | ms/batch 278.71 | loss  4.89 | ppl   133.33\n",
            "| epoch   8 |   200/  663 batches | lr 30.00 | ms/batch 276.11 | loss  4.85 | ppl   128.09\n",
            "| epoch   8 |   300/  663 batches | lr 30.00 | ms/batch 276.16 | loss  4.85 | ppl   127.55\n",
            "| epoch   8 |   400/  663 batches | lr 30.00 | ms/batch 276.20 | loss  4.80 | ppl   121.48\n",
            "| epoch   8 |   500/  663 batches | lr 30.00 | ms/batch 276.19 | loss  4.84 | ppl   126.96\n",
            "| epoch   8 |   600/  663 batches | lr 30.00 | ms/batch 276.16 | loss  4.74 | ppl   114.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 192.87s | valid loss  4.67 | valid ppl   106.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   100/  663 batches | lr 30.00 | ms/batch 278.89 | loss  4.83 | ppl   125.39\n",
            "| epoch   9 |   200/  663 batches | lr 30.00 | ms/batch 276.20 | loss  4.79 | ppl   120.56\n",
            "| epoch   9 |   300/  663 batches | lr 30.00 | ms/batch 276.14 | loss  4.78 | ppl   119.42\n",
            "| epoch   9 |   400/  663 batches | lr 30.00 | ms/batch 276.24 | loss  4.73 | ppl   113.57\n",
            "| epoch   9 |   500/  663 batches | lr 30.00 | ms/batch 276.33 | loss  4.79 | ppl   120.84\n",
            "| epoch   9 |   600/  663 batches | lr 30.00 | ms/batch 276.47 | loss  4.68 | ppl   107.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 192.98s | valid loss  4.64 | valid ppl   104.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   100/  663 batches | lr 30.00 | ms/batch 278.87 | loss  4.77 | ppl   118.23\n",
            "| epoch  10 |   200/  663 batches | lr 30.00 | ms/batch 276.41 | loss  4.75 | ppl   115.91\n",
            "| epoch  10 |   300/  663 batches | lr 30.00 | ms/batch 276.07 | loss  4.74 | ppl   113.93\n",
            "| epoch  10 |   400/  663 batches | lr 30.00 | ms/batch 276.32 | loss  4.69 | ppl   108.82\n",
            "| epoch  10 |   500/  663 batches | lr 30.00 | ms/batch 276.08 | loss  4.76 | ppl   116.43\n",
            "| epoch  10 |   600/  663 batches | lr 30.00 | ms/batch 276.51 | loss  4.65 | ppl   104.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 192.97s | valid loss  4.61 | valid ppl   100.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   100/  663 batches | lr 30.00 | ms/batch 278.96 | loss  4.71 | ppl   111.33\n",
            "| epoch  11 |   200/  663 batches | lr 30.00 | ms/batch 276.08 | loss  4.71 | ppl   110.92\n",
            "| epoch  11 |   300/  663 batches | lr 30.00 | ms/batch 276.13 | loss  4.69 | ppl   108.75\n",
            "| epoch  11 |   400/  663 batches | lr 30.00 | ms/batch 276.21 | loss  4.64 | ppl   103.74\n",
            "| epoch  11 |   500/  663 batches | lr 30.00 | ms/batch 276.13 | loss  4.70 | ppl   110.23\n",
            "| epoch  11 |   600/  663 batches | lr 30.00 | ms/batch 276.11 | loss  4.59 | ppl    98.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 192.91s | valid loss  4.58 | valid ppl    97.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   100/  663 batches | lr 30.00 | ms/batch 279.05 | loss  4.69 | ppl   108.67\n",
            "| epoch  12 |   200/  663 batches | lr 30.00 | ms/batch 276.27 | loss  4.66 | ppl   105.74\n",
            "| epoch  12 |   300/  663 batches | lr 30.00 | ms/batch 276.32 | loss  4.66 | ppl   105.27\n",
            "| epoch  12 |   400/  663 batches | lr 30.00 | ms/batch 276.36 | loss  4.61 | ppl   100.19\n",
            "| epoch  12 |   500/  663 batches | lr 30.00 | ms/batch 276.15 | loss  4.66 | ppl   106.12\n",
            "| epoch  12 |   600/  663 batches | lr 30.00 | ms/batch 276.31 | loss  4.57 | ppl    96.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 192.99s | valid loss  4.55 | valid ppl    94.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   100/  663 batches | lr 30.00 | ms/batch 279.12 | loss  4.67 | ppl   106.24\n",
            "| epoch  13 |   200/  663 batches | lr 30.00 | ms/batch 276.41 | loss  4.63 | ppl   102.38\n",
            "| epoch  13 |   300/  663 batches | lr 30.00 | ms/batch 276.63 | loss  4.62 | ppl   101.68\n",
            "| epoch  13 |   400/  663 batches | lr 30.00 | ms/batch 276.54 | loss  4.58 | ppl    97.58\n",
            "| epoch  13 |   500/  663 batches | lr 30.00 | ms/batch 276.33 | loss  4.64 | ppl   103.62\n",
            "| epoch  13 |   600/  663 batches | lr 30.00 | ms/batch 276.79 | loss  4.52 | ppl    92.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 193.12s | valid loss  4.53 | valid ppl    92.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   100/  663 batches | lr 30.00 | ms/batch 279.28 | loss  4.62 | ppl   101.93\n",
            "| epoch  14 |   200/  663 batches | lr 30.00 | ms/batch 276.41 | loss  4.61 | ppl   100.52\n",
            "| epoch  14 |   300/  663 batches | lr 30.00 | ms/batch 276.53 | loss  4.60 | ppl    99.80\n",
            "| epoch  14 |   400/  663 batches | lr 30.00 | ms/batch 276.74 | loss  4.55 | ppl    94.35\n",
            "| epoch  14 |   500/  663 batches | lr 30.00 | ms/batch 276.19 | loss  4.61 | ppl   100.72\n",
            "| epoch  14 |   600/  663 batches | lr 30.00 | ms/batch 276.68 | loss  4.50 | ppl    90.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 193.14s | valid loss  4.51 | valid ppl    90.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   100/  663 batches | lr 30.00 | ms/batch 279.52 | loss  4.60 | ppl    99.13\n",
            "| epoch  15 |   200/  663 batches | lr 30.00 | ms/batch 276.48 | loss  4.57 | ppl    96.99\n",
            "| epoch  15 |   300/  663 batches | lr 30.00 | ms/batch 276.77 | loss  4.57 | ppl    96.78\n",
            "| epoch  15 |   400/  663 batches | lr 30.00 | ms/batch 277.01 | loss  4.52 | ppl    91.61\n",
            "| epoch  15 |   500/  663 batches | lr 30.00 | ms/batch 276.61 | loss  4.59 | ppl    98.96\n",
            "| epoch  15 |   600/  663 batches | lr 30.00 | ms/batch 276.95 | loss  4.47 | ppl    87.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 193.32s | valid loss  4.49 | valid ppl    89.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   100/  663 batches | lr 30.00 | ms/batch 279.77 | loss  4.58 | ppl    97.16\n",
            "| epoch  16 |   200/  663 batches | lr 30.00 | ms/batch 276.57 | loss  4.55 | ppl    94.56\n",
            "| epoch  16 |   300/  663 batches | lr 30.00 | ms/batch 276.84 | loss  4.54 | ppl    94.08\n",
            "| epoch  16 |   400/  663 batches | lr 30.00 | ms/batch 277.12 | loss  4.49 | ppl    88.76\n",
            "| epoch  16 |   500/  663 batches | lr 30.00 | ms/batch 276.62 | loss  4.56 | ppl    95.70\n",
            "| epoch  16 |   600/  663 batches | lr 30.00 | ms/batch 276.99 | loss  4.45 | ppl    85.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 193.39s | valid loss  4.49 | valid ppl    88.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   100/  663 batches | lr 30.00 | ms/batch 279.45 | loss  4.56 | ppl    96.03\n",
            "| epoch  17 |   200/  663 batches | lr 30.00 | ms/batch 276.70 | loss  4.52 | ppl    92.17\n",
            "| epoch  17 |   300/  663 batches | lr 30.00 | ms/batch 277.14 | loss  4.51 | ppl    91.06\n",
            "| epoch  17 |   400/  663 batches | lr 30.00 | ms/batch 277.02 | loss  4.47 | ppl    87.40\n",
            "| epoch  17 |   500/  663 batches | lr 30.00 | ms/batch 276.59 | loss  4.54 | ppl    93.84\n",
            "| epoch  17 |   600/  663 batches | lr 30.00 | ms/batch 277.13 | loss  4.44 | ppl    84.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 193.40s | valid loss  4.48 | valid ppl    88.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   100/  663 batches | lr 30.00 | ms/batch 279.68 | loss  4.52 | ppl    92.23\n",
            "| epoch  18 |   200/  663 batches | lr 30.00 | ms/batch 276.93 | loss  4.51 | ppl    90.77\n",
            "| epoch  18 |   300/  663 batches | lr 30.00 | ms/batch 277.07 | loss  4.50 | ppl    90.32\n",
            "| epoch  18 |   400/  663 batches | lr 30.00 | ms/batch 277.01 | loss  4.45 | ppl    85.67\n",
            "| epoch  18 |   500/  663 batches | lr 30.00 | ms/batch 276.53 | loss  4.53 | ppl    92.90\n",
            "| epoch  18 |   600/  663 batches | lr 30.00 | ms/batch 276.27 | loss  4.41 | ppl    82.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 193.17s | valid loss  4.47 | valid ppl    87.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   100/  663 batches | lr 30.00 | ms/batch 277.76 | loss  4.50 | ppl    90.28\n",
            "| epoch  19 |   200/  663 batches | lr 30.00 | ms/batch 274.92 | loss  4.49 | ppl    88.95\n",
            "| epoch  19 |   300/  663 batches | lr 30.00 | ms/batch 274.68 | loss  4.49 | ppl    89.10\n",
            "| epoch  19 |   400/  663 batches | lr 30.00 | ms/batch 274.90 | loss  4.42 | ppl    83.25\n",
            "| epoch  19 |   500/  663 batches | lr 30.00 | ms/batch 274.83 | loss  4.50 | ppl    89.71\n",
            "| epoch  19 |   600/  663 batches | lr 30.00 | ms/batch 274.54 | loss  4.38 | ppl    79.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 191.90s | valid loss  4.45 | valid ppl    85.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   100/  663 batches | lr 30.00 | ms/batch 277.13 | loss  4.49 | ppl    89.32\n",
            "| epoch  20 |   200/  663 batches | lr 30.00 | ms/batch 274.40 | loss  4.47 | ppl    87.02\n",
            "| epoch  20 |   300/  663 batches | lr 30.00 | ms/batch 274.66 | loss  4.46 | ppl    86.78\n",
            "| epoch  20 |   400/  663 batches | lr 30.00 | ms/batch 274.72 | loss  4.41 | ppl    82.09\n",
            "| epoch  20 |   500/  663 batches | lr 30.00 | ms/batch 274.23 | loss  4.49 | ppl    89.36\n",
            "| epoch  20 |   600/  663 batches | lr 30.00 | ms/batch 274.66 | loss  4.37 | ppl    78.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 191.69s | valid loss  4.45 | valid ppl    85.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   100/  663 batches | lr 30.00 | ms/batch 277.25 | loss  4.48 | ppl    88.11\n",
            "| epoch  21 |   200/  663 batches | lr 30.00 | ms/batch 274.75 | loss  4.46 | ppl    86.46\n",
            "| epoch  21 |   300/  663 batches | lr 30.00 | ms/batch 275.28 | loss  4.45 | ppl    86.05\n",
            "| epoch  21 |   400/  663 batches | lr 30.00 | ms/batch 275.40 | loss  4.39 | ppl    80.57\n",
            "| epoch  21 |   500/  663 batches | lr 30.00 | ms/batch 275.93 | loss  4.47 | ppl    87.41\n",
            "| epoch  21 |   600/  663 batches | lr 30.00 | ms/batch 276.18 | loss  4.36 | ppl    78.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 192.32s | valid loss  4.45 | valid ppl    85.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   100/  663 batches | lr 30.00 | ms/batch 278.97 | loss  4.47 | ppl    87.38\n",
            "| epoch  22 |   200/  663 batches | lr 30.00 | ms/batch 276.25 | loss  4.44 | ppl    84.51\n",
            "| epoch  22 |   300/  663 batches | lr 30.00 | ms/batch 276.24 | loss  4.44 | ppl    84.92\n",
            "| epoch  22 |   400/  663 batches | lr 30.00 | ms/batch 276.21 | loss  4.41 | ppl    81.86\n",
            "| epoch  22 |   500/  663 batches | lr 30.00 | ms/batch 276.09 | loss  4.45 | ppl    85.95\n",
            "| epoch  22 |   600/  663 batches | lr 30.00 | ms/batch 276.12 | loss  4.34 | ppl    76.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 192.75s | valid loss  4.43 | valid ppl    84.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   100/  663 batches | lr 30.00 | ms/batch 277.51 | loss  4.45 | ppl    85.49\n",
            "| epoch  23 |   200/  663 batches | lr 30.00 | ms/batch 274.66 | loss  4.43 | ppl    83.73\n",
            "| epoch  23 |   300/  663 batches | lr 30.00 | ms/batch 274.81 | loss  4.43 | ppl    83.90\n",
            "| epoch  23 |   400/  663 batches | lr 30.00 | ms/batch 274.72 | loss  4.38 | ppl    79.50\n",
            "| epoch  23 |   500/  663 batches | lr 30.00 | ms/batch 274.40 | loss  4.44 | ppl    84.96\n",
            "| epoch  23 |   600/  663 batches | lr 30.00 | ms/batch 274.83 | loss  4.33 | ppl    75.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 191.83s | valid loss  4.42 | valid ppl    83.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   100/  663 batches | lr 30.00 | ms/batch 277.48 | loss  4.44 | ppl    84.36\n",
            "| epoch  24 |   200/  663 batches | lr 30.00 | ms/batch 274.76 | loss  4.40 | ppl    81.72\n",
            "| epoch  24 |   300/  663 batches | lr 30.00 | ms/batch 274.88 | loss  4.40 | ppl    81.32\n",
            "| epoch  24 |   400/  663 batches | lr 30.00 | ms/batch 274.86 | loss  4.35 | ppl    77.52\n",
            "| epoch  24 |   500/  663 batches | lr 30.00 | ms/batch 274.84 | loss  4.44 | ppl    84.78\n",
            "| epoch  24 |   600/  663 batches | lr 30.00 | ms/batch 275.13 | loss  4.32 | ppl    75.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 191.92s | valid loss  4.43 | valid ppl    84.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   100/  663 batches | lr 30.00 | ms/batch 277.60 | loss  4.43 | ppl    83.91\n",
            "| epoch  25 |   200/  663 batches | lr 30.00 | ms/batch 275.00 | loss  4.41 | ppl    82.17\n",
            "| epoch  25 |   300/  663 batches | lr 30.00 | ms/batch 274.89 | loss  4.40 | ppl    81.27\n",
            "| epoch  25 |   400/  663 batches | lr 30.00 | ms/batch 274.89 | loss  4.35 | ppl    77.86\n",
            "| epoch  25 |   500/  663 batches | lr 30.00 | ms/batch 274.69 | loss  4.42 | ppl    83.46\n",
            "| epoch  25 |   600/  663 batches | lr 30.00 | ms/batch 274.96 | loss  4.30 | ppl    73.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 191.93s | valid loss  4.42 | valid ppl    82.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   100/  663 batches | lr 30.00 | ms/batch 277.24 | loss  4.42 | ppl    82.87\n",
            "| epoch  26 |   200/  663 batches | lr 30.00 | ms/batch 274.80 | loss  4.39 | ppl    80.66\n",
            "| epoch  26 |   300/  663 batches | lr 30.00 | ms/batch 274.96 | loss  4.39 | ppl    80.46\n",
            "| epoch  26 |   400/  663 batches | lr 30.00 | ms/batch 274.67 | loss  4.35 | ppl    77.10\n",
            "| epoch  26 |   500/  663 batches | lr 30.00 | ms/batch 274.78 | loss  4.41 | ppl    82.34\n",
            "| epoch  26 |   600/  663 batches | lr 30.00 | ms/batch 274.82 | loss  4.30 | ppl    73.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 191.84s | valid loss  4.42 | valid ppl    82.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   100/  663 batches | lr 30.00 | ms/batch 277.66 | loss  4.39 | ppl    80.57\n",
            "| epoch  27 |   200/  663 batches | lr 30.00 | ms/batch 275.08 | loss  4.37 | ppl    78.94\n",
            "| epoch  27 |   300/  663 batches | lr 30.00 | ms/batch 274.78 | loss  4.36 | ppl    78.16\n",
            "| epoch  27 |   400/  663 batches | lr 30.00 | ms/batch 275.15 | loss  4.32 | ppl    75.28\n",
            "| epoch  27 |   500/  663 batches | lr 30.00 | ms/batch 274.83 | loss  4.40 | ppl    81.31\n",
            "| epoch  27 |   600/  663 batches | lr 30.00 | ms/batch 275.06 | loss  4.28 | ppl    72.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 191.99s | valid loss  4.42 | valid ppl    82.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   100/  663 batches | lr 30.00 | ms/batch 277.31 | loss  4.39 | ppl    80.30\n",
            "| epoch  28 |   200/  663 batches | lr 30.00 | ms/batch 274.82 | loss  4.36 | ppl    78.64\n",
            "| epoch  28 |   300/  663 batches | lr 30.00 | ms/batch 274.79 | loss  4.38 | ppl    79.45\n",
            "| epoch  28 |   400/  663 batches | lr 30.00 | ms/batch 274.97 | loss  4.31 | ppl    74.59\n",
            "| epoch  28 |   500/  663 batches | lr 30.00 | ms/batch 274.47 | loss  4.39 | ppl    80.86\n",
            "| epoch  28 |   600/  663 batches | lr 30.00 | ms/batch 274.76 | loss  4.27 | ppl    71.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 191.85s | valid loss  4.42 | valid ppl    82.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   100/  663 batches | lr 30.00 | ms/batch 277.53 | loss  4.38 | ppl    79.76\n",
            "| epoch  29 |   200/  663 batches | lr 30.00 | ms/batch 274.98 | loss  4.34 | ppl    77.07\n",
            "| epoch  29 |   300/  663 batches | lr 30.00 | ms/batch 274.68 | loss  4.36 | ppl    78.38\n",
            "| epoch  29 |   400/  663 batches | lr 30.00 | ms/batch 275.06 | loss  4.31 | ppl    74.49\n",
            "| epoch  29 |   500/  663 batches | lr 30.00 | ms/batch 274.59 | loss  4.37 | ppl    79.05\n",
            "| epoch  29 |   600/  663 batches | lr 30.00 | ms/batch 275.09 | loss  4.26 | ppl    70.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 191.92s | valid loss  4.42 | valid ppl    83.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   100/  663 batches | lr 30.00 | ms/batch 277.04 | loss  4.37 | ppl    79.42\n",
            "| epoch  30 |   200/  663 batches | lr 30.00 | ms/batch 274.54 | loss  4.35 | ppl    77.22\n",
            "| epoch  30 |   300/  663 batches | lr 30.00 | ms/batch 274.69 | loss  4.34 | ppl    77.08\n",
            "| epoch  30 |   400/  663 batches | lr 30.00 | ms/batch 274.54 | loss  4.30 | ppl    73.40\n",
            "| epoch  30 |   500/  663 batches | lr 30.00 | ms/batch 274.53 | loss  4.36 | ppl    78.29\n",
            "| epoch  30 |   600/  663 batches | lr 30.00 | ms/batch 274.51 | loss  4.26 | ppl    70.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 191.69s | valid loss  4.39 | valid ppl    80.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   100/  663 batches | lr 30.00 | ms/batch 277.15 | loss  4.35 | ppl    77.79\n",
            "| epoch  31 |   200/  663 batches | lr 30.00 | ms/batch 274.40 | loss  4.34 | ppl    76.72\n",
            "| epoch  31 |   300/  663 batches | lr 30.00 | ms/batch 274.38 | loss  4.34 | ppl    76.64\n",
            "| epoch  31 |   400/  663 batches | lr 30.00 | ms/batch 274.57 | loss  4.28 | ppl    72.56\n",
            "| epoch  31 |   500/  663 batches | lr 30.00 | ms/batch 274.41 | loss  4.37 | ppl    78.81\n",
            "| epoch  31 |   600/  663 batches | lr 30.00 | ms/batch 274.88 | loss  4.25 | ppl    70.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 191.71s | valid loss  4.40 | valid ppl    81.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   100/  663 batches | lr 30.00 | ms/batch 277.63 | loss  4.34 | ppl    77.07\n",
            "| epoch  32 |   200/  663 batches | lr 30.00 | ms/batch 275.20 | loss  4.33 | ppl    75.75\n",
            "| epoch  32 |   300/  663 batches | lr 30.00 | ms/batch 275.03 | loss  4.34 | ppl    76.87\n",
            "| epoch  32 |   400/  663 batches | lr 30.00 | ms/batch 274.78 | loss  4.28 | ppl    72.38\n",
            "| epoch  32 |   500/  663 batches | lr 30.00 | ms/batch 274.58 | loss  4.35 | ppl    77.73\n",
            "| epoch  32 |   600/  663 batches | lr 30.00 | ms/batch 275.70 | loss  4.24 | ppl    69.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 192.11s | valid loss  4.39 | valid ppl    80.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   100/  663 batches | lr 30.00 | ms/batch 279.28 | loss  4.34 | ppl    76.81\n",
            "| epoch  33 |   200/  663 batches | lr 30.00 | ms/batch 277.09 | loss  4.32 | ppl    75.30\n",
            "| epoch  33 |   300/  663 batches | lr 30.00 | ms/batch 277.31 | loss  4.32 | ppl    75.14\n",
            "| epoch  33 |   400/  663 batches | lr 30.00 | ms/batch 277.59 | loss  4.28 | ppl    72.50\n",
            "| epoch  33 |   500/  663 batches | lr 30.00 | ms/batch 277.51 | loss  4.36 | ppl    78.16\n",
            "| epoch  33 |   600/  663 batches | lr 30.00 | ms/batch 277.82 | loss  4.23 | ppl    68.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 193.71s | valid loss  4.39 | valid ppl    80.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   100/  663 batches | lr 30.00 | ms/batch 280.37 | loss  4.34 | ppl    76.43\n",
            "| epoch  34 |   200/  663 batches | lr 30.00 | ms/batch 277.70 | loss  4.32 | ppl    74.91\n",
            "| epoch  34 |   300/  663 batches | lr 30.00 | ms/batch 277.63 | loss  4.31 | ppl    74.49\n",
            "| epoch  34 |   400/  663 batches | lr 30.00 | ms/batch 277.64 | loss  4.26 | ppl    70.91\n",
            "| epoch  34 |   500/  663 batches | lr 30.00 | ms/batch 277.59 | loss  4.33 | ppl    75.62\n",
            "| epoch  34 |   600/  663 batches | lr 30.00 | ms/batch 277.64 | loss  4.23 | ppl    69.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 193.88s | valid loss  4.40 | valid ppl    81.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   100/  663 batches | lr 30.00 | ms/batch 280.38 | loss  4.33 | ppl    76.30\n",
            "| epoch  35 |   200/  663 batches | lr 30.00 | ms/batch 277.61 | loss  4.30 | ppl    73.69\n",
            "| epoch  35 |   300/  663 batches | lr 30.00 | ms/batch 277.77 | loss  4.31 | ppl    74.07\n",
            "| epoch  35 |   400/  663 batches | lr 30.00 | ms/batch 277.57 | loss  4.28 | ppl    71.97\n",
            "| epoch  35 |   500/  663 batches | lr 30.00 | ms/batch 277.54 | loss  4.33 | ppl    76.00\n",
            "| epoch  35 |   600/  663 batches | lr 30.00 | ms/batch 277.68 | loss  4.23 | ppl    68.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 193.89s | valid loss  4.39 | valid ppl    80.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   100/  663 batches | lr 30.00 | ms/batch 280.33 | loss  4.32 | ppl    75.26\n",
            "| epoch  36 |   200/  663 batches | lr 30.00 | ms/batch 277.46 | loss  4.30 | ppl    73.45\n",
            "| epoch  36 |   300/  663 batches | lr 30.00 | ms/batch 277.55 | loss  4.29 | ppl    72.75\n",
            "| epoch  36 |   400/  663 batches | lr 30.00 | ms/batch 277.54 | loss  4.25 | ppl    70.26\n",
            "| epoch  36 |   500/  663 batches | lr 30.00 | ms/batch 277.42 | loss  4.32 | ppl    75.43\n",
            "| epoch  36 |   600/  663 batches | lr 30.00 | ms/batch 277.56 | loss  4.21 | ppl    67.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 193.81s | valid loss  4.39 | valid ppl    80.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   100/  663 batches | lr 30.00 | ms/batch 280.00 | loss  4.31 | ppl    74.43\n",
            "| epoch  37 |   200/  663 batches | lr 30.00 | ms/batch 277.42 | loss  4.29 | ppl    73.04\n",
            "| epoch  37 |   300/  663 batches | lr 30.00 | ms/batch 277.41 | loss  4.29 | ppl    73.27\n",
            "| epoch  37 |   400/  663 batches | lr 30.00 | ms/batch 277.52 | loss  4.25 | ppl    69.80\n",
            "| epoch  37 |   500/  663 batches | lr 30.00 | ms/batch 277.48 | loss  4.32 | ppl    75.18\n",
            "| epoch  37 |   600/  663 batches | lr 30.00 | ms/batch 277.60 | loss  4.20 | ppl    66.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 193.78s | valid loss  4.38 | valid ppl    79.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   100/  663 batches | lr 30.00 | ms/batch 280.01 | loss  4.32 | ppl    75.19\n",
            "| epoch  38 |   200/  663 batches | lr 30.00 | ms/batch 277.47 | loss  4.29 | ppl    72.76\n",
            "| epoch  38 |   300/  663 batches | lr 30.00 | ms/batch 277.48 | loss  4.29 | ppl    72.73\n",
            "| epoch  38 |   400/  663 batches | lr 30.00 | ms/batch 277.43 | loss  4.26 | ppl    70.46\n",
            "| epoch  38 |   500/  663 batches | lr 30.00 | ms/batch 277.45 | loss  4.32 | ppl    75.33\n",
            "| epoch  38 |   600/  663 batches | lr 30.00 | ms/batch 277.40 | loss  4.21 | ppl    67.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 193.75s | valid loss  4.38 | valid ppl    79.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   100/  663 batches | lr 30.00 | ms/batch 280.11 | loss  4.30 | ppl    74.03\n",
            "| epoch  39 |   200/  663 batches | lr 30.00 | ms/batch 277.62 | loss  4.28 | ppl    72.28\n",
            "| epoch  39 |   300/  663 batches | lr 30.00 | ms/batch 277.69 | loss  4.28 | ppl    71.99\n",
            "| epoch  39 |   400/  663 batches | lr 30.00 | ms/batch 277.57 | loss  4.23 | ppl    68.96\n",
            "| epoch  39 |   500/  663 batches | lr 30.00 | ms/batch 277.54 | loss  4.31 | ppl    74.23\n",
            "| epoch  39 |   600/  663 batches | lr 30.00 | ms/batch 277.54 | loss  4.20 | ppl    66.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 193.84s | valid loss  4.38 | valid ppl    79.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   100/  663 batches | lr 30.00 | ms/batch 280.31 | loss  4.31 | ppl    74.60\n",
            "| epoch  40 |   200/  663 batches | lr 30.00 | ms/batch 277.66 | loss  4.27 | ppl    71.85\n",
            "| epoch  40 |   300/  663 batches | lr 30.00 | ms/batch 277.47 | loss  4.27 | ppl    71.69\n",
            "| epoch  40 |   400/  663 batches | lr 30.00 | ms/batch 277.57 | loss  4.24 | ppl    69.24\n",
            "| epoch  40 |   500/  663 batches | lr 30.00 | ms/batch 277.70 | loss  4.30 | ppl    73.45\n",
            "| epoch  40 |   600/  663 batches | lr 30.00 | ms/batch 277.60 | loss  4.19 | ppl    65.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 193.86s | valid loss  4.38 | valid ppl    79.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   100/  663 batches | lr 30.00 | ms/batch 280.11 | loss  4.30 | ppl    73.79\n",
            "| epoch  41 |   200/  663 batches | lr 30.00 | ms/batch 277.43 | loss  4.27 | ppl    71.28\n",
            "| epoch  41 |   300/  663 batches | lr 30.00 | ms/batch 277.44 | loss  4.27 | ppl    71.83\n",
            "| epoch  41 |   400/  663 batches | lr 30.00 | ms/batch 277.53 | loss  4.23 | ppl    68.38\n",
            "| epoch  41 |   500/  663 batches | lr 30.00 | ms/batch 277.44 | loss  4.30 | ppl    73.77\n",
            "| epoch  41 |   600/  663 batches | lr 30.00 | ms/batch 277.59 | loss  4.20 | ppl    66.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time: 193.77s | valid loss  4.37 | valid ppl    78.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |   100/  663 batches | lr 30.00 | ms/batch 280.27 | loss  4.30 | ppl    73.67\n",
            "| epoch  42 |   200/  663 batches | lr 30.00 | ms/batch 277.77 | loss  4.25 | ppl    70.42\n",
            "| epoch  42 |   300/  663 batches | lr 30.00 | ms/batch 277.60 | loss  4.25 | ppl    70.11\n",
            "| epoch  42 |   400/  663 batches | lr 30.00 | ms/batch 277.63 | loss  4.23 | ppl    68.79\n",
            "| epoch  42 |   500/  663 batches | lr 30.00 | ms/batch 277.45 | loss  4.30 | ppl    73.91\n",
            "| epoch  42 |   600/  663 batches | lr 30.00 | ms/batch 277.64 | loss  4.19 | ppl    65.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time: 193.86s | valid loss  4.36 | valid ppl    78.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   100/  663 batches | lr 30.00 | ms/batch 280.37 | loss  4.29 | ppl    72.71\n",
            "| epoch  43 |   200/  663 batches | lr 30.00 | ms/batch 277.58 | loss  4.26 | ppl    70.85\n",
            "| epoch  43 |   300/  663 batches | lr 30.00 | ms/batch 277.61 | loss  4.27 | ppl    71.36\n",
            "| epoch  43 |   400/  663 batches | lr 30.00 | ms/batch 277.50 | loss  4.22 | ppl    67.84\n",
            "| epoch  43 |   500/  663 batches | lr 30.00 | ms/batch 277.33 | loss  4.28 | ppl    72.35\n",
            "| epoch  43 |   600/  663 batches | lr 30.00 | ms/batch 277.53 | loss  4.17 | ppl    64.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time: 193.82s | valid loss  4.37 | valid ppl    78.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   100/  663 batches | lr 30.00 | ms/batch 280.33 | loss  4.27 | ppl    71.84\n",
            "| epoch  44 |   200/  663 batches | lr 30.00 | ms/batch 277.46 | loss  4.26 | ppl    70.83\n",
            "| epoch  44 |   300/  663 batches | lr 30.00 | ms/batch 277.56 | loss  4.25 | ppl    70.37\n",
            "| epoch  44 |   400/  663 batches | lr 30.00 | ms/batch 277.57 | loss  4.20 | ppl    66.64\n",
            "| epoch  44 |   500/  663 batches | lr 30.00 | ms/batch 277.59 | loss  4.28 | ppl    72.41\n",
            "| epoch  44 |   600/  663 batches | lr 30.00 | ms/batch 277.51 | loss  4.16 | ppl    64.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time: 193.85s | valid loss  4.38 | valid ppl    79.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "Switching to ASGD\n",
            "| epoch  45 |   100/  663 batches | lr 30.00 | ms/batch 286.68 | loss  4.26 | ppl    70.84\n",
            "| epoch  45 |   200/  663 batches | lr 30.00 | ms/batch 284.01 | loss  4.25 | ppl    69.81\n",
            "| epoch  45 |   300/  663 batches | lr 30.00 | ms/batch 284.30 | loss  4.25 | ppl    70.17\n",
            "| epoch  45 |   400/  663 batches | lr 30.00 | ms/batch 284.28 | loss  4.20 | ppl    66.94\n",
            "| epoch  45 |   500/  663 batches | lr 30.00 | ms/batch 284.19 | loss  4.27 | ppl    71.46\n",
            "| epoch  45 |   600/  663 batches | lr 30.00 | ms/batch 283.96 | loss  4.17 | ppl    64.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time: 198.18s | valid loss  4.31 | valid ppl    74.61 | valid bpc    6.221\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |   100/  663 batches | lr 30.00 | ms/batch 286.84 | loss  4.28 | ppl    72.03\n",
            "| epoch  46 |   200/  663 batches | lr 30.00 | ms/batch 284.05 | loss  4.25 | ppl    70.46\n",
            "| epoch  46 |   300/  663 batches | lr 30.00 | ms/batch 284.13 | loss  4.24 | ppl    69.25\n",
            "| epoch  46 |   400/  663 batches | lr 30.00 | ms/batch 283.99 | loss  4.21 | ppl    67.52\n",
            "| epoch  46 |   500/  663 batches | lr 30.00 | ms/batch 284.02 | loss  4.27 | ppl    71.63\n",
            "| epoch  46 |   600/  663 batches | lr 30.00 | ms/batch 284.03 | loss  4.18 | ppl    65.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time: 198.14s | valid loss  4.31 | valid ppl    74.26 | valid bpc    6.215\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |   100/  663 batches | lr 30.00 | ms/batch 286.62 | loss  4.26 | ppl    70.96\n",
            "| epoch  47 |   200/  663 batches | lr 30.00 | ms/batch 283.99 | loss  4.22 | ppl    68.17\n",
            "| epoch  47 |   300/  663 batches | lr 30.00 | ms/batch 284.09 | loss  4.23 | ppl    68.74\n",
            "| epoch  47 |   400/  663 batches | lr 30.00 | ms/batch 283.92 | loss  4.19 | ppl    65.84\n",
            "| epoch  47 |   500/  663 batches | lr 30.00 | ms/batch 283.94 | loss  4.28 | ppl    72.06\n",
            "| epoch  47 |   600/  663 batches | lr 30.00 | ms/batch 283.73 | loss  4.16 | ppl    64.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time: 198.07s | valid loss  4.31 | valid ppl    74.07 | valid bpc    6.211\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |   100/  663 batches | lr 30.00 | ms/batch 287.01 | loss  4.25 | ppl    69.78\n",
            "| epoch  48 |   200/  663 batches | lr 30.00 | ms/batch 284.04 | loss  4.21 | ppl    67.68\n",
            "| epoch  48 |   300/  663 batches | lr 30.00 | ms/batch 284.17 | loss  4.22 | ppl    67.74\n",
            "| epoch  48 |   400/  663 batches | lr 30.00 | ms/batch 284.06 | loss  4.19 | ppl    66.25\n",
            "| epoch  48 |   500/  663 batches | lr 30.00 | ms/batch 284.11 | loss  4.26 | ppl    70.95\n",
            "| epoch  48 |   600/  663 batches | lr 30.00 | ms/batch 284.01 | loss  4.15 | ppl    63.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time: 198.18s | valid loss  4.30 | valid ppl    73.90 | valid bpc    6.208\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |   100/  663 batches | lr 30.00 | ms/batch 286.89 | loss  4.25 | ppl    70.30\n",
            "| epoch  49 |   200/  663 batches | lr 30.00 | ms/batch 284.23 | loss  4.23 | ppl    68.68\n",
            "| epoch  49 |   300/  663 batches | lr 30.00 | ms/batch 284.40 | loss  4.22 | ppl    68.10\n",
            "| epoch  49 |   400/  663 batches | lr 30.00 | ms/batch 284.40 | loss  4.19 | ppl    65.89\n",
            "| epoch  49 |   500/  663 batches | lr 30.00 | ms/batch 284.26 | loss  4.25 | ppl    70.28\n",
            "| epoch  49 |   600/  663 batches | lr 30.00 | ms/batch 284.21 | loss  4.15 | ppl    63.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time: 198.28s | valid loss  4.30 | valid ppl    73.74 | valid bpc    6.204\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |   100/  663 batches | lr 30.00 | ms/batch 286.96 | loss  4.25 | ppl    70.09\n",
            "| epoch  50 |   200/  663 batches | lr 30.00 | ms/batch 284.08 | loss  4.23 | ppl    68.58\n",
            "| epoch  50 |   300/  663 batches | lr 30.00 | ms/batch 283.93 | loss  4.22 | ppl    68.20\n",
            "| epoch  50 |   400/  663 batches | lr 30.00 | ms/batch 284.18 | loss  4.18 | ppl    65.38\n",
            "| epoch  50 |   500/  663 batches | lr 30.00 | ms/batch 283.99 | loss  4.25 | ppl    70.04\n",
            "| epoch  50 |   600/  663 batches | lr 30.00 | ms/batch 284.15 | loss  4.15 | ppl    63.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time: 198.19s | valid loss  4.30 | valid ppl    73.61 | valid bpc    6.202\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  51 |   100/  663 batches | lr 30.00 | ms/batch 286.81 | loss  4.23 | ppl    69.05\n",
            "| epoch  51 |   200/  663 batches | lr 30.00 | ms/batch 284.16 | loss  4.23 | ppl    68.39\n",
            "| epoch  51 |   300/  663 batches | lr 30.00 | ms/batch 284.07 | loss  4.22 | ppl    67.96\n",
            "| epoch  51 |   400/  663 batches | lr 30.00 | ms/batch 284.28 | loss  4.17 | ppl    64.96\n",
            "| epoch  51 |   500/  663 batches | lr 30.00 | ms/batch 284.21 | loss  4.27 | ppl    71.23\n",
            "| epoch  51 |   600/  663 batches | lr 30.00 | ms/batch 283.98 | loss  4.13 | ppl    62.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time: 198.19s | valid loss  4.30 | valid ppl    73.48 | valid bpc    6.199\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  52 |   100/  663 batches | lr 30.00 | ms/batch 286.77 | loss  4.25 | ppl    70.17\n",
            "| epoch  52 |   200/  663 batches | lr 30.00 | ms/batch 284.38 | loss  4.22 | ppl    67.84\n",
            "| epoch  52 |   300/  663 batches | lr 30.00 | ms/batch 284.41 | loss  4.22 | ppl    67.91\n",
            "| epoch  52 |   400/  663 batches | lr 30.00 | ms/batch 284.31 | loss  4.18 | ppl    65.23\n",
            "| epoch  52 |   500/  663 batches | lr 30.00 | ms/batch 284.22 | loss  4.25 | ppl    70.15\n",
            "| epoch  52 |   600/  663 batches | lr 30.00 | ms/batch 284.17 | loss  4.13 | ppl    61.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time: 198.29s | valid loss  4.30 | valid ppl    73.35 | valid bpc    6.197\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  53 |   100/  663 batches | lr 30.00 | ms/batch 287.04 | loss  4.24 | ppl    69.38\n",
            "| epoch  53 |   200/  663 batches | lr 30.00 | ms/batch 283.98 | loss  4.21 | ppl    67.13\n",
            "| epoch  53 |   300/  663 batches | lr 30.00 | ms/batch 284.14 | loss  4.21 | ppl    67.54\n",
            "| epoch  53 |   400/  663 batches | lr 30.00 | ms/batch 284.10 | loss  4.18 | ppl    65.25\n",
            "| epoch  53 |   500/  663 batches | lr 30.00 | ms/batch 283.96 | loss  4.25 | ppl    70.22\n",
            "| epoch  53 |   600/  663 batches | lr 30.00 | ms/batch 283.98 | loss  4.13 | ppl    62.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time: 198.18s | valid loss  4.29 | valid ppl    73.21 | valid bpc    6.194\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  54 |   100/  663 batches | lr 30.00 | ms/batch 286.71 | loss  4.23 | ppl    68.55\n",
            "| epoch  54 |   200/  663 batches | lr 30.00 | ms/batch 284.26 | loss  4.21 | ppl    67.20\n",
            "| epoch  54 |   300/  663 batches | lr 30.00 | ms/batch 284.33 | loss  4.20 | ppl    66.39\n",
            "| epoch  54 |   400/  663 batches | lr 30.00 | ms/batch 284.14 | loss  4.18 | ppl    65.59\n",
            "| epoch  54 |   500/  663 batches | lr 30.00 | ms/batch 284.15 | loss  4.24 | ppl    69.14\n",
            "| epoch  54 |   600/  663 batches | lr 30.00 | ms/batch 284.20 | loss  4.13 | ppl    62.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time: 198.22s | valid loss  4.29 | valid ppl    73.10 | valid bpc    6.192\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  55 |   100/  663 batches | lr 30.00 | ms/batch 286.80 | loss  4.24 | ppl    69.39\n",
            "| epoch  55 |   200/  663 batches | lr 30.00 | ms/batch 284.27 | loss  4.20 | ppl    67.01\n",
            "| epoch  55 |   300/  663 batches | lr 30.00 | ms/batch 284.10 | loss  4.20 | ppl    66.80\n",
            "| epoch  55 |   400/  663 batches | lr 30.00 | ms/batch 283.83 | loss  4.16 | ppl    64.26\n",
            "| epoch  55 |   500/  663 batches | lr 30.00 | ms/batch 283.80 | loss  4.24 | ppl    69.26\n",
            "| epoch  55 |   600/  663 batches | lr 30.00 | ms/batch 283.92 | loss  4.13 | ppl    62.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time: 198.10s | valid loss  4.29 | valid ppl    72.99 | valid bpc    6.190\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  56 |   100/  663 batches | lr 30.00 | ms/batch 286.61 | loss  4.25 | ppl    70.12\n",
            "| epoch  56 |   200/  663 batches | lr 30.00 | ms/batch 283.74 | loss  4.20 | ppl    66.73\n",
            "| epoch  56 |   300/  663 batches | lr 30.00 | ms/batch 283.76 | loss  4.19 | ppl    65.91\n",
            "| epoch  56 |   400/  663 batches | lr 30.00 | ms/batch 283.74 | loss  4.16 | ppl    64.37\n",
            "| epoch  56 |   500/  663 batches | lr 30.00 | ms/batch 283.59 | loss  4.23 | ppl    68.80\n",
            "| epoch  56 |   600/  663 batches | lr 30.00 | ms/batch 283.69 | loss  4.15 | ppl    63.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time: 197.93s | valid loss  4.29 | valid ppl    72.90 | valid bpc    6.188\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  57 |   100/  663 batches | lr 30.00 | ms/batch 286.58 | loss  4.22 | ppl    68.20\n",
            "| epoch  57 |   200/  663 batches | lr 30.00 | ms/batch 283.86 | loss  4.19 | ppl    66.00\n",
            "| epoch  57 |   300/  663 batches | lr 30.00 | ms/batch 283.85 | loss  4.20 | ppl    66.53\n",
            "| epoch  57 |   400/  663 batches | lr 30.00 | ms/batch 283.99 | loss  4.15 | ppl    63.67\n",
            "| epoch  57 |   500/  663 batches | lr 30.00 | ms/batch 283.74 | loss  4.23 | ppl    68.68\n",
            "| epoch  57 |   600/  663 batches | lr 30.00 | ms/batch 283.75 | loss  4.12 | ppl    61.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time: 198.02s | valid loss  4.29 | valid ppl    72.83 | valid bpc    6.186\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  58 |   100/  663 batches | lr 30.00 | ms/batch 286.38 | loss  4.23 | ppl    68.37\n",
            "| epoch  58 |   200/  663 batches | lr 30.00 | ms/batch 283.69 | loss  4.19 | ppl    65.83\n",
            "| epoch  58 |   300/  663 batches | lr 30.00 | ms/batch 283.71 | loss  4.20 | ppl    66.53\n",
            "| epoch  58 |   400/  663 batches | lr 30.00 | ms/batch 283.89 | loss  4.16 | ppl    63.99\n",
            "| epoch  58 |   500/  663 batches | lr 30.00 | ms/batch 283.69 | loss  4.23 | ppl    68.83\n",
            "| epoch  58 |   600/  663 batches | lr 30.00 | ms/batch 284.00 | loss  4.11 | ppl    60.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time: 197.98s | valid loss  4.29 | valid ppl    72.75 | valid bpc    6.185\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  59 |   100/  663 batches | lr 30.00 | ms/batch 286.29 | loss  4.22 | ppl    68.11\n",
            "| epoch  59 |   200/  663 batches | lr 30.00 | ms/batch 283.60 | loss  4.17 | ppl    64.92\n",
            "| epoch  59 |   300/  663 batches | lr 30.00 | ms/batch 283.57 | loss  4.19 | ppl    66.22\n",
            "| epoch  59 |   400/  663 batches | lr 30.00 | ms/batch 283.75 | loss  4.14 | ppl    63.11\n",
            "| epoch  59 |   500/  663 batches | lr 30.00 | ms/batch 283.69 | loss  4.22 | ppl    68.02\n",
            "| epoch  59 |   600/  663 batches | lr 30.00 | ms/batch 283.67 | loss  4.10 | ppl    60.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time: 197.83s | valid loss  4.29 | valid ppl    72.68 | valid bpc    6.183\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  60 |   100/  663 batches | lr 30.00 | ms/batch 286.53 | loss  4.22 | ppl    68.31\n",
            "| epoch  60 |   200/  663 batches | lr 30.00 | ms/batch 283.85 | loss  4.19 | ppl    65.73\n",
            "| epoch  60 |   300/  663 batches | lr 30.00 | ms/batch 283.71 | loss  4.18 | ppl    65.52\n",
            "| epoch  60 |   400/  663 batches | lr 30.00 | ms/batch 283.66 | loss  4.15 | ppl    63.39\n",
            "| epoch  60 |   500/  663 batches | lr 30.00 | ms/batch 283.69 | loss  4.20 | ppl    66.98\n",
            "| epoch  60 |   600/  663 batches | lr 30.00 | ms/batch 283.64 | loss  4.11 | ppl    60.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time: 197.93s | valid loss  4.29 | valid ppl    72.61 | valid bpc    6.182\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  61 |   100/  663 batches | lr 30.00 | ms/batch 286.42 | loss  4.21 | ppl    67.11\n",
            "| epoch  61 |   200/  663 batches | lr 30.00 | ms/batch 283.67 | loss  4.19 | ppl    66.15\n",
            "| epoch  61 |   300/  663 batches | lr 30.00 | ms/batch 283.52 | loss  4.16 | ppl    64.38\n",
            "| epoch  61 |   400/  663 batches | lr 30.00 | ms/batch 283.53 | loss  4.15 | ppl    63.29\n",
            "| epoch  61 |   500/  663 batches | lr 30.00 | ms/batch 283.82 | loss  4.22 | ppl    67.93\n",
            "| epoch  61 |   600/  663 batches | lr 30.00 | ms/batch 283.59 | loss  4.11 | ppl    60.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time: 197.89s | valid loss  4.28 | valid ppl    72.55 | valid bpc    6.181\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  62 |   100/  663 batches | lr 30.00 | ms/batch 286.21 | loss  4.22 | ppl    67.91\n",
            "| epoch  62 |   200/  663 batches | lr 30.00 | ms/batch 283.39 | loss  4.19 | ppl    65.75\n",
            "| epoch  62 |   300/  663 batches | lr 30.00 | ms/batch 283.52 | loss  4.18 | ppl    65.27\n",
            "| epoch  62 |   400/  663 batches | lr 30.00 | ms/batch 283.63 | loss  4.15 | ppl    63.59\n",
            "| epoch  62 |   500/  663 batches | lr 30.00 | ms/batch 283.51 | loss  4.22 | ppl    67.85\n",
            "| epoch  62 |   600/  663 batches | lr 30.00 | ms/batch 283.70 | loss  4.11 | ppl    60.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time: 197.79s | valid loss  4.28 | valid ppl    72.49 | valid bpc    6.180\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  63 |   100/  663 batches | lr 30.00 | ms/batch 286.29 | loss  4.21 | ppl    67.33\n",
            "| epoch  63 |   200/  663 batches | lr 30.00 | ms/batch 283.44 | loss  4.18 | ppl    65.57\n",
            "| epoch  63 |   300/  663 batches | lr 30.00 | ms/batch 283.59 | loss  4.17 | ppl    64.76\n",
            "| epoch  63 |   400/  663 batches | lr 30.00 | ms/batch 283.61 | loss  4.15 | ppl    63.19\n",
            "| epoch  63 |   500/  663 batches | lr 30.00 | ms/batch 283.69 | loss  4.21 | ppl    67.11\n",
            "| epoch  63 |   600/  663 batches | lr 30.00 | ms/batch 283.52 | loss  4.10 | ppl    60.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time: 197.84s | valid loss  4.28 | valid ppl    72.43 | valid bpc    6.179\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  64 |   100/  663 batches | lr 30.00 | ms/batch 286.23 | loss  4.20 | ppl    66.74\n",
            "| epoch  64 |   200/  663 batches | lr 30.00 | ms/batch 283.65 | loss  4.17 | ppl    64.43\n",
            "| epoch  64 |   300/  663 batches | lr 30.00 | ms/batch 283.56 | loss  4.17 | ppl    64.79\n",
            "| epoch  64 |   400/  663 batches | lr 30.00 | ms/batch 283.62 | loss  4.12 | ppl    61.75\n",
            "| epoch  64 |   500/  663 batches | lr 30.00 | ms/batch 283.55 | loss  4.21 | ppl    67.16\n",
            "| epoch  64 |   600/  663 batches | lr 30.00 | ms/batch 283.58 | loss  4.11 | ppl    60.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time: 197.86s | valid loss  4.28 | valid ppl    72.38 | valid bpc    6.178\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  65 |   100/  663 batches | lr 30.00 | ms/batch 286.30 | loss  4.19 | ppl    65.84\n",
            "| epoch  65 |   200/  663 batches | lr 30.00 | ms/batch 283.54 | loss  4.19 | ppl    65.75\n",
            "| epoch  65 |   300/  663 batches | lr 30.00 | ms/batch 283.60 | loss  4.16 | ppl    64.38\n",
            "| epoch  65 |   400/  663 batches | lr 30.00 | ms/batch 283.83 | loss  4.14 | ppl    62.73\n",
            "| epoch  65 |   500/  663 batches | lr 30.00 | ms/batch 283.57 | loss  4.19 | ppl    65.72\n",
            "| epoch  65 |   600/  663 batches | lr 30.00 | ms/batch 283.78 | loss  4.11 | ppl    61.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time: 197.87s | valid loss  4.28 | valid ppl    72.33 | valid bpc    6.176\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  66 |   100/  663 batches | lr 30.00 | ms/batch 286.24 | loss  4.19 | ppl    65.95\n",
            "| epoch  66 |   200/  663 batches | lr 30.00 | ms/batch 283.85 | loss  4.16 | ppl    64.11\n",
            "| epoch  66 |   300/  663 batches | lr 30.00 | ms/batch 283.91 | loss  4.16 | ppl    63.91\n",
            "| epoch  66 |   400/  663 batches | lr 30.00 | ms/batch 283.61 | loss  4.13 | ppl    62.12\n",
            "| epoch  66 |   500/  663 batches | lr 30.00 | ms/batch 283.62 | loss  4.19 | ppl    65.88\n",
            "| epoch  66 |   600/  663 batches | lr 30.00 | ms/batch 283.86 | loss  4.08 | ppl    59.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time: 197.93s | valid loss  4.28 | valid ppl    72.27 | valid bpc    6.175\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  67 |   100/  663 batches | lr 30.00 | ms/batch 286.32 | loss  4.20 | ppl    66.65\n",
            "| epoch  67 |   200/  663 batches | lr 30.00 | ms/batch 283.62 | loss  4.15 | ppl    63.72\n",
            "| epoch  67 |   300/  663 batches | lr 30.00 | ms/batch 283.79 | loss  4.17 | ppl    64.84\n",
            "| epoch  67 |   400/  663 batches | lr 30.00 | ms/batch 283.53 | loss  4.12 | ppl    61.36\n",
            "| epoch  67 |   500/  663 batches | lr 30.00 | ms/batch 283.49 | loss  4.19 | ppl    66.00\n",
            "| epoch  67 |   600/  663 batches | lr 30.00 | ms/batch 283.65 | loss  4.09 | ppl    59.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time: 197.88s | valid loss  4.28 | valid ppl    72.22 | valid bpc    6.174\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  68 |   100/  663 batches | lr 30.00 | ms/batch 286.14 | loss  4.20 | ppl    66.72\n",
            "| epoch  68 |   200/  663 batches | lr 30.00 | ms/batch 283.59 | loss  4.15 | ppl    63.68\n",
            "| epoch  68 |   300/  663 batches | lr 30.00 | ms/batch 283.71 | loss  4.18 | ppl    65.16\n",
            "| epoch  68 |   400/  663 batches | lr 30.00 | ms/batch 283.55 | loss  4.11 | ppl    60.95\n",
            "| epoch  68 |   500/  663 batches | lr 30.00 | ms/batch 283.43 | loss  4.19 | ppl    65.88\n",
            "| epoch  68 |   600/  663 batches | lr 30.00 | ms/batch 283.63 | loss  4.09 | ppl    59.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time: 197.83s | valid loss  4.28 | valid ppl    72.16 | valid bpc    6.173\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  69 |   100/  663 batches | lr 30.00 | ms/batch 286.29 | loss  4.17 | ppl    64.90\n",
            "| epoch  69 |   200/  663 batches | lr 30.00 | ms/batch 283.51 | loss  4.16 | ppl    64.24\n",
            "| epoch  69 |   300/  663 batches | lr 30.00 | ms/batch 283.56 | loss  4.14 | ppl    63.07\n",
            "| epoch  69 |   400/  663 batches | lr 30.00 | ms/batch 283.76 | loss  4.13 | ppl    61.89\n",
            "| epoch  69 |   500/  663 batches | lr 30.00 | ms/batch 283.36 | loss  4.19 | ppl    65.76\n",
            "| epoch  69 |   600/  663 batches | lr 30.00 | ms/batch 283.58 | loss  4.08 | ppl    59.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time: 197.82s | valid loss  4.28 | valid ppl    72.10 | valid bpc    6.172\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  70 |   100/  663 batches | lr 30.00 | ms/batch 285.99 | loss  4.19 | ppl    66.29\n",
            "| epoch  70 |   200/  663 batches | lr 30.00 | ms/batch 283.36 | loss  4.15 | ppl    63.68\n",
            "| epoch  70 |   300/  663 batches | lr 30.00 | ms/batch 283.53 | loss  4.17 | ppl    64.55\n",
            "| epoch  70 |   400/  663 batches | lr 30.00 | ms/batch 283.40 | loss  4.11 | ppl    60.79\n",
            "| epoch  70 |   500/  663 batches | lr 30.00 | ms/batch 283.51 | loss  4.20 | ppl    66.55\n",
            "| epoch  70 |   600/  663 batches | lr 30.00 | ms/batch 283.52 | loss  4.08 | ppl    59.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time: 197.72s | valid loss  4.28 | valid ppl    72.05 | valid bpc    6.171\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  71 |   100/  663 batches | lr 30.00 | ms/batch 286.09 | loss  4.18 | ppl    65.51\n",
            "| epoch  71 |   200/  663 batches | lr 30.00 | ms/batch 283.57 | loss  4.16 | ppl    63.97\n",
            "| epoch  71 |   300/  663 batches | lr 30.00 | ms/batch 283.53 | loss  4.16 | ppl    63.94\n",
            "| epoch  71 |   400/  663 batches | lr 30.00 | ms/batch 283.33 | loss  4.12 | ppl    61.26\n",
            "| epoch  71 |   500/  663 batches | lr 30.00 | ms/batch 283.39 | loss  4.17 | ppl    64.81\n",
            "| epoch  71 |   600/  663 batches | lr 30.00 | ms/batch 283.33 | loss  4.07 | ppl    58.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time: 197.72s | valid loss  4.28 | valid ppl    72.00 | valid bpc    6.170\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  72 |   100/  663 batches | lr 30.00 | ms/batch 286.20 | loss  4.18 | ppl    65.56\n",
            "| epoch  72 |   200/  663 batches | lr 30.00 | ms/batch 283.41 | loss  4.14 | ppl    62.52\n",
            "| epoch  72 |   300/  663 batches | lr 30.00 | ms/batch 283.58 | loss  4.15 | ppl    63.26\n",
            "| epoch  72 |   400/  663 batches | lr 30.00 | ms/batch 283.54 | loss  4.11 | ppl    61.16\n",
            "| epoch  72 |   500/  663 batches | lr 30.00 | ms/batch 283.11 | loss  4.19 | ppl    65.73\n",
            "| epoch  72 |   600/  663 batches | lr 30.00 | ms/batch 283.53 | loss  4.08 | ppl    58.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time: 197.73s | valid loss  4.28 | valid ppl    71.96 | valid bpc    6.169\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  73 |   100/  663 batches | lr 30.00 | ms/batch 286.18 | loss  4.17 | ppl    64.64\n",
            "| epoch  73 |   200/  663 batches | lr 30.00 | ms/batch 283.46 | loss  4.15 | ppl    63.52\n",
            "| epoch  73 |   300/  663 batches | lr 30.00 | ms/batch 283.42 | loss  4.14 | ppl    63.05\n",
            "| epoch  73 |   400/  663 batches | lr 30.00 | ms/batch 283.28 | loss  4.12 | ppl    61.52\n",
            "| epoch  73 |   500/  663 batches | lr 30.00 | ms/batch 282.65 | loss  4.18 | ppl    65.07\n",
            "| epoch  73 |   600/  663 batches | lr 30.00 | ms/batch 283.31 | loss  4.05 | ppl    57.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time: 197.60s | valid loss  4.28 | valid ppl    71.92 | valid bpc    6.168\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  74 |   100/  663 batches | lr 30.00 | ms/batch 285.87 | loss  4.17 | ppl    64.56\n",
            "| epoch  74 |   200/  663 batches | lr 30.00 | ms/batch 283.40 | loss  4.15 | ppl    63.34\n",
            "| epoch  74 |   300/  663 batches | lr 30.00 | ms/batch 283.33 | loss  4.15 | ppl    63.25\n",
            "| epoch  74 |   400/  663 batches | lr 30.00 | ms/batch 283.27 | loss  4.10 | ppl    60.50\n",
            "| epoch  74 |   500/  663 batches | lr 30.00 | ms/batch 283.14 | loss  4.16 | ppl    63.88\n",
            "| epoch  74 |   600/  663 batches | lr 30.00 | ms/batch 283.32 | loss  4.06 | ppl    58.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time: 197.63s | valid loss  4.27 | valid ppl    71.88 | valid bpc    6.168\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  75 |   100/  663 batches | lr 30.00 | ms/batch 286.08 | loss  4.16 | ppl    64.03\n",
            "| epoch  75 |   200/  663 batches | lr 30.00 | ms/batch 283.43 | loss  4.14 | ppl    62.53\n",
            "| epoch  75 |   300/  663 batches | lr 30.00 | ms/batch 283.58 | loss  4.13 | ppl    62.03\n",
            "| epoch  75 |   400/  663 batches | lr 30.00 | ms/batch 283.56 | loss  4.09 | ppl    60.03\n",
            "| epoch  75 |   500/  663 batches | lr 30.00 | ms/batch 283.44 | loss  4.17 | ppl    64.62\n",
            "| epoch  75 |   600/  663 batches | lr 30.00 | ms/batch 284.16 | loss  4.06 | ppl    58.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time: 197.84s | valid loss  4.27 | valid ppl    71.84 | valid bpc    6.167\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  76 |   100/  663 batches | lr 30.00 | ms/batch 286.59 | loss  4.16 | ppl    64.05\n",
            "| epoch  76 |   200/  663 batches | lr 30.00 | ms/batch 283.77 | loss  4.13 | ppl    62.17\n",
            "| epoch  76 |   300/  663 batches | lr 30.00 | ms/batch 283.95 | loss  4.14 | ppl    62.92\n",
            "| epoch  76 |   400/  663 batches | lr 30.00 | ms/batch 283.83 | loss  4.11 | ppl    61.20\n",
            "| epoch  76 |   500/  663 batches | lr 30.00 | ms/batch 283.75 | loss  4.16 | ppl    64.05\n",
            "| epoch  76 |   600/  663 batches | lr 30.00 | ms/batch 283.90 | loss  4.05 | ppl    57.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time: 197.98s | valid loss  4.27 | valid ppl    71.81 | valid bpc    6.166\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  77 |   100/  663 batches | lr 30.00 | ms/batch 286.02 | loss  4.16 | ppl    63.91\n",
            "| epoch  77 |   200/  663 batches | lr 30.00 | ms/batch 283.47 | loss  4.12 | ppl    61.80\n",
            "| epoch  77 |   300/  663 batches | lr 30.00 | ms/batch 283.35 | loss  4.13 | ppl    62.09\n",
            "| epoch  77 |   400/  663 batches | lr 30.00 | ms/batch 283.66 | loss  4.10 | ppl    60.55\n",
            "| epoch  77 |   500/  663 batches | lr 30.00 | ms/batch 283.69 | loss  4.16 | ppl    64.03\n",
            "| epoch  77 |   600/  663 batches | lr 30.00 | ms/batch 283.94 | loss  4.06 | ppl    57.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time: 197.85s | valid loss  4.27 | valid ppl    71.78 | valid bpc    6.165\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  78 |   100/  663 batches | lr 30.00 | ms/batch 286.37 | loss  4.16 | ppl    64.21\n",
            "| epoch  78 |   200/  663 batches | lr 30.00 | ms/batch 283.68 | loss  4.12 | ppl    61.60\n",
            "| epoch  78 |   300/  663 batches | lr 30.00 | ms/batch 283.58 | loss  4.13 | ppl    61.99\n",
            "| epoch  78 |   400/  663 batches | lr 30.00 | ms/batch 283.36 | loss  4.09 | ppl    59.49\n",
            "| epoch  78 |   500/  663 batches | lr 30.00 | ms/batch 283.59 | loss  4.15 | ppl    63.39\n",
            "| epoch  78 |   600/  663 batches | lr 30.00 | ms/batch 283.80 | loss  4.07 | ppl    58.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time: 197.84s | valid loss  4.27 | valid ppl    71.74 | valid bpc    6.165\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  79 |   100/  663 batches | lr 30.00 | ms/batch 286.68 | loss  4.16 | ppl    63.86\n",
            "| epoch  79 |   200/  663 batches | lr 30.00 | ms/batch 283.78 | loss  4.12 | ppl    61.29\n",
            "| epoch  79 |   300/  663 batches | lr 30.00 | ms/batch 283.87 | loss  4.14 | ppl    62.64\n",
            "| epoch  79 |   400/  663 batches | lr 30.00 | ms/batch 283.71 | loss  4.09 | ppl    59.75\n",
            "| epoch  79 |   500/  663 batches | lr 30.00 | ms/batch 283.74 | loss  4.15 | ppl    63.43\n",
            "| epoch  79 |   600/  663 batches | lr 30.00 | ms/batch 283.85 | loss  4.04 | ppl    56.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time: 197.98s | valid loss  4.27 | valid ppl    71.70 | valid bpc    6.164\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  80 |   100/  663 batches | lr 30.00 | ms/batch 286.12 | loss  4.15 | ppl    63.49\n",
            "| epoch  80 |   200/  663 batches | lr 30.00 | ms/batch 283.68 | loss  4.12 | ppl    61.71\n",
            "| epoch  80 |   300/  663 batches | lr 30.00 | ms/batch 283.68 | loss  4.12 | ppl    61.58\n",
            "| epoch  80 |   400/  663 batches | lr 30.00 | ms/batch 283.94 | loss  4.10 | ppl    60.27\n",
            "| epoch  80 |   500/  663 batches | lr 30.00 | ms/batch 283.89 | loss  4.15 | ppl    63.33\n",
            "| epoch  80 |   600/  663 batches | lr 30.00 | ms/batch 283.91 | loss  4.03 | ppl    56.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time: 197.93s | valid loss  4.27 | valid ppl    71.66 | valid bpc    6.163\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  81 |   100/  663 batches | lr 30.00 | ms/batch 286.56 | loss  4.14 | ppl    62.91\n",
            "| epoch  81 |   200/  663 batches | lr 30.00 | ms/batch 283.86 | loss  4.11 | ppl    61.05\n",
            "| epoch  81 |   300/  663 batches | lr 30.00 | ms/batch 283.85 | loss  4.12 | ppl    61.46\n",
            "| epoch  81 |   400/  663 batches | lr 30.00 | ms/batch 283.98 | loss  4.08 | ppl    58.94\n",
            "| epoch  81 |   500/  663 batches | lr 30.00 | ms/batch 283.49 | loss  4.16 | ppl    63.85\n",
            "| epoch  81 |   600/  663 batches | lr 30.00 | ms/batch 283.86 | loss  4.04 | ppl    57.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time: 197.98s | valid loss  4.27 | valid ppl    71.62 | valid bpc    6.162\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  82 |   100/  663 batches | lr 30.00 | ms/batch 286.85 | loss  4.15 | ppl    63.34\n",
            "| epoch  82 |   200/  663 batches | lr 30.00 | ms/batch 283.87 | loss  4.11 | ppl    61.03\n",
            "| epoch  82 |   300/  663 batches | lr 30.00 | ms/batch 283.89 | loss  4.12 | ppl    61.86\n",
            "| epoch  82 |   400/  663 batches | lr 30.00 | ms/batch 283.97 | loss  4.08 | ppl    59.21\n",
            "| epoch  82 |   500/  663 batches | lr 30.00 | ms/batch 283.60 | loss  4.14 | ppl    62.76\n",
            "| epoch  82 |   600/  663 batches | lr 30.00 | ms/batch 283.92 | loss  4.04 | ppl    56.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time: 198.02s | valid loss  4.27 | valid ppl    71.57 | valid bpc    6.161\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  83 |   100/  663 batches | lr 30.00 | ms/batch 286.40 | loss  4.15 | ppl    63.16\n",
            "| epoch  83 |   200/  663 batches | lr 30.00 | ms/batch 283.46 | loss  4.11 | ppl    60.80\n",
            "| epoch  83 |   300/  663 batches | lr 30.00 | ms/batch 283.79 | loss  4.12 | ppl    61.68\n",
            "| epoch  83 |   400/  663 batches | lr 30.00 | ms/batch 283.80 | loss  4.07 | ppl    58.34\n",
            "| epoch  83 |   500/  663 batches | lr 30.00 | ms/batch 283.70 | loss  4.14 | ppl    62.76\n",
            "| epoch  83 |   600/  663 batches | lr 30.00 | ms/batch 283.78 | loss  4.04 | ppl    57.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time: 197.92s | valid loss  4.27 | valid ppl    71.53 | valid bpc    6.160\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  84 |   100/  663 batches | lr 30.00 | ms/batch 286.51 | loss  4.13 | ppl    62.33\n",
            "| epoch  84 |   200/  663 batches | lr 30.00 | ms/batch 283.77 | loss  4.11 | ppl    60.75\n",
            "| epoch  84 |   300/  663 batches | lr 30.00 | ms/batch 283.75 | loss  4.12 | ppl    61.45\n",
            "| epoch  84 |   400/  663 batches | lr 30.00 | ms/batch 283.89 | loss  4.09 | ppl    59.65\n",
            "| epoch  84 |   500/  663 batches | lr 30.00 | ms/batch 283.45 | loss  4.14 | ppl    62.63\n",
            "| epoch  84 |   600/  663 batches | lr 30.00 | ms/batch 283.48 | loss  4.04 | ppl    56.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time: 197.88s | valid loss  4.27 | valid ppl    71.49 | valid bpc    6.160\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  85 |   100/  663 batches | lr 30.00 | ms/batch 286.40 | loss  4.14 | ppl    62.86\n",
            "| epoch  85 |   200/  663 batches | lr 30.00 | ms/batch 283.88 | loss  4.11 | ppl    60.65\n",
            "| epoch  85 |   300/  663 batches | lr 30.00 | ms/batch 283.68 | loss  4.11 | ppl    61.15\n",
            "| epoch  85 |   400/  663 batches | lr 30.00 | ms/batch 283.72 | loss  4.07 | ppl    58.33\n",
            "| epoch  85 |   500/  663 batches | lr 30.00 | ms/batch 283.43 | loss  4.13 | ppl    62.41\n",
            "| epoch  85 |   600/  663 batches | lr 30.00 | ms/batch 283.70 | loss  4.03 | ppl    56.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time: 197.88s | valid loss  4.27 | valid ppl    71.46 | valid bpc    6.159\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  86 |   100/  663 batches | lr 30.00 | ms/batch 286.42 | loss  4.13 | ppl    62.17\n",
            "| epoch  86 |   200/  663 batches | lr 30.00 | ms/batch 283.61 | loss  4.12 | ppl    61.80\n",
            "| epoch  86 |   300/  663 batches | lr 30.00 | ms/batch 283.54 | loss  4.11 | ppl    60.79\n",
            "| epoch  86 |   400/  663 batches | lr 30.00 | ms/batch 283.96 | loss  4.07 | ppl    58.33\n",
            "| epoch  86 |   500/  663 batches | lr 30.00 | ms/batch 283.61 | loss  4.13 | ppl    62.47\n",
            "| epoch  86 |   600/  663 batches | lr 30.00 | ms/batch 283.61 | loss  4.01 | ppl    55.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time: 197.88s | valid loss  4.27 | valid ppl    71.42 | valid bpc    6.158\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  87 |   100/  663 batches | lr 30.00 | ms/batch 286.78 | loss  4.15 | ppl    63.19\n",
            "| epoch  87 |   200/  663 batches | lr 30.00 | ms/batch 283.86 | loss  4.12 | ppl    61.62\n",
            "| epoch  87 |   300/  663 batches | lr 30.00 | ms/batch 284.20 | loss  4.11 | ppl    61.03\n",
            "| epoch  87 |   400/  663 batches | lr 30.00 | ms/batch 284.15 | loss  4.07 | ppl    58.58\n",
            "| epoch  87 |   500/  663 batches | lr 30.00 | ms/batch 283.56 | loss  4.12 | ppl    61.85\n",
            "| epoch  87 |   600/  663 batches | lr 30.00 | ms/batch 283.95 | loss  4.03 | ppl    56.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time: 198.07s | valid loss  4.27 | valid ppl    71.39 | valid bpc    6.158\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  88 |   100/  663 batches | lr 30.00 | ms/batch 286.56 | loss  4.14 | ppl    63.07\n",
            "| epoch  88 |   200/  663 batches | lr 30.00 | ms/batch 284.12 | loss  4.11 | ppl    60.64\n",
            "| epoch  88 |   300/  663 batches | lr 30.00 | ms/batch 284.21 | loss  4.11 | ppl    61.13\n",
            "| epoch  88 |   400/  663 batches | lr 30.00 | ms/batch 284.31 | loss  4.06 | ppl    58.06\n",
            "| epoch  88 |   500/  663 batches | lr 30.00 | ms/batch 284.17 | loss  4.14 | ppl    62.70\n",
            "| epoch  88 |   600/  663 batches | lr 30.00 | ms/batch 284.38 | loss  4.03 | ppl    56.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time: 198.26s | valid loss  4.27 | valid ppl    71.35 | valid bpc    6.157\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  89 |   100/  663 batches | lr 30.00 | ms/batch 287.63 | loss  4.13 | ppl    61.94\n",
            "| epoch  89 |   200/  663 batches | lr 30.00 | ms/batch 284.66 | loss  4.10 | ppl    60.13\n",
            "| epoch  89 |   300/  663 batches | lr 30.00 | ms/batch 284.72 | loss  4.11 | ppl    60.80\n",
            "| epoch  89 |   400/  663 batches | lr 30.00 | ms/batch 284.20 | loss  4.05 | ppl    57.40\n",
            "| epoch  89 |   500/  663 batches | lr 30.00 | ms/batch 284.36 | loss  4.13 | ppl    61.91\n",
            "| epoch  89 |   600/  663 batches | lr 30.00 | ms/batch 284.80 | loss  4.02 | ppl    55.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time: 198.50s | valid loss  4.27 | valid ppl    71.32 | valid bpc    6.156\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  90 |   100/  663 batches | lr 30.00 | ms/batch 287.52 | loss  4.12 | ppl    61.38\n",
            "| epoch  90 |   200/  663 batches | lr 30.00 | ms/batch 284.55 | loss  4.10 | ppl    60.07\n",
            "| epoch  90 |   300/  663 batches | lr 30.00 | ms/batch 284.37 | loss  4.10 | ppl    60.21\n",
            "| epoch  90 |   400/  663 batches | lr 30.00 | ms/batch 284.80 | loss  4.07 | ppl    58.30\n",
            "| epoch  90 |   500/  663 batches | lr 30.00 | ms/batch 284.92 | loss  4.11 | ppl    61.04\n",
            "| epoch  90 |   600/  663 batches | lr 30.00 | ms/batch 284.69 | loss  4.03 | ppl    56.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time: 198.57s | valid loss  4.27 | valid ppl    71.29 | valid bpc    6.156\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  91 |   100/  663 batches | lr 30.00 | ms/batch 286.41 | loss  4.12 | ppl    61.69\n",
            "| epoch  91 |   200/  663 batches | lr 30.00 | ms/batch 284.41 | loss  4.10 | ppl    60.12\n",
            "| epoch  91 |   300/  663 batches | lr 30.00 | ms/batch 284.46 | loss  4.08 | ppl    59.33\n",
            "| epoch  91 |   400/  663 batches | lr 30.00 | ms/batch 284.67 | loss  4.05 | ppl    57.67\n",
            "| epoch  91 |   500/  663 batches | lr 30.00 | ms/batch 284.28 | loss  4.13 | ppl    62.37\n",
            "| epoch  91 |   600/  663 batches | lr 30.00 | ms/batch 284.07 | loss  4.02 | ppl    55.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time: 198.25s | valid loss  4.27 | valid ppl    71.26 | valid bpc    6.155\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  92 |   100/  663 batches | lr 30.00 | ms/batch 287.32 | loss  4.11 | ppl    61.23\n",
            "| epoch  92 |   200/  663 batches | lr 30.00 | ms/batch 284.50 | loss  4.08 | ppl    59.25\n",
            "| epoch  92 |   300/  663 batches | lr 30.00 | ms/batch 284.80 | loss  4.09 | ppl    60.03\n",
            "| epoch  92 |   400/  663 batches | lr 30.00 | ms/batch 284.82 | loss  4.05 | ppl    57.26\n",
            "| epoch  92 |   500/  663 batches | lr 30.00 | ms/batch 283.90 | loss  4.12 | ppl    61.47\n",
            "| epoch  92 |   600/  663 batches | lr 30.00 | ms/batch 284.68 | loss  4.03 | ppl    56.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time: 198.51s | valid loss  4.27 | valid ppl    71.23 | valid bpc    6.154\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  93 |   100/  663 batches | lr 30.00 | ms/batch 287.45 | loss  4.12 | ppl    61.53\n",
            "| epoch  93 |   200/  663 batches | lr 30.00 | ms/batch 284.76 | loss  4.10 | ppl    60.41\n",
            "| epoch  93 |   300/  663 batches | lr 30.00 | ms/batch 284.43 | loss  4.08 | ppl    59.43\n",
            "| epoch  93 |   400/  663 batches | lr 30.00 | ms/batch 284.73 | loss  4.04 | ppl    56.80\n",
            "| epoch  93 |   500/  663 batches | lr 30.00 | ms/batch 284.76 | loss  4.12 | ppl    61.63\n",
            "| epoch  93 |   600/  663 batches | lr 30.00 | ms/batch 284.90 | loss  4.03 | ppl    56.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time: 198.59s | valid loss  4.27 | valid ppl    71.20 | valid bpc    6.154\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  94 |   100/  663 batches | lr 30.00 | ms/batch 287.08 | loss  4.10 | ppl    60.62\n",
            "| epoch  94 |   200/  663 batches | lr 30.00 | ms/batch 284.39 | loss  4.10 | ppl    60.13\n",
            "| epoch  94 |   300/  663 batches | lr 30.00 | ms/batch 284.69 | loss  4.09 | ppl    59.75\n",
            "| epoch  94 |   400/  663 batches | lr 30.00 | ms/batch 284.80 | loss  4.05 | ppl    57.18\n",
            "| epoch  94 |   500/  663 batches | lr 30.00 | ms/batch 284.54 | loss  4.12 | ppl    61.38\n",
            "| epoch  94 |   600/  663 batches | lr 30.00 | ms/batch 284.46 | loss  4.02 | ppl    55.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time: 198.47s | valid loss  4.27 | valid ppl    71.17 | valid bpc    6.153\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  95 |   100/  663 batches | lr 30.00 | ms/batch 287.17 | loss  4.11 | ppl    60.83\n",
            "| epoch  95 |   200/  663 batches | lr 30.00 | ms/batch 284.43 | loss  4.10 | ppl    60.12\n",
            "| epoch  95 |   300/  663 batches | lr 30.00 | ms/batch 284.83 | loss  4.09 | ppl    59.66\n",
            "| epoch  95 |   400/  663 batches | lr 30.00 | ms/batch 284.62 | loss  4.04 | ppl    57.03\n",
            "| epoch  95 |   500/  663 batches | lr 30.00 | ms/batch 284.23 | loss  4.10 | ppl    60.54\n",
            "| epoch  95 |   600/  663 batches | lr 30.00 | ms/batch 284.24 | loss  4.01 | ppl    55.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time: 198.43s | valid loss  4.26 | valid ppl    71.15 | valid bpc    6.153\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  96 |   100/  663 batches | lr 30.00 | ms/batch 287.31 | loss  4.10 | ppl    60.23\n",
            "| epoch  96 |   200/  663 batches | lr 30.00 | ms/batch 284.68 | loss  4.07 | ppl    58.44\n",
            "| epoch  96 |   300/  663 batches | lr 30.00 | ms/batch 284.83 | loss  4.09 | ppl    59.53\n",
            "| epoch  96 |   400/  663 batches | lr 30.00 | ms/batch 284.56 | loss  4.04 | ppl    56.76\n",
            "| epoch  96 |   500/  663 batches | lr 30.00 | ms/batch 284.49 | loss  4.11 | ppl    61.04\n",
            "| epoch  96 |   600/  663 batches | lr 30.00 | ms/batch 284.55 | loss  4.01 | ppl    55.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time: 198.53s | valid loss  4.26 | valid ppl    71.12 | valid bpc    6.152\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  97 |   100/  663 batches | lr 30.00 | ms/batch 287.33 | loss  4.10 | ppl    60.25\n",
            "| epoch  97 |   200/  663 batches | lr 30.00 | ms/batch 284.35 | loss  4.07 | ppl    58.84\n",
            "| epoch  97 |   300/  663 batches | lr 30.00 | ms/batch 284.81 | loss  4.09 | ppl    59.56\n",
            "| epoch  97 |   400/  663 batches | lr 30.00 | ms/batch 284.94 | loss  4.06 | ppl    58.13\n",
            "| epoch  97 |   500/  663 batches | lr 30.00 | ms/batch 284.28 | loss  4.11 | ppl    61.24\n",
            "| epoch  97 |   600/  663 batches | lr 30.00 | ms/batch 282.84 | loss  4.00 | ppl    54.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time: 198.10s | valid loss  4.26 | valid ppl    71.09 | valid bpc    6.152\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  98 |   100/  663 batches | lr 30.00 | ms/batch 284.13 | loss  4.11 | ppl    60.67\n",
            "| epoch  98 |   200/  663 batches | lr 30.00 | ms/batch 281.55 | loss  4.06 | ppl    58.10\n",
            "| epoch  98 |   300/  663 batches | lr 30.00 | ms/batch 281.40 | loss  4.07 | ppl    58.50\n",
            "| epoch  98 |   400/  663 batches | lr 30.00 | ms/batch 281.55 | loss  4.04 | ppl    56.64\n",
            "| epoch  98 |   500/  663 batches | lr 30.00 | ms/batch 281.36 | loss  4.11 | ppl    61.09\n",
            "| epoch  98 |   600/  663 batches | lr 30.00 | ms/batch 281.12 | loss  3.99 | ppl    54.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time: 196.27s | valid loss  4.26 | valid ppl    71.06 | valid bpc    6.151\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  99 |   100/  663 batches | lr 30.00 | ms/batch 284.09 | loss  4.10 | ppl    60.51\n",
            "| epoch  99 |   200/  663 batches | lr 30.00 | ms/batch 281.21 | loss  4.08 | ppl    58.90\n",
            "| epoch  99 |   300/  663 batches | lr 30.00 | ms/batch 281.33 | loss  4.08 | ppl    59.03\n",
            "| epoch  99 |   400/  663 batches | lr 30.00 | ms/batch 281.04 | loss  4.04 | ppl    56.59\n",
            "| epoch  99 |   500/  663 batches | lr 30.00 | ms/batch 281.12 | loss  4.11 | ppl    60.72\n",
            "| epoch  99 |   600/  663 batches | lr 30.00 | ms/batch 281.39 | loss  4.00 | ppl    54.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time: 196.20s | valid loss  4.26 | valid ppl    71.04 | valid bpc    6.151\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 100 |   100/  663 batches | lr 30.00 | ms/batch 283.85 | loss  4.09 | ppl    59.81\n",
            "| epoch 100 |   200/  663 batches | lr 30.00 | ms/batch 281.30 | loss  4.05 | ppl    57.62\n",
            "| epoch 100 |   300/  663 batches | lr 30.00 | ms/batch 281.07 | loss  4.07 | ppl    58.49\n",
            "| epoch 100 |   400/  663 batches | lr 30.00 | ms/batch 281.03 | loss  4.04 | ppl    56.98\n",
            "| epoch 100 |   500/  663 batches | lr 30.00 | ms/batch 281.08 | loss  4.11 | ppl    60.77\n",
            "| epoch 100 |   600/  663 batches | lr 30.00 | ms/batch 281.22 | loss  3.99 | ppl    54.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time: 196.12s | valid loss  4.26 | valid ppl    71.02 | valid bpc    6.150\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 101 |   100/  663 batches | lr 30.00 | ms/batch 283.65 | loss  4.10 | ppl    60.63\n",
            "| epoch 101 |   200/  663 batches | lr 30.00 | ms/batch 281.00 | loss  4.08 | ppl    59.01\n",
            "| epoch 101 |   300/  663 batches | lr 30.00 | ms/batch 281.41 | loss  4.07 | ppl    58.79\n",
            "| epoch 101 |   400/  663 batches | lr 30.00 | ms/batch 281.28 | loss  4.05 | ppl    57.15\n",
            "| epoch 101 |   500/  663 batches | lr 30.00 | ms/batch 280.74 | loss  4.11 | ppl    60.66\n",
            "| epoch 101 |   600/  663 batches | lr 30.00 | ms/batch 281.14 | loss  4.01 | ppl    54.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 101 | time: 196.06s | valid loss  4.26 | valid ppl    70.99 | valid bpc    6.150\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 102 |   100/  663 batches | lr 30.00 | ms/batch 283.30 | loss  4.11 | ppl    60.73\n",
            "| epoch 102 |   200/  663 batches | lr 30.00 | ms/batch 281.08 | loss  4.07 | ppl    58.36\n",
            "| epoch 102 |   300/  663 batches | lr 30.00 | ms/batch 281.15 | loss  4.08 | ppl    59.06\n",
            "| epoch 102 |   400/  663 batches | lr 30.00 | ms/batch 280.94 | loss  4.03 | ppl    56.17\n",
            "| epoch 102 |   500/  663 batches | lr 30.00 | ms/batch 280.35 | loss  4.09 | ppl    59.87\n",
            "| epoch 102 |   600/  663 batches | lr 30.00 | ms/batch 280.83 | loss  4.00 | ppl    54.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 102 | time: 195.90s | valid loss  4.26 | valid ppl    70.97 | valid bpc    6.149\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 103 |   100/  663 batches | lr 30.00 | ms/batch 283.72 | loss  4.08 | ppl    59.40\n",
            "| epoch 103 |   200/  663 batches | lr 30.00 | ms/batch 280.78 | loss  4.07 | ppl    58.60\n",
            "| epoch 103 |   300/  663 batches | lr 30.00 | ms/batch 280.92 | loss  4.07 | ppl    58.44\n",
            "| epoch 103 |   400/  663 batches | lr 30.00 | ms/batch 280.75 | loss  4.02 | ppl    55.66\n",
            "| epoch 103 |   500/  663 batches | lr 30.00 | ms/batch 280.20 | loss  4.08 | ppl    59.41\n",
            "| epoch 103 |   600/  663 batches | lr 30.00 | ms/batch 280.82 | loss  4.01 | ppl    55.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 103 | time: 195.86s | valid loss  4.26 | valid ppl    70.95 | valid bpc    6.149\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 104 |   100/  663 batches | lr 30.00 | ms/batch 283.21 | loss  4.10 | ppl    60.40\n",
            "| epoch 104 |   200/  663 batches | lr 30.00 | ms/batch 280.58 | loss  4.07 | ppl    58.80\n",
            "| epoch 104 |   300/  663 batches | lr 30.00 | ms/batch 281.11 | loss  4.07 | ppl    58.39\n",
            "| epoch 104 |   400/  663 batches | lr 30.00 | ms/batch 281.01 | loss  4.02 | ppl    55.77\n",
            "| epoch 104 |   500/  663 batches | lr 30.00 | ms/batch 280.99 | loss  4.09 | ppl    59.86\n",
            "| epoch 104 |   600/  663 batches | lr 30.00 | ms/batch 280.97 | loss  3.99 | ppl    53.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 104 | time: 195.93s | valid loss  4.26 | valid ppl    70.93 | valid bpc    6.148\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 105 |   100/  663 batches | lr 30.00 | ms/batch 283.27 | loss  4.09 | ppl    59.85\n",
            "| epoch 105 |   200/  663 batches | lr 30.00 | ms/batch 280.61 | loss  4.08 | ppl    59.04\n",
            "| epoch 105 |   300/  663 batches | lr 30.00 | ms/batch 280.98 | loss  4.05 | ppl    57.39\n",
            "| epoch 105 |   400/  663 batches | lr 30.00 | ms/batch 280.86 | loss  4.03 | ppl    56.09\n",
            "| epoch 105 |   500/  663 batches | lr 30.00 | ms/batch 280.86 | loss  4.09 | ppl    59.75\n",
            "| epoch 105 |   600/  663 batches | lr 30.00 | ms/batch 280.72 | loss  3.99 | ppl    53.95\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 105 | time: 195.88s | valid loss  4.26 | valid ppl    70.90 | valid bpc    6.148\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 106 |   100/  663 batches | lr 30.00 | ms/batch 283.56 | loss  4.09 | ppl    59.76\n",
            "| epoch 106 |   200/  663 batches | lr 30.00 | ms/batch 280.98 | loss  4.06 | ppl    58.10\n",
            "| epoch 106 |   300/  663 batches | lr 30.00 | ms/batch 280.98 | loss  4.07 | ppl    58.42\n",
            "| epoch 106 |   400/  663 batches | lr 30.00 | ms/batch 280.80 | loss  4.03 | ppl    56.23\n",
            "| epoch 106 |   500/  663 batches | lr 30.00 | ms/batch 280.79 | loss  4.08 | ppl    59.10\n",
            "| epoch 106 |   600/  663 batches | lr 30.00 | ms/batch 280.53 | loss  3.98 | ppl    53.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 106 | time: 195.90s | valid loss  4.26 | valid ppl    70.88 | valid bpc    6.147\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 107 |   100/  663 batches | lr 30.00 | ms/batch 283.52 | loss  4.08 | ppl    59.27\n",
            "| epoch 107 |   200/  663 batches | lr 30.00 | ms/batch 280.84 | loss  4.04 | ppl    56.99\n",
            "| epoch 107 |   300/  663 batches | lr 30.00 | ms/batch 280.67 | loss  4.05 | ppl    57.45\n",
            "| epoch 107 |   400/  663 batches | lr 30.00 | ms/batch 281.13 | loss  4.02 | ppl    55.94\n",
            "| epoch 107 |   500/  663 batches | lr 30.00 | ms/batch 280.19 | loss  4.08 | ppl    59.15\n",
            "| epoch 107 |   600/  663 batches | lr 30.00 | ms/batch 280.87 | loss  4.00 | ppl    54.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 107 | time: 195.81s | valid loss  4.26 | valid ppl    70.86 | valid bpc    6.147\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 108 |   100/  663 batches | lr 30.00 | ms/batch 283.46 | loss  4.09 | ppl    59.64\n",
            "| epoch 108 |   200/  663 batches | lr 30.00 | ms/batch 280.56 | loss  4.04 | ppl    56.67\n",
            "| epoch 108 |   300/  663 batches | lr 30.00 | ms/batch 280.15 | loss  4.06 | ppl    58.03\n",
            "| epoch 108 |   400/  663 batches | lr 30.00 | ms/batch 280.68 | loss  4.01 | ppl    55.22\n",
            "| epoch 108 |   500/  663 batches | lr 30.00 | ms/batch 280.13 | loss  4.07 | ppl    58.81\n",
            "| epoch 108 |   600/  663 batches | lr 30.00 | ms/batch 280.81 | loss  3.98 | ppl    53.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 108 | time: 195.65s | valid loss  4.26 | valid ppl    70.83 | valid bpc    6.146\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 109 |   100/  663 batches | lr 30.00 | ms/batch 283.18 | loss  4.09 | ppl    59.59\n",
            "| epoch 109 |   200/  663 batches | lr 30.00 | ms/batch 280.49 | loss  4.05 | ppl    57.18\n",
            "| epoch 109 |   300/  663 batches | lr 30.00 | ms/batch 280.66 | loss  4.06 | ppl    57.74\n",
            "| epoch 109 |   400/  663 batches | lr 30.00 | ms/batch 280.56 | loss  4.02 | ppl    55.65\n",
            "| epoch 109 |   500/  663 batches | lr 30.00 | ms/batch 280.36 | loss  4.09 | ppl    59.54\n",
            "| epoch 109 |   600/  663 batches | lr 30.00 | ms/batch 280.46 | loss  3.96 | ppl    52.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 109 | time: 195.60s | valid loss  4.26 | valid ppl    70.81 | valid bpc    6.146\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 110 |   100/  663 batches | lr 30.00 | ms/batch 283.19 | loss  4.08 | ppl    58.92\n",
            "| epoch 110 |   200/  663 batches | lr 30.00 | ms/batch 280.41 | loss  4.05 | ppl    57.33\n",
            "| epoch 110 |   300/  663 batches | lr 30.00 | ms/batch 280.50 | loss  4.05 | ppl    57.29\n",
            "| epoch 110 |   400/  663 batches | lr 30.00 | ms/batch 280.55 | loss  4.00 | ppl    54.77\n",
            "| epoch 110 |   500/  663 batches | lr 30.00 | ms/batch 280.22 | loss  4.07 | ppl    58.36\n",
            "| epoch 110 |   600/  663 batches | lr 30.00 | ms/batch 280.74 | loss  3.99 | ppl    54.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 110 | time: 195.64s | valid loss  4.26 | valid ppl    70.79 | valid bpc    6.146\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 111 |   100/  663 batches | lr 30.00 | ms/batch 283.12 | loss  4.07 | ppl    58.31\n",
            "| epoch 111 |   200/  663 batches | lr 30.00 | ms/batch 280.57 | loss  4.05 | ppl    57.11\n",
            "| epoch 111 |   300/  663 batches | lr 30.00 | ms/batch 280.74 | loss  4.05 | ppl    57.58\n",
            "| epoch 111 |   400/  663 batches | lr 30.00 | ms/batch 280.42 | loss  4.01 | ppl    55.16\n",
            "| epoch 111 |   500/  663 batches | lr 30.00 | ms/batch 280.35 | loss  4.07 | ppl    58.79\n",
            "| epoch 111 |   600/  663 batches | lr 30.00 | ms/batch 280.62 | loss  3.98 | ppl    53.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 111 | time: 195.65s | valid loss  4.26 | valid ppl    70.77 | valid bpc    6.145\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 112 |   100/  663 batches | lr 30.00 | ms/batch 283.18 | loss  4.08 | ppl    59.08\n",
            "| epoch 112 |   200/  663 batches | lr 30.00 | ms/batch 280.53 | loss  4.04 | ppl    56.73\n",
            "| epoch 112 |   300/  663 batches | lr 30.00 | ms/batch 280.71 | loss  4.05 | ppl    57.30\n",
            "| epoch 112 |   400/  663 batches | lr 30.00 | ms/batch 280.85 | loss  4.02 | ppl    55.80\n",
            "| epoch 112 |   500/  663 batches | lr 30.00 | ms/batch 280.15 | loss  4.07 | ppl    58.60\n",
            "| epoch 112 |   600/  663 batches | lr 30.00 | ms/batch 280.85 | loss  3.96 | ppl    52.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 112 | time: 195.70s | valid loss  4.26 | valid ppl    70.76 | valid bpc    6.145\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 113 |   100/  663 batches | lr 30.00 | ms/batch 282.57 | loss  4.07 | ppl    58.62\n",
            "| epoch 113 |   200/  663 batches | lr 30.00 | ms/batch 280.40 | loss  4.05 | ppl    57.18\n",
            "| epoch 113 |   300/  663 batches | lr 30.00 | ms/batch 280.69 | loss  4.07 | ppl    58.57\n",
            "| epoch 113 |   400/  663 batches | lr 30.00 | ms/batch 280.33 | loss  4.01 | ppl    55.16\n",
            "| epoch 113 |   500/  663 batches | lr 30.00 | ms/batch 280.40 | loss  4.07 | ppl    58.52\n",
            "| epoch 113 |   600/  663 batches | lr 30.00 | ms/batch 280.49 | loss  3.98 | ppl    53.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 113 | time: 195.57s | valid loss  4.26 | valid ppl    70.74 | valid bpc    6.144\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 114 |   100/  663 batches | lr 30.00 | ms/batch 282.98 | loss  4.05 | ppl    57.36\n",
            "| epoch 114 |   200/  663 batches | lr 30.00 | ms/batch 280.42 | loss  4.04 | ppl    56.81\n",
            "| epoch 114 |   300/  663 batches | lr 30.00 | ms/batch 280.84 | loss  4.04 | ppl    56.74\n",
            "| epoch 114 |   400/  663 batches | lr 30.00 | ms/batch 280.51 | loss  4.00 | ppl    54.74\n",
            "| epoch 114 |   500/  663 batches | lr 30.00 | ms/batch 279.94 | loss  4.06 | ppl    58.05\n",
            "| epoch 114 |   600/  663 batches | lr 30.00 | ms/batch 280.65 | loss  3.98 | ppl    53.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 114 | time: 195.61s | valid loss  4.26 | valid ppl    70.73 | valid bpc    6.144\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 115 |   100/  663 batches | lr 30.00 | ms/batch 282.95 | loss  4.06 | ppl    57.75\n",
            "| epoch 115 |   200/  663 batches | lr 30.00 | ms/batch 280.42 | loss  4.05 | ppl    57.16\n",
            "| epoch 115 |   300/  663 batches | lr 30.00 | ms/batch 280.71 | loss  4.05 | ppl    57.57\n",
            "| epoch 115 |   400/  663 batches | lr 30.00 | ms/batch 280.77 | loss  3.99 | ppl    54.24\n",
            "| epoch 115 |   500/  663 batches | lr 30.00 | ms/batch 280.57 | loss  4.07 | ppl    58.53\n",
            "| epoch 115 |   600/  663 batches | lr 30.00 | ms/batch 280.71 | loss  3.96 | ppl    52.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 115 | time: 195.69s | valid loss  4.26 | valid ppl    70.71 | valid bpc    6.144\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 116 |   100/  663 batches | lr 30.00 | ms/batch 282.96 | loss  4.08 | ppl    58.90\n",
            "| epoch 116 |   200/  663 batches | lr 30.00 | ms/batch 279.98 | loss  4.04 | ppl    56.66\n",
            "| epoch 116 |   300/  663 batches | lr 30.00 | ms/batch 280.68 | loss  4.04 | ppl    56.66\n",
            "| epoch 116 |   400/  663 batches | lr 30.00 | ms/batch 280.81 | loss  3.99 | ppl    54.25\n",
            "| epoch 116 |   500/  663 batches | lr 30.00 | ms/batch 279.97 | loss  4.07 | ppl    58.46\n",
            "| epoch 116 |   600/  663 batches | lr 30.00 | ms/batch 280.70 | loss  3.97 | ppl    53.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 116 | time: 195.59s | valid loss  4.26 | valid ppl    70.70 | valid bpc    6.144\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 117 |   100/  663 batches | lr 30.00 | ms/batch 283.00 | loss  4.07 | ppl    58.61\n",
            "| epoch 117 |   200/  663 batches | lr 30.00 | ms/batch 280.60 | loss  4.03 | ppl    56.38\n",
            "| epoch 117 |   300/  663 batches | lr 30.00 | ms/batch 280.55 | loss  4.04 | ppl    56.90\n",
            "| epoch 117 |   400/  663 batches | lr 30.00 | ms/batch 280.62 | loss  4.01 | ppl    54.89\n",
            "| epoch 117 |   500/  663 batches | lr 30.00 | ms/batch 280.20 | loss  4.06 | ppl    58.25\n",
            "| epoch 117 |   600/  663 batches | lr 30.00 | ms/batch 279.64 | loss  3.95 | ppl    51.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 117 | time: 195.52s | valid loss  4.26 | valid ppl    70.68 | valid bpc    6.143\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 118 |   100/  663 batches | lr 30.00 | ms/batch 283.37 | loss  4.07 | ppl    58.42\n",
            "| epoch 118 |   200/  663 batches | lr 30.00 | ms/batch 280.28 | loss  4.03 | ppl    56.51\n",
            "| epoch 118 |   300/  663 batches | lr 30.00 | ms/batch 280.63 | loss  4.02 | ppl    55.97\n",
            "| epoch 118 |   400/  663 batches | lr 30.00 | ms/batch 280.41 | loss  4.00 | ppl    54.41\n",
            "| epoch 118 |   500/  663 batches | lr 30.00 | ms/batch 280.22 | loss  4.07 | ppl    58.46\n",
            "| epoch 118 |   600/  663 batches | lr 30.00 | ms/batch 280.60 | loss  3.97 | ppl    52.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 118 | time: 195.61s | valid loss  4.26 | valid ppl    70.67 | valid bpc    6.143\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 119 |   100/  663 batches | lr 30.00 | ms/batch 283.02 | loss  4.06 | ppl    58.04\n",
            "| epoch 119 |   200/  663 batches | lr 30.00 | ms/batch 280.61 | loss  4.04 | ppl    56.88\n",
            "| epoch 119 |   300/  663 batches | lr 30.00 | ms/batch 279.94 | loss  4.03 | ppl    56.49\n",
            "| epoch 119 |   400/  663 batches | lr 30.00 | ms/batch 280.40 | loss  3.99 | ppl    53.83\n",
            "| epoch 119 |   500/  663 batches | lr 30.00 | ms/batch 280.15 | loss  4.06 | ppl    57.84\n",
            "| epoch 119 |   600/  663 batches | lr 30.00 | ms/batch 279.80 | loss  3.96 | ppl    52.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 119 | time: 195.45s | valid loss  4.26 | valid ppl    70.65 | valid bpc    6.143\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 120 |   100/  663 batches | lr 30.00 | ms/batch 283.14 | loss  4.05 | ppl    57.38\n",
            "| epoch 120 |   200/  663 batches | lr 30.00 | ms/batch 280.52 | loss  4.05 | ppl    57.13\n",
            "| epoch 120 |   300/  663 batches | lr 30.00 | ms/batch 280.38 | loss  4.02 | ppl    55.61\n",
            "| epoch 120 |   400/  663 batches | lr 30.00 | ms/batch 280.33 | loss  4.01 | ppl    55.16\n",
            "| epoch 120 |   500/  663 batches | lr 30.00 | ms/batch 280.09 | loss  4.06 | ppl    57.84\n",
            "| epoch 120 |   600/  663 batches | lr 30.00 | ms/batch 280.26 | loss  3.96 | ppl    52.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 120 | time: 195.49s | valid loss  4.26 | valid ppl    70.63 | valid bpc    6.142\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 121 |   100/  663 batches | lr 30.00 | ms/batch 283.02 | loss  4.05 | ppl    57.37\n",
            "| epoch 121 |   200/  663 batches | lr 30.00 | ms/batch 280.40 | loss  4.03 | ppl    56.50\n",
            "| epoch 121 |   300/  663 batches | lr 30.00 | ms/batch 280.18 | loss  4.02 | ppl    55.96\n",
            "| epoch 121 |   400/  663 batches | lr 30.00 | ms/batch 280.45 | loss  3.98 | ppl    53.71\n",
            "| epoch 121 |   500/  663 batches | lr 30.00 | ms/batch 280.18 | loss  4.05 | ppl    57.31\n",
            "| epoch 121 |   600/  663 batches | lr 30.00 | ms/batch 280.48 | loss  3.95 | ppl    51.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 121 | time: 195.53s | valid loss  4.26 | valid ppl    70.62 | valid bpc    6.142\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 122 |   100/  663 batches | lr 30.00 | ms/batch 283.09 | loss  4.07 | ppl    58.37\n",
            "| epoch 122 |   200/  663 batches | lr 30.00 | ms/batch 280.34 | loss  4.01 | ppl    55.28\n",
            "| epoch 122 |   300/  663 batches | lr 30.00 | ms/batch 280.50 | loss  4.02 | ppl    55.76\n",
            "| epoch 122 |   400/  663 batches | lr 30.00 | ms/batch 280.01 | loss  3.99 | ppl    53.89\n",
            "| epoch 122 |   500/  663 batches | lr 30.00 | ms/batch 280.17 | loss  4.04 | ppl    56.95\n",
            "| epoch 122 |   600/  663 batches | lr 30.00 | ms/batch 280.54 | loss  3.95 | ppl    51.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 122 | time: 195.52s | valid loss  4.26 | valid ppl    70.60 | valid bpc    6.142\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 123 |   100/  663 batches | lr 30.00 | ms/batch 283.38 | loss  4.05 | ppl    57.56\n",
            "| epoch 123 |   200/  663 batches | lr 30.00 | ms/batch 280.35 | loss  4.03 | ppl    56.35\n",
            "| epoch 123 |   300/  663 batches | lr 30.00 | ms/batch 280.56 | loss  4.01 | ppl    55.07\n",
            "| epoch 123 |   400/  663 batches | lr 30.00 | ms/batch 280.65 | loss  3.99 | ppl    54.11\n",
            "| epoch 123 |   500/  663 batches | lr 30.00 | ms/batch 280.01 | loss  4.05 | ppl    57.21\n",
            "| epoch 123 |   600/  663 batches | lr 30.00 | ms/batch 280.54 | loss  3.95 | ppl    52.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 123 | time: 195.62s | valid loss  4.26 | valid ppl    70.58 | valid bpc    6.141\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 124 |   100/  663 batches | lr 30.00 | ms/batch 282.90 | loss  4.05 | ppl    57.66\n",
            "| epoch 124 |   200/  663 batches | lr 30.00 | ms/batch 280.53 | loss  4.05 | ppl    57.37\n",
            "| epoch 124 |   300/  663 batches | lr 30.00 | ms/batch 280.37 | loss  4.01 | ppl    55.16\n",
            "| epoch 124 |   400/  663 batches | lr 30.00 | ms/batch 280.21 | loss  3.99 | ppl    54.14\n",
            "| epoch 124 |   500/  663 batches | lr 30.00 | ms/batch 280.47 | loss  4.05 | ppl    57.31\n",
            "| epoch 124 |   600/  663 batches | lr 30.00 | ms/batch 280.47 | loss  3.94 | ppl    51.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 124 | time: 195.55s | valid loss  4.26 | valid ppl    70.57 | valid bpc    6.141\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 125 |   100/  663 batches | lr 30.00 | ms/batch 282.87 | loss  4.05 | ppl    57.27\n",
            "| epoch 125 |   200/  663 batches | lr 30.00 | ms/batch 280.56 | loss  4.02 | ppl    55.71\n",
            "| epoch 125 |   300/  663 batches | lr 30.00 | ms/batch 280.59 | loss  4.02 | ppl    55.85\n",
            "| epoch 125 |   400/  663 batches | lr 30.00 | ms/batch 280.49 | loss  3.98 | ppl    53.31\n",
            "| epoch 125 |   500/  663 batches | lr 30.00 | ms/batch 280.00 | loss  4.04 | ppl    57.11\n",
            "| epoch 125 |   600/  663 batches | lr 30.00 | ms/batch 280.39 | loss  3.94 | ppl    51.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 125 | time: 195.53s | valid loss  4.26 | valid ppl    70.55 | valid bpc    6.141\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 126 |   100/  663 batches | lr 30.00 | ms/batch 282.98 | loss  4.05 | ppl    57.67\n",
            "| epoch 126 |   200/  663 batches | lr 30.00 | ms/batch 280.34 | loss  4.01 | ppl    54.97\n",
            "| epoch 126 |   300/  663 batches | lr 30.00 | ms/batch 280.31 | loss  4.00 | ppl    54.55\n",
            "| epoch 126 |   400/  663 batches | lr 30.00 | ms/batch 280.39 | loss  3.98 | ppl    53.51\n",
            "| epoch 126 |   500/  663 batches | lr 30.00 | ms/batch 280.04 | loss  4.04 | ppl    56.66\n",
            "| epoch 126 |   600/  663 batches | lr 30.00 | ms/batch 280.45 | loss  3.94 | ppl    51.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 126 | time: 195.50s | valid loss  4.26 | valid ppl    70.54 | valid bpc    6.140\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 127 |   100/  663 batches | lr 30.00 | ms/batch 282.90 | loss  4.05 | ppl    57.39\n",
            "| epoch 127 |   200/  663 batches | lr 30.00 | ms/batch 279.95 | loss  4.01 | ppl    55.20\n",
            "| epoch 127 |   300/  663 batches | lr 30.00 | ms/batch 280.29 | loss  4.00 | ppl    54.83\n",
            "| epoch 127 |   400/  663 batches | lr 30.00 | ms/batch 280.02 | loss  3.98 | ppl    53.28\n",
            "| epoch 127 |   500/  663 batches | lr 30.00 | ms/batch 280.14 | loss  4.03 | ppl    56.38\n",
            "| epoch 127 |   600/  663 batches | lr 30.00 | ms/batch 280.44 | loss  3.94 | ppl    51.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 127 | time: 195.43s | valid loss  4.26 | valid ppl    70.52 | valid bpc    6.140\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 128 |   100/  663 batches | lr 30.00 | ms/batch 283.14 | loss  4.04 | ppl    56.65\n",
            "| epoch 128 |   200/  663 batches | lr 30.00 | ms/batch 280.37 | loss  4.02 | ppl    55.87\n",
            "| epoch 128 |   300/  663 batches | lr 30.00 | ms/batch 280.43 | loss  4.00 | ppl    54.81\n",
            "| epoch 128 |   400/  663 batches | lr 30.00 | ms/batch 280.60 | loss  3.97 | ppl    53.08\n",
            "| epoch 128 |   500/  663 batches | lr 30.00 | ms/batch 280.07 | loss  4.05 | ppl    57.31\n",
            "| epoch 128 |   600/  663 batches | lr 30.00 | ms/batch 279.93 | loss  3.95 | ppl    51.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 128 | time: 195.49s | valid loss  4.26 | valid ppl    70.51 | valid bpc    6.140\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 129 |   100/  663 batches | lr 30.00 | ms/batch 282.91 | loss  4.05 | ppl    57.12\n",
            "| epoch 129 |   200/  663 batches | lr 30.00 | ms/batch 280.67 | loss  4.02 | ppl    55.91\n",
            "| epoch 129 |   300/  663 batches | lr 30.00 | ms/batch 280.55 | loss  4.02 | ppl    55.48\n",
            "| epoch 129 |   400/  663 batches | lr 30.00 | ms/batch 280.32 | loss  3.97 | ppl    52.97\n",
            "| epoch 129 |   500/  663 batches | lr 30.00 | ms/batch 280.16 | loss  4.02 | ppl    55.91\n",
            "| epoch 129 |   600/  663 batches | lr 30.00 | ms/batch 280.25 | loss  3.93 | ppl    50.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 129 | time: 195.54s | valid loss  4.26 | valid ppl    70.49 | valid bpc    6.139\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 130 |   100/  663 batches | lr 30.00 | ms/batch 283.01 | loss  4.06 | ppl    57.74\n",
            "| epoch 130 |   200/  663 batches | lr 30.00 | ms/batch 280.15 | loss  4.01 | ppl    55.16\n",
            "| epoch 130 |   300/  663 batches | lr 30.00 | ms/batch 280.10 | loss  4.01 | ppl    55.08\n",
            "| epoch 130 |   400/  663 batches | lr 30.00 | ms/batch 280.11 | loss  3.96 | ppl    52.60\n",
            "| epoch 130 |   500/  663 batches | lr 30.00 | ms/batch 279.47 | loss  4.04 | ppl    56.95\n",
            "| epoch 130 |   600/  663 batches | lr 30.00 | ms/batch 280.33 | loss  3.93 | ppl    50.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 130 | time: 195.37s | valid loss  4.26 | valid ppl    70.47 | valid bpc    6.139\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 131 |   100/  663 batches | lr 30.00 | ms/batch 282.89 | loss  4.04 | ppl    56.64\n",
            "| epoch 131 |   200/  663 batches | lr 30.00 | ms/batch 280.09 | loss  4.01 | ppl    55.41\n",
            "| epoch 131 |   300/  663 batches | lr 30.00 | ms/batch 280.28 | loss  4.02 | ppl    55.78\n",
            "| epoch 131 |   400/  663 batches | lr 30.00 | ms/batch 280.28 | loss  3.95 | ppl    52.17\n",
            "| epoch 131 |   500/  663 batches | lr 30.00 | ms/batch 279.92 | loss  4.03 | ppl    56.45\n",
            "| epoch 131 |   600/  663 batches | lr 30.00 | ms/batch 280.29 | loss  3.93 | ppl    51.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 131 | time: 195.44s | valid loss  4.26 | valid ppl    70.46 | valid bpc    6.139\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 132 |   100/  663 batches | lr 30.00 | ms/batch 282.49 | loss  4.03 | ppl    56.38\n",
            "| epoch 132 |   200/  663 batches | lr 30.00 | ms/batch 279.97 | loss  4.01 | ppl    55.01\n",
            "| epoch 132 |   300/  663 batches | lr 30.00 | ms/batch 280.14 | loss  4.01 | ppl    55.02\n",
            "| epoch 132 |   400/  663 batches | lr 30.00 | ms/batch 280.26 | loss  3.97 | ppl    53.08\n",
            "| epoch 132 |   500/  663 batches | lr 30.00 | ms/batch 279.86 | loss  4.02 | ppl    55.45\n",
            "| epoch 132 |   600/  663 batches | lr 30.00 | ms/batch 280.48 | loss  3.93 | ppl    51.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 132 | time: 195.39s | valid loss  4.25 | valid ppl    70.44 | valid bpc    6.138\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 133 |   100/  663 batches | lr 30.00 | ms/batch 283.00 | loss  4.03 | ppl    56.29\n",
            "| epoch 133 |   200/  663 batches | lr 30.00 | ms/batch 280.10 | loss  4.02 | ppl    55.48\n",
            "| epoch 133 |   300/  663 batches | lr 30.00 | ms/batch 280.64 | loss  4.01 | ppl    55.39\n",
            "| epoch 133 |   400/  663 batches | lr 30.00 | ms/batch 280.24 | loss  3.98 | ppl    53.31\n",
            "| epoch 133 |   500/  663 batches | lr 30.00 | ms/batch 279.42 | loss  4.03 | ppl    56.26\n",
            "| epoch 133 |   600/  663 batches | lr 30.00 | ms/batch 280.02 | loss  3.94 | ppl    51.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 133 | time: 195.40s | valid loss  4.25 | valid ppl    70.43 | valid bpc    6.138\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 134 |   100/  663 batches | lr 30.00 | ms/batch 282.44 | loss  4.04 | ppl    56.98\n",
            "| epoch 134 |   200/  663 batches | lr 30.00 | ms/batch 280.03 | loss  3.99 | ppl    54.25\n",
            "| epoch 134 |   300/  663 batches | lr 30.00 | ms/batch 280.48 | loss  4.02 | ppl    55.69\n",
            "| epoch 134 |   400/  663 batches | lr 30.00 | ms/batch 280.11 | loss  3.97 | ppl    53.01\n",
            "| epoch 134 |   500/  663 batches | lr 30.00 | ms/batch 279.88 | loss  4.04 | ppl    56.68\n",
            "| epoch 134 |   600/  663 batches | lr 30.00 | ms/batch 280.23 | loss  3.93 | ppl    50.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 134 | time: 195.38s | valid loss  4.25 | valid ppl    70.41 | valid bpc    6.138\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 135 |   100/  663 batches | lr 30.00 | ms/batch 283.17 | loss  4.02 | ppl    55.90\n",
            "| epoch 135 |   200/  663 batches | lr 30.00 | ms/batch 279.57 | loss  4.00 | ppl    54.86\n",
            "| epoch 135 |   300/  663 batches | lr 30.00 | ms/batch 279.93 | loss  3.98 | ppl    53.44\n",
            "| epoch 135 |   400/  663 batches | lr 30.00 | ms/batch 280.49 | loss  3.94 | ppl    51.56\n",
            "| epoch 135 |   500/  663 batches | lr 30.00 | ms/batch 280.11 | loss  4.02 | ppl    55.84\n",
            "| epoch 135 |   600/  663 batches | lr 30.00 | ms/batch 280.37 | loss  3.93 | ppl    51.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 135 | time: 195.43s | valid loss  4.25 | valid ppl    70.40 | valid bpc    6.137\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 136 |   100/  663 batches | lr 30.00 | ms/batch 282.74 | loss  4.04 | ppl    56.67\n",
            "| epoch 136 |   200/  663 batches | lr 30.00 | ms/batch 280.11 | loss  4.00 | ppl    54.80\n",
            "| epoch 136 |   300/  663 batches | lr 30.00 | ms/batch 280.30 | loss  3.98 | ppl    53.66\n",
            "| epoch 136 |   400/  663 batches | lr 30.00 | ms/batch 280.17 | loss  3.97 | ppl    52.86\n",
            "| epoch 136 |   500/  663 batches | lr 30.00 | ms/batch 279.95 | loss  4.02 | ppl    55.92\n",
            "| epoch 136 |   600/  663 batches | lr 30.00 | ms/batch 279.71 | loss  3.91 | ppl    49.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 136 | time: 195.33s | valid loss  4.25 | valid ppl    70.39 | valid bpc    6.137\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 137 |   100/  663 batches | lr 30.00 | ms/batch 282.64 | loss  4.01 | ppl    55.19\n",
            "| epoch 137 |   200/  663 batches | lr 30.00 | ms/batch 280.04 | loss  3.99 | ppl    54.02\n",
            "| epoch 137 |   300/  663 batches | lr 30.00 | ms/batch 280.47 | loss  4.01 | ppl    54.90\n",
            "| epoch 137 |   400/  663 batches | lr 30.00 | ms/batch 280.27 | loss  3.96 | ppl    52.31\n",
            "| epoch 137 |   500/  663 batches | lr 30.00 | ms/batch 279.80 | loss  4.01 | ppl    55.24\n",
            "| epoch 137 |   600/  663 batches | lr 30.00 | ms/batch 280.18 | loss  3.92 | ppl    50.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 137 | time: 195.39s | valid loss  4.25 | valid ppl    70.37 | valid bpc    6.137\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 138 |   100/  663 batches | lr 30.00 | ms/batch 282.73 | loss  4.02 | ppl    55.89\n",
            "| epoch 138 |   200/  663 batches | lr 30.00 | ms/batch 280.09 | loss  4.01 | ppl    54.95\n",
            "| epoch 138 |   300/  663 batches | lr 30.00 | ms/batch 279.76 | loss  3.99 | ppl    54.10\n",
            "| epoch 138 |   400/  663 batches | lr 30.00 | ms/batch 280.32 | loss  3.97 | ppl    53.04\n",
            "| epoch 138 |   500/  663 batches | lr 30.00 | ms/batch 279.90 | loss  4.01 | ppl    55.37\n",
            "| epoch 138 |   600/  663 batches | lr 30.00 | ms/batch 280.41 | loss  3.92 | ppl    50.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 138 | time: 195.37s | valid loss  4.25 | valid ppl    70.36 | valid bpc    6.137\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 139 |   100/  663 batches | lr 30.00 | ms/batch 282.94 | loss  4.02 | ppl    55.77\n",
            "| epoch 139 |   200/  663 batches | lr 30.00 | ms/batch 280.31 | loss  3.99 | ppl    54.27\n",
            "| epoch 139 |   300/  663 batches | lr 30.00 | ms/batch 280.32 | loss  4.00 | ppl    54.73\n",
            "| epoch 139 |   400/  663 batches | lr 30.00 | ms/batch 280.03 | loss  3.96 | ppl    52.28\n",
            "| epoch 139 |   500/  663 batches | lr 30.00 | ms/batch 279.84 | loss  4.02 | ppl    55.45\n",
            "| epoch 139 |   600/  663 batches | lr 30.00 | ms/batch 280.39 | loss  3.91 | ppl    49.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 139 | time: 195.37s | valid loss  4.25 | valid ppl    70.35 | valid bpc    6.136\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 140 |   100/  663 batches | lr 30.00 | ms/batch 282.85 | loss  4.03 | ppl    56.07\n",
            "| epoch 140 |   200/  663 batches | lr 30.00 | ms/batch 280.26 | loss  3.99 | ppl    53.91\n",
            "| epoch 140 |   300/  663 batches | lr 30.00 | ms/batch 280.42 | loss  3.98 | ppl    53.59\n",
            "| epoch 140 |   400/  663 batches | lr 30.00 | ms/batch 280.39 | loss  3.94 | ppl    51.63\n",
            "| epoch 140 |   500/  663 batches | lr 30.00 | ms/batch 280.06 | loss  4.01 | ppl    54.98\n",
            "| epoch 140 |   600/  663 batches | lr 30.00 | ms/batch 280.25 | loss  3.92 | ppl    50.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 140 | time: 195.46s | valid loss  4.25 | valid ppl    70.33 | valid bpc    6.136\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FqHtDk-4kUor"
      },
      "cell_type": "markdown",
      "source": [
        "Finally,  open the best saved model run it on the test data"
      ]
    },
    {
      "metadata": {
        "id": "3rP_dOtpkCTX",
        "outputId": "c2b97724-6ef0-4288-f930-79199f15bb0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the best saved model.\n",
        "model_load(args_save)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
            "  result = self.forward(*input, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  4.21 | test ppl    67.40\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3X3GRsmJYya"
      },
      "cell_type": "markdown",
      "source": [
        "# Word generator\n",
        "\n",
        "First define the arguments and load the corpus"
      ]
    },
    {
      "metadata": {
        "id": "DSz0R2g97VAk"
      },
      "cell_type": "code",
      "source": [
        "model_load(args_save)\n",
        "\n",
        "args_words = 300\n",
        "args_temperature = 0.8\n",
        "model.eval()\n",
        "\n",
        "corpus = Corpus(args_data)\n",
        "ntokens = len(corpus.dictionary)\n",
        "hidden = model.init_hidden(1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5sr47CxlJeMb"
      },
      "cell_type": "markdown",
      "source": [
        "Then generate some data"
      ]
    },
    {
      "metadata": {
        "id": "pK2WqvLjJcYh",
        "outputId": "cc8b21fc-2c7c-49de-dc03-5d1d1efc148c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "cell_type": "code",
      "source": [
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "words = []\n",
        "probs = []\n",
        "\n",
        "with torch.no_grad():  # no tracking history\n",
        "    for i in range(args_words):\n",
        "        output, hidden = model(input, hidden)\n",
        "        \n",
        "        word_weights = output.squeeze().div(args_temperature).exp().cpu()\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        input.fill_(word_idx)\n",
        "        word = corpus.dictionary.idx2word[word_idx]\n",
        "        \n",
        "        # We replace <unk> and <eos> with * to get a cleaner look, but thats just\n",
        "        # personal preference\n",
        "        if(word == \"<unk>\" or word == \"<eos>\"):\n",
        "          word = \"*\"\n",
        "\n",
        "        print(word + ('\\n' if i % 20 == 19 else ' '),end='')\n",
        "        \n",
        "        # We also create arrays with the generated words and their probability \n",
        "        # to be used for visualizing them in a tool that we created for this\n",
        "        # purpose\n",
        "        words.append(word)\n",
        "        probs.append(output.squeeze()[word_idx].data.tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py:477: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
            "  result = self.forward(*input, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "used in the improved asia * * producers play its * now * themselves of its * as they were\n",
            "off tag says * stein president and chief executive of * group inc. a oil and gas company * this\n",
            "june the executives for instance have * their down * the new york 's main real-estate refinery are * and\n",
            "* these rumors last year on my technologies and ballot * he dressed himself a new mexico * in *\n",
            "* and business trying to get wine is english at a * time to stay today * mr. sherman what\n",
            "'s more those swings might have counted * and a * cheaper ducks but a * pressure consistent with the\n",
            "seniors and investment privacy as low as sophisticated parties have turned when they are * there are charities * who\n",
            "can afford as the plants * in recent weeks on the bargain this year he has a * reputation for\n",
            "the first of these things * filling new time for giants now if i give it a * there will\n",
            "be a slew of real rooms * investors you support the big board to fit * * he adds he\n",
            "wants years later after much bear stearns says it is one of the least * to the bureau oct. N\n",
            "* in just the * firm 's story season together some industry analysts say that a week after the way\n",
            "the new york stock exchange approved * listing * corp. says contract contributions altogether did n't have a lot of\n",
            "action * if stock prices and other big three major dealers would * more he says * * * by\n",
            "* * smith mr. * * the legendary of edward jefferson * de * * securities according to exchange observers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zEwdXHKF2tz-"
      },
      "cell_type": "markdown",
      "source": [
        "Print the words and probabilities for use in a [vizualization tool](https://github.com/mikkelbrusen/text-weight-visualizer) we created "
      ]
    },
    {
      "metadata": {
        "id": "kRRaUq9ZJsNT",
        "outputId": "7702dab5-c200-45b4-f901-1bac36878f95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "print(words)\n",
        "print(probs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['used', 'in', 'the', 'improved', 'asia', '*', '*', 'producers', 'play', 'its', '*', 'now', '*', 'themselves', 'of', 'its', '*', 'as', 'they', 'were', 'off', 'tag', 'says', '*', 'stein', 'president', 'and', 'chief', 'executive', 'of', '*', 'group', 'inc.', 'a', 'oil', 'and', 'gas', 'company', '*', 'this', 'june', 'the', 'executives', 'for', 'instance', 'have', '*', 'their', 'down', '*', 'the', 'new', 'york', \"'s\", 'main', 'real-estate', 'refinery', 'are', '*', 'and', '*', 'these', 'rumors', 'last', 'year', 'on', 'my', 'technologies', 'and', 'ballot', '*', 'he', 'dressed', 'himself', 'a', 'new', 'mexico', '*', 'in', '*', '*', 'and', 'business', 'trying', 'to', 'get', 'wine', 'is', 'english', 'at', 'a', '*', 'time', 'to', 'stay', 'today', '*', 'mr.', 'sherman', 'what', \"'s\", 'more', 'those', 'swings', 'might', 'have', 'counted', '*', 'and', 'a', '*', 'cheaper', 'ducks', 'but', 'a', '*', 'pressure', 'consistent', 'with', 'the', 'seniors', 'and', 'investment', 'privacy', 'as', 'low', 'as', 'sophisticated', 'parties', 'have', 'turned', 'when', 'they', 'are', '*', 'there', 'are', 'charities', '*', 'who', 'can', 'afford', 'as', 'the', 'plants', '*', 'in', 'recent', 'weeks', 'on', 'the', 'bargain', 'this', 'year', 'he', 'has', 'a', '*', 'reputation', 'for', 'the', 'first', 'of', 'these', 'things', '*', 'filling', 'new', 'time', 'for', 'giants', 'now', 'if', 'i', 'give', 'it', 'a', '*', 'there', 'will', 'be', 'a', 'slew', 'of', 'real', 'rooms', '*', 'investors', 'you', 'support', 'the', 'big', 'board', 'to', 'fit', '*', '*', 'he', 'adds', 'he', 'wants', 'years', 'later', 'after', 'much', 'bear', 'stearns', 'says', 'it', 'is', 'one', 'of', 'the', 'least', '*', 'to', 'the', 'bureau', 'oct.', 'N', '*', 'in', 'just', 'the', '*', 'firm', \"'s\", 'story', 'season', 'together', 'some', 'industry', 'analysts', 'say', 'that', 'a', 'week', 'after', 'the', 'way', 'the', 'new', 'york', 'stock', 'exchange', 'approved', '*', 'listing', '*', 'corp.', 'says', 'contract', 'contributions', 'altogether', 'did', \"n't\", 'have', 'a', 'lot', 'of', 'action', '*', 'if', 'stock', 'prices', 'and', 'other', 'big', 'three', 'major', 'dealers', 'would', '*', 'more', 'he', 'says', '*', '*', '*', 'by', '*', '*', 'smith', 'mr.', '*', '*', 'the', 'legendary', 'of', 'edward', 'jefferson', '*', 'de', '*', '*', 'securities', 'according', 'to', 'exchange', 'observers']\n",
            "[3.771145820617676, 11.369213104248047, 10.436104774475098, 0.13516151905059814, 0.3973260521888733, 8.786392211914062, 10.272981643676758, 2.862051486968994, 3.980703353881836, 7.147835731506348, 10.63596248626709, 3.5277562141418457, 8.866939544677734, 5.2740864753723145, 7.906059741973877, 6.856526851654053, 10.429065704345703, 6.389941692352295, 8.033683776855469, 10.464689254760742, 5.33371639251709, -0.2657870650291443, 6.686994552612305, 11.567791938781738, 6.3699235916137695, 10.646923065185547, 13.380460739135742, 15.544424057006836, 18.345291137695312, 15.521333694458008, 11.452930450439453, 8.875432968139648, 14.480661392211914, 12.96415901184082, 5.914664268493652, 14.11048698425293, 18.28464126586914, 14.833683013916016, 14.251232147216797, 7.953346252441406, 3.736015796661377, 9.972921371459961, 4.90095853805542, 7.5442023277282715, 8.871905326843262, 9.997167587280273, 9.539459228515625, 8.59453010559082, 3.567610502243042, 8.307217597961426, 9.717443466186523, 7.673157215118408, 11.253119468688965, 7.434220790863037, 6.460310935974121, 5.435682773590088, 6.0107011795043945, 7.373818397521973, 9.94645881652832, 8.759235382080078, 9.816699028015137, 4.035771369934082, 6.367401599884033, 4.2513837814331055, 13.577919006347656, 6.736871719360352, 5.36161994934082, 0.8217712640762329, 9.757185935974121, 0.399341881275177, 9.6946439743042, 8.051332473754883, 1.710181474685669, 7.752700328826904, 9.508362770080566, 7.785453796386719, 6.941113471984863, 10.718989372253418, 8.42707633972168, 10.113780975341797, 9.676858901977539, 9.484299659729004, 2.971970558166504, 3.296255111694336, 16.11878776550293, 10.310434341430664, 1.8050042390823364, 5.832589626312256, 2.6262567043304443, 8.100348472595215, 10.599021911621094, 11.325652122497559, 7.092032432556152, 9.254019737243652, 7.655665397644043, 6.371761322021484, 12.940056800842285, 8.507784843444824, 6.223886489868164, 4.309431552886963, 9.745757102966309, 12.578012466430664, 5.315005302429199, 5.48605489730835, 6.77655553817749, 11.408332824707031, 4.271780967712402, 8.875770568847656, 9.027162551879883, 7.54793119430542, 10.795488357543945, 1.5691250562667847, 0.7360264658927917, 7.819679260253906, 8.280055046081543, 10.383716583251953, 2.6522955894470215, 1.8836748600006104, 15.195756912231445, 10.756486892700195, 2.2143218517303467, 10.010395050048828, 5.971789360046387, 1.347936749458313, 7.240184783935547, 6.2764410972595215, 14.682191848754883, 3.5763790607452393, 3.7000510692596436, 8.179252624511719, 6.697369575500488, 6.146722793579102, 10.225214958190918, 11.77102279663086, 8.159316062927246, 7.47451114654541, 15.210874557495117, 2.6831774711608887, 8.863614082336426, 7.406057357788086, 9.958909034729004, 8.08282470703125, 7.011951923370361, 9.590763092041016, 3.03678035736084, 9.316049575805664, 8.905753135681152, 9.243824005126953, 15.617997169494629, 6.908970832824707, 11.444748878479004, 3.52339243888855, 5.856034278869629, 11.064729690551758, 8.95235538482666, 10.27260971069336, 9.559471130371094, 10.751049995422363, 5.662084579467773, 11.128746032714844, 9.917543411254883, 7.077676773071289, 8.883354187011719, 8.860803604125977, 10.78366470336914, 11.6985502243042, 1.2898494005203247, 7.152432918548584, 7.941965103149414, 9.34394359588623, 2.5427358150482178, 5.868509769439697, 6.536918640136719, 8.865388870239258, 7.932973861694336, 11.157773971557617, 11.495909690856934, 10.529365539550781, 6.15435266494751, 11.917888641357422, 17.03506088256836, 12.342389106750488, 5.932089805603027, 15.888801574707031, 6.506553649902344, 7.791563510894775, 10.777914047241211, 4.941690444946289, 4.391092300415039, 5.7456464767456055, 9.954536437988281, 6.739520072937012, 11.903068542480469, 9.721735954284668, 3.310122489929199, 9.027680397033691, 9.46313762664795, 8.058212280273438, 10.747852325439453, 9.770977020263672, 10.409980773925781, 5.001729965209961, 9.058538436889648, 8.827890396118164, 4.640229225158691, 2.423403263092041, 15.804591178894043, 8.273048400878906, 10.298168182373047, 11.70747184753418, 7.99831485748291, 13.100461959838867, 13.973584175109863, 9.000181198120117, 10.333181381225586, 8.359171867370605, 10.151369094848633, 3.38517427444458, 4.013308048248291, 20.93503189086914, 12.126922607421875, 8.836199760437012, 5.718515396118164, 10.782364845275879, 10.009244918823242, 4.574187278747559, 8.751219749450684, 3.9585764408111572, 2.610908031463623, 2.844932794570923, 6.919133186340332, 7.120312213897705, 13.440778732299805, 13.697433471679688, 10.46933650970459, 8.394623756408691, 5.987244606018066, 10.542253494262695, 11.173107147216797, 5.483043670654297, 10.276403427124023, 8.013505935668945, 12.303411483764648, 14.65816879272461, 22.25042724609375, 4.614611625671387, 9.34570598602295, 2.945434808731079, 8.610479354858398, 2.0621442794799805, 7.5239996910095215, 2.4370479583740234, 5.971918106079102, 2.3767974376678467, 5.457437038421631, 14.604425430297852, 10.324989318847656, 11.390694618225098, 8.200161933898926, 13.130489349365234, 5.233066558837891, 11.567781448364258, 7.471081256866455, 6.674075126647949, 17.27712059020996, 10.326818466186523, 9.916081428527832, 9.832097053527832, 9.700204849243164, 7.499362945556641, 3.8790016174316406, 7.404292583465576, 9.941106796264648, 7.092525482177734, 6.332201957702637, 12.862510681152344, 11.462587356567383, 8.449230194091797, 9.882999420166016, 5.468742370605469, 10.44758415222168, 10.980881690979004, 5.698960304260254, 5.858609676361084, 14.308111190795898, 9.721283912658691, 10.182891845703125, 2.0325121879577637, 6.844775676727295, 5.126440048217773, 10.725132942199707, 10.249673843383789, 4.439821243286133, 14.088113784790039, 9.63200569152832, 6.239521026611328, 4.774617671966553, 19.07306480407715, 5.859836578369141, 10.155213356018066]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}